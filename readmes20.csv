,content
0,"
Netdata    
    

Netdata is distributed, real-time, performance and health monitoring for systems and applications. It is a highly optimized monitoring agent you install on all your systems and containers.
Netdata provides unparalleled insights, in real-time, of everything happening on the systems it runs (including web servers, databases, applications), using highly interactive web dashboards. It can run autonomously, without any third party components, or it can be integrated to existing monitoring tool chains (Prometheus, Graphite, OpenTSDB, Kafka, Grafana, etc).
Netdata is fast and efficient, designed to permanently run on all systems (physical & virtual servers, containers, IoT devices), without disrupting their core function.
Netdata is free, open-source software and it currently runs on Linux, FreeBSD, and MacOS.
Netdata is not hosted by the CNCF but is the 3rd most starred open-source project in the Cloud Native Computing Foundation (CNCF) landscape.

People get addicted to Netdata.
Once you use it on your systems, there is no going back! You have been warned...


Contents

How it looks - have a quick look at it
User base - who uses Netdata?
Quick Start - try it now on your systems
Why Netdata - why people love Netdata, how it compares with other solutions
News - latest news about Netdata
How it works - high level diagram of how Netdata works
infographic - everything about Netdata, in a page
Features - what features does it have
Visualization - unique visualization features
What does it monitor - which metrics it collects
Documentation - read the docs
Community - discuss with others and get support
License - check the license of Netdata
Is it any good? - Yes
Is it awesome? - Yes

How it looks
The following animated image, shows the top part of a typical Netdata dashboard.

A typical Netdata dashboard, in 1:1 timing. Charts can be panned by dragging them, zoomed in/out with SHIFT + mouse wheel, an area can be selected for zoom-in with SHIFT + mouse selection. Netdata is highly interactive and real-time, optimized to get the work done!

We have a few online demos to experience it live: https://www.netdata.cloud

User base
Netdata is used by hundreds of thousands of users all over the world.
Check our GitHub watchers list.
You will find people working for Amazon, Atos, Baidu, Cisco Systems, Citrix, Deutsche Telekom, DigitalOcean,
Elastic, EPAM Systems, Ericsson, Google, Groupon, Hortonworks, HP, Huawei,
IBM, Microsoft, NewRelic, Nvidia, Red Hat, SAP, Selectel, TicketMaster,
Vimeo, and many more!
Docker pulls
We provide docker images for the most common architectures. These are statistics reported by docker hub:
  
Registry
When you install multiple Netdata, they are integrated into one distributed application, via a Netdata registry. This is a web browser feature and it allows us to count the number of unique users and unique Netdata servers installed. The following information comes from the global public Netdata registry we run:
  
in the last 24 hours:   
Quick Start
 
To install Netdata from source on any Linux system (physical, virtual, container, IoT, edge) and keep it up to date with our nightly releases automatically, run the following:
# make sure you run `bash` for your shell
bash

# install Netdata directly from GitHub source
bash <(curl -Ss https://my-netdata.io/kickstart.sh)
To learn more about the pros and cons of using nightly vs. stable releases, see our notice about the two options.
The above command will:

Install any required packages on your system (it will ask you to confirm before doing so)
Compile it, install it, and start it.

More installation methods and additional options can be found at the installation page.
To try Netdata in a docker container, run this:
docker run -d --name=netdata \
  -p 19999:19999 \
  -v /etc/passwd:/host/etc/passwd:ro \
  -v /etc/group:/host/etc/group:ro \
  -v /proc:/host/proc:ro \
  -v /sys:/host/sys:ro \
  -v /var/run/docker.sock:/var/run/docker.sock:ro \
  --cap-add SYS_PTRACE \
  --security-opt apparmor=unconfined \
  netdata/netdata
For more information about running Netdata in docker, check the docker installation page.

From Netdata v1.12 and above, anonymous usage information is collected by default and sent to Google Analytics. To read more about the information collected and how to opt-out, check the anonymous statistics page.
Why Netdata
Netdata has a quite different approach to monitoring.
Netdata is a monitoring agent you install on all your systems. It is:

a metrics collector - for system and application metrics (including web servers, databases, containers, etc)
a time-series database - all stored in memory (does not touch the disks while it runs)
a metrics visualizer - super fast, interactive, modern, optimized for anomaly detection
an alarms notification engine - an advanced watchdog for detecting performance and availability issues

All the above, are packaged together in a very flexible, extremely modular, distributed application.
This is how Netdata compares to other monitoring solutions:



Netdata
others (open-source and commercial)




High resolution metrics (1s granularity)
Low resolution metrics (10s granularity at best)


Monitors everything, thousands of metrics per node
Monitor just a few metrics


UI is super fast, optimized for anomaly detection
UI is good for just an abstract view


Meaningful presentation, to help you understand the metrics
You have to know the metrics before you start


Install and get results immediately
Long preparation is required to get any useful results


Use it for troubleshooting performance problems
Use them to get statistics of past performance


Kills the console for tracing performance issues
The console is always required for troubleshooting


Requires zero dedicated resources
Require large dedicated resources



Netdata is open-source, free, super fast, very easy, completely open, extremely efficient,
flexible and integrate-able.
It has been designed by SysAdmins, DevOps and Developers for troubleshooting performance problems,
not just visualize metrics.
News
Nov 27th, 2019 - Netdata v1.19.0 released!
Release v1.19.0 contains 2 new collectors, 19 bug fixes, 17 improvements, and 19 documentation updates.
We completed a major rewrite of our web log collector to dramatically improve its flexibility and performance. The new collector, written entirely in Go, can parse and chart logs from Nginx and Apache servers, and combines numerous improvements. Netdata now supports the LTSV log format, creates charts for TLS and cipher usage, and is amazingly fast. In a test using SSD storage, the collector parsed the logs for 200,000 requests in about 200ms, using 30% of a single core.
This Go-based collector also has powerful custom log parsing capabilities, which means we're one step closer to a generic application log parser for Netdata. We're continuing to work on this parser to support more application log formatting in the future.
We have a new tutorial on enabling the Go web log collector and using it with Nginx and/or Apache access logs with minimal configuration. Thanks to Wing924 for starting the Go rewrite!
We introduced more cmocka unit testing to Netdata. In this release, we're testing how Netdata's internal web server processes HTTP requests—the first step to improve the quality of code throughout, reduce bugs, and make refactoring easier. We wanted to validate the web server's behavior but needed to build a layer of parametric testing on top of the CMocka test runner. Read all about our process of testing and selecting cmocka on our blog post: Building an agile team's 'safety harness' with cmocka and FOSS.
Netdata's Unbound collector was also completely rewritten in Go to improve how it collects and displays metrics. This new version can get dozens of metrics, including details on queries, cache, uptime, and even show per-thread metrics. See our tutorial on enabling the new collector via Netdata's amazing auto-detection feature.
We fixed an error where invalid spikes appeared on certain charts by improving the incremental counter reset/wraparound detection algorithm.
Netdata can now send health alarm notifications to IRC channels thanks to Strykar!
And, Netdata can now monitor AM2320 sensors, thanks to hard work from Tom Buck.

Oct 18th, 2019 - Netdata v1.18.1 released!
Release v1.18.1 contains 17 bug fixes, 5 improvements, and 5 documentation updates.
The patch has several bug fixes, mainly related to FreeBSD and the binary package generation process.
Netdata can now send notifications to Google Hangouts Chat!
On certain systems, the slabinfo plugin introduced in v1.18.0 added thousands of new metrics. We decided the collector's usefulness to most users didn't justify the increase in resource requirements. This release disables the collector by default.
Finally, we added a chart under Netdata Monitoring to present a better view of the RAM used by the database engine (dbengine). The chart doesn't currently take into consideration the RAM used for slave nodes, so we intend to add more related charts in the future.

Oct 10th, 2019 - Netdata v1.18.0 released!
Release v1.18.0 contains 5 new collectors, 16 bug fixes, 27 improvements, and 20 documentation updates.
The database engine is now the default method of storing metrics in Netdata. You immediately get more efficient and configurable long-term metrics storage without any work on your part. By saving recent metrics in RAM and ""spilling"" historical metrics to disk for long-term storage, the database engine is laying the foundation for many more improvements to distributed metrics.
We even have a tutorial on switching to the database engine and getting the most from it. Or, just read up on how performant the database engine really is.
Both our python.d and go.d plugins now have more intelligent auto-detection by periodically dump a list of active modules to disk. When Netdata starts, such as after a reboot, the plugins use this list of known services to re-establish metrics collection much more reliably. No more worrying if the service or application you need to monitor starts up minutes after Netdata.
Two of our new collectors will help those with Hadoop big data infrastructures. The HDFS and Zookeeper collection modules come with essential alarms requested by our community and Netdata's auto-detection capabilities to keep the required configuration to an absolute minimum. Read up on the process via our HDFS and Zookeeper tutorial.
Speaking of new collectors—we also added the ability to collect metrics from SLAB cache, Gearman, and vCenter Server Appliances.
Before v1.18, if you wanted to create alarms for each dimension in a single chart, you need to write separate entities for each dimension—not very efficient or user-friendly. New dimension templates fix that hassle. Now, a single entity can automatically generate alarms for any number of dimensions in a chart, even those you weren't aware of! Our tutorial on dimension templates has all the details.
v1.18 brings support for installing Netdata on offline or air-gapped systems. To help users comply with strict security policies, our installation scripts can now install Netdata using previously-downloaded tarball and checksums instead of downloading them at runtime. We have guides for installing offline via kickstart.sh or kickstart-static64.sh in our installation documentation. We're excited to bring real-time monitoring to once-inaccessible systems!

Sep 12th, 2019 - Netdata v1.17.1 released!
Release v1.17.1 contains 2 bug fixes, 6 improvements, and 2 documentation updates.
The main reason for the patch release is an essential fix to the repeating alarm notifications we introduced in v1.17.0. If you enabled repeating notifications, Netdata would not then send CLEAR notifications for the selected alarms.
The release also includes a significant improvement to Netdata's auto-detection capabilities, especially after a system restart. Netdata now remembers which python.d plugin jobs were successfully collecting data the last time it was running, and retries to run those jobs for 5 minutes before giving up. As a result, you no longer have to worry if your system starts Netdata before the monitored services have had a chance to start properly. We will complete the same improvement for go.d plugins in v1.18.0.
We also made some improvements to our binary packages and added a neat sample custom dashboard that can show charts from multiple Netdata agents.

Sep 3rd, 2019 - Netdata v1.17.0 released!
Release v1.17.0 contains 38 bug fixes, 33 improvements, and 20 documentation updates.
You can now change the data collection frequency at will, without losing previously collected values. A major improvement to the new database engine allows you not only to store metrics at variable granularity, but also to autoscale the time axis of the charts, depending on the data collection frequencies used during the presented time.
You can also now monitor VM performance from one or more vCenter servers with a new VSphere collector. In addition, the proc plugin now also collects ZRAM device performance metrics and the apps plugin monitors process uptime for the defined process groups.
Continuing our efforts to integrate with as many existing solutions as possible, you can now directly archive metrics from Netdata to MongoDB via a new backend.
Netdata badges now support international (UTF8) characters! We also made our URL parser smarter, not only for international character support, but also for other strange API queries.
We also added .DEB packages to our binary distribution repositories at Packagecloud, a new collector for Linux zram device metrics, and support for plain text email notifications.
This release includes several fixes and improvements to the TLS encryption feature we introduced in v1.16.0. First, encryption slave-to-master streaming connections wasn't working as intended. And second, our community helped us discover cases where HTTP requests were not correctly redirected to HTTPS with TLS enabled. This release mitigates those issues and improves TLS support overall.
Finally, we improved the way Netdata displays charts with no metrics. By default, Netdata displays charts for disks, memory, and networks only when the associated metrics are not zero. Users could enable these charts permanently using the corresponding configuration options, but they would need to change more than 200 options. With this new improvement, users can enable all charts with zero values using a single, global configuration parameter.

Jul 9th, 2019 - Netdata v1.16.0 released!
Release v1.16.0 contains 40 bug fixes, 31 improvements and 20 documentation updates
Binary distributions. To improve the security, speed and reliability of new Netdata installations, we are delivering our own, industry standard installation method, with binary package distributions. The RPM binaries for the most common OSs are already available on packagecloud and we’ll have the DEB ones available very soon. All distributions are considered in Beta and, as always, we depend on our amazing community for feedback on improvements.

Our stable distributions are at netdata/netdata @ packagecloud.io
The nightly builds are at netdata/netdata-edge @ packagecloud.io

Netdata now supports TLS encryption! You can secure the communication to the web server, the streaming connections from slaves to the master and the connection to an openTSDB backend.
This version also brings two long-awaited features to Netdata’s health monitoring:

The health management API introduced in v1.12 allowed you to easily disable alarms and/or notifications while Netdata was running. However, those changes were not persisted across Netdata restarts. Since part of routine maintenance activities may involve completely restarting a monitoring node, Netdata now saves these configurations to disk, every time you issue a command to change the silencer settings. The new LIST command of the API allows you to view at any time which alarms are currently disabled or silenced.
A way for Netdata to repeatedly send alarm notifications for some, or all active alarms, at a frequency of your choosing. As a result, you will no longer have to worry about missing a notification, forgetting about a raised alarm. The default is still to only send a single notification, so that existing users are not surprised by a different behavior.

As always, we’ve introduced new collectors, 5 of them this time:

Of special interest to people with Windows servers in their infrastructure is the WMI collector, though we are fully aware that we need to continue our efforts to do a proper port to Windows.
The new perf plugin collects system-wide CPU performance statistics from Performance Monitoring Units (PMU) using the perf_event_open() system call. You can read a wonderful article on why this is useful here.
The other three are collectors to monitor Dnsmasq DHCP leases, Riak KV servers and Pihole instances.

Finally, the DB Engine introduced in v1.15.0 now uses much less memory and is more robust than before.

May 21st, 2019 - Netdata v1.15.0 released!
Release v1.15.0 contains 11 bug fixes and 30 improvements.
We are very happy and proud to be able to include two major improvements in this release: The aggregated node view and the new database engine.
Aggregated node view
The No. 1 request from our community has been a better way to view and manage their Netdata installations, via an aggregated view. The node menu with the simple list of hosts on the agent UI just didn't do it for people with hundreds, or thousands of instances. This release introduces the node view, which uses the power of Netdata Cloud to deliver powerful views of a Netdata-based monitoring infrastructure. You can read more about Netdata Cloud and the future of Netdata here.
New database engine
Historically, Netdata has required a lot of memory for long-term metrics storage. To mitigate this we've been building a new DB engine for several months and will continue improving until it can become the default memory mode for new Netdata installations. The version included in release v1.15.0 already permits longer-term storage of compressed data and we'll continue reducing the required memory in following releases.
Other major additions
We have added support for the AWS Kinesis backend and new collectors for OpenVPN, the Tengine web server, ScaleIO (VxFlex OS), ioping-like latency metrics and Energi Core node instances.
We now have a new, ""text-only"" chart type, cpu limits for v2 cgroups, docker swarm metrics and improved documentation.
We continued improving the Kubernetes helmchart with liveness probes for slaves, persistence options, a fix for a Cannot allocate memory issue and easy configuration for the kubelet, kube-proxy and coredns collectors.
Finally, we built a process to quickly replace any problematic nightly builds and added more automated CI tests to prevent such builds from being published in the first place.

Apr 26th, 2019 - Netdata v1.14.0 released!
Release 1.14 contains 14 bug fixes and 24 improvements.
The release introduces major additions to Kubernetes monitoring, with tens of new charts for Kubelet, kube-proxy and coredns metrics, as well as significant improvements to the Netdata helm chart.
Two new collectors were added, to monitor Docker hub and Docker engine metrics.
Finally, v1.14  adds support for version 2 cgroups, OpenLDAP over TLS, NVIDIA SMI free and per process memory and configurable syslog facilities.

Mar 14th, 2019 - Netdata v1.13.0 released!
Release 1.13.0 contains 14 bug fixes and 8 improvements.
Netdata has taken the first step into the world of Kubernetes, with a beta version of a Helm chart for deployment to a k8s cluster and proper naming of the cgroup containers. We have big plans for Kubernetes, so stay tuned!
A major refactoring of the python.d plugin has resulted in a dramatic decrease of the required memory, making Netdata even more resource efficient.
We also added charts for IPC shared memory segments and total memory used.

Feb 28th, 2019 - Netdata v1.12.2 released!
Patch release 1.12.2 contains 7 bug fixes and 4 improvements.
The main motivation behind a new patch release is the introduction of a stable release channel.
A ""stable"" installation and update channel was always on our roadmap, but it became a necessity when we realized that our users in China could not use the nightly releases published on Google Cloud. The ""stable"" channel is based on our official GitHub releases and uses assets hosted on GitHub.
We are also introducing a new Oracle DB collector module, implemented in Python.

Feb 21st, 2019 - Netdata v1.12.1 released!
Patch release 1.12.1 contains 22 bug fixes and 8 improvements.

Feb 14th, 2019 - Netdata v1.12.0 released!
Release 1.12 is made out of 211 pull requests and 22 bug fixes.
The key improvements are:

Introducing netdata.cloud, the free Netdata service for all Netdata users
High performance plugins with go.d.plugin (data collection orchestrator written in Go)
7 new data collectors and 11 rewrites of existing data collectors for improved performance
A new management API for all Netdata servers
Bind different functions of the Netdata APIs to different ports
Improved installation and updates


Nov 22nd, 2018 - Netdata v1.11.1 released!

Improved internal database to support values above 64bit.
New data collection plugins: openldap, tor, nvidia_smi.
Improved data collection plugins: Netdata now supports monitoring network interface aliases, smartd_log, cpufreq, sensors.
Health monitoring improvements: network interface congestion alarm restored, alerta.io, conntrack_max.
my-netdatamenu has been refactored.
Packaging: openrc service definition got a few improvements.


Sep 18, 2018 - Netdata has its own organization
Netdata used to be a firehol.org project, accessible as firehol/netdata.
Netdata now has its own github organization netdata, so all github URLs are now netdata/netdata. The old github URLs, repo clones, forks, etc redirect automatically to the new repo.
How it works
Netdata is a highly efficient, highly modular, metrics management engine. Its lockless design makes it ideal for concurrent operations on the metrics.

This is how it works:



Function
Description
Documentation




Collect
Multiple independent data collection workers are collecting metrics from their sources using the optimal protocol for each application and push the metrics to the database. Each data collection worker has lockless write access to the metrics it collects.
collectors


Store
Metrics are stored in RAM in a round robin database (ring buffer), using a custom made floating point number for minimal footprint.
database


Check
A lockless independent watchdog is evaluating health checks on the collected metrics, triggers alarms, maintains a health transaction log and dispatches alarm notifications.
health


Stream
An lockless independent worker is streaming metrics, in full detail and in real-time, to remote Netdata servers, as soon as they are collected.
streaming


Archive
A lockless independent worker is down-sampling the metrics and pushes them to backend time-series databases.
backends


Query
Multiple independent workers are attached to the internal web server, servicing API requests, including data queries.
web/api



The result is a highly efficient, low latency system, supporting multiple readers and one writer on each metric.
Infographic
This is a high level overview of Netdata feature set and architecture.
Click it to to interact with it (it has direct links to documentation).

Features

This is what you should expect from Netdata:
General

1s granularity - the highest possible resolution for all metrics.
Unlimited metrics - collects all the available metrics, the more the better.
1% CPU utilization of a single core - it is super fast, unbelievably optimized.
A few MB of RAM - by default it uses 25MB RAM. You size it.
Zero disk I/O - while it runs, it does not load or save anything (except error and access logs).
Zero configuration - auto-detects everything, it can collect up to 10000 metrics per server out of the box.
Zero maintenance - You just run it, it does the rest.
Zero dependencies - it is even its own web server, for its static web files and its web API (though its plugins may require additional libraries, depending on the applications monitored).
Scales to infinity - you can install it on all your servers, containers, VMs and IoTs. Metrics are not centralized by default, so there is no limit.
Several operating modes - Autonomous host monitoring (the default), headless data collector, forwarding proxy, store and forward proxy, central multi-host monitoring, in all possible configurations. Each node may have different metrics retention policy and run with or without health monitoring.

Health Monitoring & Alarms

Sophisticated alerting - comes with hundreds of alarms, out of the box! Supports dynamic thresholds, hysteresis, alarm templates, multiple role-based notification methods.
Notifications: alerta.io, amazon sns, discordapp.com, email, flock.com, hangouts, irc, kavenegar.com, messagebird.com, pagerduty.com, prowl, pushbullet.com, pushover.net, rocket.chat, slack.com, smstools3, syslog, telegram.org, twilio.com, web and custom notifications.

Integrations

time-series dbs - can archive its metrics to Graphite, OpenTSDB, Prometheus, AWS Kinesis, MongoDB, JSON document DBs, in the same or lower resolution (lower: to prevent it from congesting these servers due to the amount of data collected). Netdata also supports Prometheus remote write API which allows storing metrics to Elasticsearch, Gnocchi, InfluxDB, Kafka, PostgreSQL/TimescaleDB, Splunk, VictoriaMetrics, and a lot of other storage providers.

Visualization

Stunning interactive dashboards - mouse, touchpad and touch-screen friendly in 2 themes: slate (dark) and white.
Amazingly fast visualization - responds to all queries in less than 1 ms per metric, even on low-end hardware.
Visual anomaly detection - the dashboards are optimized for detecting anomalies visually.
Embeddable - its charts can be embedded on your web pages, wikis and blogs. You can even use Atlassian's Confluence as a monitoring dashboard.
Customizable - custom dashboards can be built using simple HTML (no javascript necessary).

Positive and negative values
To improve clarity on charts, Netdata dashboards present positive values for metrics representing read, input, inbound, received and negative values for metrics representing write, output, outbound, sent.

Netdata charts showing the bandwidth and packets of a network interface. received is positive and sent is negative.
Autoscaled y-axis
Netdata charts automatically zoom vertically, to visualize the variation of each metric within the visible time-frame.

A zero based stacked chart, automatically switches to an auto-scaled area chart when a single dimension is selected.
Charts are synchronized
Charts on Netdata dashboards are synchronized to each other. There is no master chart. Any chart can be panned or zoomed at any time, and all other charts will follow.

Charts are panned by dragging them with the mouse. Charts can be zoomed in/out withSHIFT + mouse wheel while the mouse pointer is over a chart.

The visible time-frame (pan and zoom) is propagated from Netdata server to Netdata server, when navigating via the node menu.

Highlighted time-frame
To improve visual anomaly detection across charts, the user can highlight a time-frame (by pressing ALT + mouse selection) on all charts.

A highlighted time-frame can be given by pressing ALT + mouse selection on any chart. Netdata will highlight the same range on all charts.

Highlighted ranges are propagated from Netdata server to Netdata server, when navigating via the node menu.

What does it monitor
Netdata data collection is extensible - you can monitor anything you can get a metric for.
Its Plugin API supports all programing languages (anything can be a Netdata plugin, BASH, python, perl, node.js, java, Go, ruby, etc).

For better performance, most system related plugins (cpu, memory, disks, filesystems, networking, etc) have been written in C.
For faster development and easier contributions, most application related plugins (databases, web servers, etc) have been written in python.

APM (Application Performance Monitoring)

statsd - Netdata is a fully featured statsd server.
Go expvar - collects metrics exposed by applications written in the Go programming language using the expvar package.
Spring Boot - monitors running Java Spring Boot applications that expose their metrics with the use of the Spring Boot Actuator included in Spring Boot library.
uWSGI - collects performance metrics from uWSGI applications.

System Resources

CPU Utilization - total and per core CPU usage.
Interrupts - total and per core CPU interrupts.
SoftIRQs - total and per core SoftIRQs.
SoftNet - total and per core SoftIRQs related to network activity.
CPU Throttling - collects per core CPU throttling.
CPU Frequency - collects the current CPU frequency.
CPU Idle - collects the time spent per processor state.
IdleJitter - measures CPU latency.
Entropy - random numbers pool, using in cryptography.
Interprocess Communication - IPC - such as semaphores and semaphores arrays.

Memory

ram - collects info about RAM usage.
swap - collects info about swap memory usage.
available memory - collects the amount of RAM available for userspace processes.
committed memory - collects the amount of RAM committed to userspace processes.
Page Faults - collects the system page faults (major and minor).
writeback memory - collects the system dirty memory and writeback activity.
huge pages - collects the amount of RAM used for huge pages.
KSM - collects info about Kernel Same Merging (memory dedupper).
Numa - collects Numa info on systems that support it.
slab - collects info about the Linux kernel memory usage.

Disks

block devices - per disk: I/O, operations, backlog, utilization, space, etc.
BCACHE - detailed performance of SSD caching devices.
DiskSpace - monitors disk space usage.
mdstat - software RAID.
hddtemp - disk temperatures.
smartd - disk S.M.A.R.T. values.
device mapper - naming disks.
Veritas Volume Manager - naming disks.
megacli - adapter, physical drives and battery stats.
adaptec_raid -  logical and physical devices health metrics.
ioping - to measure disk read/write latency.

Filesystems

BTRFS - detailed disk space allocation and usage.
Ceph - OSD usage, Pool usage, number of objects, etc.
NFS file servers and clients - NFS v2, v3, v4: I/O, cache, read ahead, RPC calls
Samba - performance metrics of Samba SMB2 file sharing.
ZFS - detailed performance and resource usage.

Networking

Network Stack - everything about the networking stack (both IPv4 and IPv6 for all protocols: TCP, UDP, SCTP, UDPLite, ICMP, Multicast, Broadcast, etc), and all network interfaces (per interface: bandwidth, packets, errors, drops).
Netfilter - everything about the netfilter connection tracker.
SynProxy - collects performance data about the linux SYNPROXY (DDoS).
NFacct - collects accounting data from iptables.
Network QoS - the only tool that visualizes network tc classes in real-time.
FPing - to measure latency and packet loss between any number of hosts.
ISC dhcpd - pools utilization, leases, etc.
AP - collects Linux access point performance data (hostapd).
SNMP - SNMP devices can be monitored too (although you will need to configure these).
port_check - checks TCP ports for availability and response time.

Virtual Private Networks

OpenVPN - collects status per tunnel.
LibreSwan - collects metrics per IPSEC tunnel.
Tor - collects Tor traffic statistics.

Processes

System Processes - running, blocked, forks, active.
Applications - by grouping the process tree and reporting CPU, memory, disk reads, disk writes, swap, threads, pipes, sockets - per process group.
systemd - monitors systemd services using CGROUPS.

Users

Users and User Groups resource usage - by summarizing the process tree per user and group, reporting: CPU, memory, disk reads, disk writes, swap, threads, pipes, sockets.
logind - collects sessions, users and seats connected.

Containers and VMs

Containers - collects resource usage for all kinds of containers, using CGROUPS (systemd-nspawn, lxc, lxd, docker, kubernetes, etc).
libvirt VMs - collects resource usage for all kinds of VMs, using CGROUPS.
dockerd - collects docker health metrics.

Web Servers

Apache and lighttpd - mod-status (v2.2, v2.4) and cache log statistics, for multiple servers.
IPFS - bandwidth, peers.
LiteSpeed - reads the litespeed rtreport files to collect metrics.
Nginx - stub-status, for multiple servers.
Nginx+ - connects to multiple nginx_plus servers (local or remote) to collect real-time performance metrics.
PHP-FPM - multiple instances, each reporting connections, requests, performance, etc.
Tomcat - accesses, threads, free memory, volume, etc.
web server access.log files - extracting in real-time, web server and proxy performance metrics and applying several health checks, etc.
HTTP check - checks one or more web servers for HTTP status code and returned content.

Proxies, Balancers, Accelerators

HAproxy - bandwidth, sessions, backends, etc.
Squid - multiple servers, each showing: clients bandwidth and requests, servers bandwidth and requests.
Traefik - connects to multiple traefik instances (local or remote) to collect API metrics (response status code, response time, average response time and server uptime).
Varnish - threads, sessions, hits, objects, backends, etc.
IPVS - collects metrics from the Linux IPVS load balancer.

Database Servers

CouchDB - reads/writes, request methods, status codes, tasks, replication, per-db, etc.
MemCached - multiple servers, each showing: bandwidth, connections, items, etc.
MongoDB - operations, clients, transactions, cursors, connections, asserts, locks, etc.
MySQL and mariadb - multiple servers, each showing: bandwidth, queries/s, handlers, locks, issues, tmp operations, connections, binlog metrics, threads, innodb metrics, and more.
PostgreSQL - multiple servers, each showing: per database statistics (connections, tuples read - written - returned, transactions, locks), backend processes, indexes, tables, write ahead, background writer and more.
Proxy SQL - collects Proxy SQL backend and frontend performance metrics.
Redis - multiple servers, each showing: operations, hit rate, memory, keys, clients, slaves.
RethinkDB - connects to multiple rethinkdb servers (local or remote) to collect real-time metrics.

Message Brokers

beanstalkd - global and per tube monitoring.
RabbitMQ - performance and health metrics.

Search and Indexing

ElasticSearch - search and index performance, latency, timings, cluster statistics, threads statistics, etc.

DNS Servers

bind_rndc - parses named.stats dump file to collect real-time performance metrics. All versions of bind after 9.6 are supported.
dnsdist - performance and health metrics.
ISC Bind (named) - multiple servers, each showing: clients, requests, queries, updates, failures and several per view metrics. All versions of bind after 9.9.10 are supported.
NSD - queries, zones, protocols, query types, transfers, etc.
PowerDNS - queries, answers, cache, latency, etc.
unbound - performance and resource usage metrics.
dns_query_time - DNS query time statistics.

Time Servers

chrony - uses the chronyc command to collect chrony statistics (Frequency, Last offset, RMS offset, Residual freq, Root delay, Root dispersion, Skew, System time).
ntpd - connects to multiple ntpd servers (local or remote) to provide statistics of system variables and optional also peer variables.

Mail Servers

Dovecot - POP3/IMAP servers.
Exim - message queue (emails queued).
Postfix - message queue (entries, size).

Hardware Sensors

IPMI - enterprise hardware sensors and events.
lm-sensors - temperature, voltage, fans, power, humidity, etc.
Nvidia - collects information for Nvidia GPUs.
RPi - Raspberry Pi temperature sensors.
w1sensor - collects data from connected 1-Wire sensors.

UPSes

apcupsd - load, charge, battery voltage, temperature, utility metrics, output metrics.
NUT - load, charge, battery voltage, temperature, utility metrics, output metrics.
Linux Power Supply - collects metrics reported by power supply drivers on Linux.

Social Sharing Servers

RetroShare - connects to multiple retroshare servers (local or remote) to collect real-time performance metrics.

Security

Fail2Ban - monitors the fail2ban log file to check all bans for all active jails.

Authentication, Authorization, Accounting (AAA, RADIUS, LDAP) Servers

FreeRadius - uses the radclient command to provide freeradius statistics (authentication, accounting, proxy-authentication, proxy-accounting).

Telephony Servers

opensips - connects to an opensips server (localhost only) to collect real-time performance metrics.

Household Appliances

SMA webbox - connects to multiple remote SMA webboxes to collect real-time performance metrics of the photovoltaic (solar) power generation.
Fronius - connects to multiple remote Fronius Symo servers to collect real-time performance metrics of the photovoltaic (solar) power generation.
StiebelEltron - collects the temperatures and other metrics from your Stiebel Eltron heating system using their Internet Service Gateway (ISG web).

Game Servers

SpigotMC - monitors Spigot Minecraft server ticks per second and number of online players using the Minecraft remote console.

Distributed Computing

BOINC - monitors task states for local and remote BOINC client software using the remote GUI RPC interface. Also provides alarms for a handful of error conditions.

Media Streaming Servers

IceCast - collects the number of listeners for active sources.

Monitoring Systems

Monit - collects metrics about monit targets (filesystems, applications, networks).

Provisioning Systems

Puppet - connects to multiple Puppet Server and Puppet DB instances (local or remote) to collect real-time status metrics.

You can easily extend Netdata, by writing plugins that collect data from any source, using any computer language.

Documentation
The Netdata documentation is at https://docs.netdata.cloud. But you can also find it inside the repo, so by just navigating the repo on github you can find all the documentation.
Here is a quick list:



Directory
Description




installer
Instructions to install Netdata on your systems.


docker
Instructions to install Netdata using docker.


daemon
Information about the Netdata daemon and its configuration.


collectors
Information about data collection plugins.


health
How Netdata's health monitoring works, how to create your own alarms and how to configure alarm notification methods.


streaming
How to build hierarchies of Netdata servers, by streaming metrics between them.


backends
Long term archiving of metrics to industry standard time-series databases, like prometheus, graphite, opentsdb.


web/api
Learn how to query the Netdata API and the queries it supports.


web/api/badges
Learn how to generate badges (SVG images) from live data.


web/gui/custom
Learn how to create custom Netdata dashboards.


web/gui/confluence
Learn how to create Netdata dashboards on Atlassian's Confluence.



You can also check all the other directories. Most of them have plenty of documentation.
Community
We welcome contributions. So, feel free to join the team.
To report bugs, or get help, use GitHub Issues.
You can also find Netdata on:

Facebook
Twitter
OpenHub
Repology
StackShare

License
Netdata is GPLv3+.
Netdata re-distributes other open-source tools and libraries. Please check the third party licenses.
Is it any good?
Yes.
When people first hear about a new product, they frequently ask if it is any good. A Hacker News user remarked:

Note to self: Starting immediately, all raganwald projects will have a “Is it any good?” section in the readme, and the answer shall be “yes."".

So, we follow the tradition...
Is it awesome?
These people seem to like it.

"
1,"

















    Visit https://nextjs.org/learn to get started with Next.js.
  


The below readme is the documentation for the canary (prerelease) branch. To view the documentation for the latest stable Next.js version visit nextjs.org/docs.


How to use

Setup

Quick Start
Manual Setup


Automatic code splitting
CSS

Built-in CSS support
CSS-in-JS
Importing CSS / Sass / Less / Stylus files


Static file serving (e.g.: images)
Dynamic Routing
Populating <head>
Fetching data and component lifecycle
Routing

With <Link>

With URL object
Replace instead of push url
Using a component that supports onClick
Forcing the Link to expose href to its child
Disabling the scroll changes to top on page


Imperatively
Intercepting popstate

With URL object
Router Events
Shallow Routing


useRouter
Using a Higher Order Component


Prefetching Pages

With <Link>
Imperatively


API Routes

Dynamic routes support
API Middlewares
Helper Functions


Custom server and routing

Disabling file-system routing
Dynamic assetPrefix
Changing x-powered-by


Dynamic Import

Basic Usage (Also does SSR)
With named exports
With Custom Loading Component
With No SSR


Custom <App>
Custom <Document>

Customizing renderPage


Custom error handling
Reusing the built-in error page
Custom configuration

Setting a custom build directory
Disabling etag generation
Configuring the onDemandEntries
Configuring extensions looked for when resolving pages in pages
Configuring the build ID
Configuring next process script


Customizing webpack config
Customizing babel config
Exposing configuration to the server / client side

Build-time configuration
Runtime configuration


Starting the server on alternative hostname
CDN support with Asset Prefix


Automatic Static Optimization
Automatic Static Optimization Indicator
Production deployment

Compression
Serverless deployment

One Level Lower
Summary




Browser support
TypeScript

Exported types


AMP Support

Enabling AMP Support
AMP First Page
Hybrid AMP Page
AMP Page Modes
AMP Behavior with next export
Adding AMP Components
AMP Validation
TypeScript Support


Static HTML export

Usage
Limitation


Multi Zones

How to define a zone
How to merge them


FAQ
Contributing
Authors

How to use
Setup
Quick Start
npx create-next-app
(npx comes with npm 5.2+ and higher, see instructions for older npm versions)
Manual Setup
Install it in your project:
npm install --save next react react-dom
and add a script to your package.json like this:
{
  ""scripts"": {
    ""dev"": ""next"",
    ""build"": ""next build"",
    ""start"": ""next start""
  }
}
After that, the file-system is the main API. Every .js file becomes a route that gets automatically processed and rendered.
Populate ./pages/index.js inside your project:
function Home() {
  return <div>Welcome to Next.js!</div>
}

export default Home
and then just run npm run dev and go to http://localhost:3000. To use another port, you can run npm run dev -- -p <your port here>.
So far, we get:

Automatic transpilation and bundling (with webpack and babel)
Hot code reloading
Server rendering and indexing of ./pages/
Static file serving. ./public/ is mapped to / (given you create a ./public/ directory inside your project)

Automatic code splitting
Every import you declare gets bundled and served with each page. That means pages never load unnecessary code!
import cowsay from 'cowsay-browser'

function CowsayHi() {
  return <pre>{cowsay.say({ text: 'hi there!' })}</pre>
}

export default CowsayHi
CSS
Built-in CSS support

Examples

Basic css


We bundle styled-jsx to provide support for isolated scoped CSS. The aim is to support ""shadow CSS"" similar to Web Components, which unfortunately do not support server-rendering and are JS-only.
function HelloWorld() {
  return (
    <div>
      Hello world
      <p>scoped!</p>
      <style jsx>{`
        p {
          color: blue;
        }
        div {
          background: red;
        }
        @media (max-width: 600px) {
          div {
            background: blue;
          }
        }
      `}</style>
      <style global jsx>{`
        body {
          background: black;
        }
      `}</style>
    </div>
  )
}

export default HelloWorld
Please see the styled-jsx documentation for more examples.
CSS-in-JS


Examples


Styled components
Styletron
Glamor
Cxs
Aphrodite
Fela


It's possible to use any existing CSS-in-JS solution. The simplest one is inline styles:
function HiThere() {
  return <p style={{ color: 'red' }}>hi there</p>
}

export default HiThere
To use more sophisticated CSS-in-JS solutions, you typically have to implement style flushing for server-side rendering. We enable this by allowing you to define your own custom <Document> component that wraps each page.
Importing CSS / Sass / Less / Stylus files
To support importing .css, .scss, .less or .styl files you can use these modules, which configure sensible defaults for server rendered applications.

@zeit/next-css
@zeit/next-sass
@zeit/next-less
@zeit/next-stylus

Static file serving (e.g.: images)
Create a folder called public in your project root directory. From your code you can then reference those files starting from the baseURL /
function MyImage() {
  return <img src=""/my-image.png"" alt=""my image"" />
}

export default MyImage
Note: Don't name the public directory anything else. The name can't be changed and is the only directory that Next.js uses for serving static assets.
Dynamic Routing

Examples

Dynamic routing


Defining routes by using predefined paths is not always enough for complex applications, in Next.js you can add brackets to a page ([param]) to create a dynamic route (a.k.a. url slugs, pretty urls, et al).
Consider the following page pages/post/[pid].js:
import { useRouter } from 'next/router'

const Post = () => {
  const router = useRouter()
  const { pid } = router.query

  return <p>Post: {pid}</p>
}

export default Post
Any route like /post/1, /post/abc, etc will be matched by pages/post/[pid].js.
The matched path parameter will be sent as a query parameter to the page.
For example, the route /post/abc will have the following query object: { pid: 'abc' }.
Similarly, the route /post/abc?foo=bar will have the query object: { foo: 'bar', pid: 'abc' }.

Note: Multiple dynamic route segments work the same way.
For example, pages/post/[pid]/[comment].js would match /post/1/a-comment.
Its query object would be: { pid: '1', comment: 'a-comment' }.

A <Link> for /post/abc looks like so:
<Link href=""/post/[pid]"" as=""/post/abc"">
  <a>First Post</a>
</Link>

href: the path inside pages directory.
as: the path that will be rendered in the browser URL bar.

As href is a filesystem path, it shouldn't change at runtime, instead, you will probably need to change as
dynamically according to your needs. Here's an example to create a list of links:
const pids = ['id1', 'id2', 'id3']
{
  pids.map(pid => (
    <Link href=""/post/[pid]"" as={`/post/${pid}`}>
      <a>Post {pid}</a>
    </Link>
  ))
}

You can read more about <Link> here.

However, if a query and route param name are the same, route parameters will override the matching query params.
For example, /post/abc?pid=bcd will have the query object: { pid: 'abc' }.

Note: Predefined routes take precedence over dynamic routes.
For example, if you have pages/post/[pid].js and pages/post/create.js, the route /post/create will be matched by pages/post/create.js instead of the dynamic route ([pid]).


Note: Pages that are statically optimized by automatic static optimization will be hydrated without their route parameters provided (query will be empty, i.e. {}).
After hydration, Next.js will trigger an update to your application to provide the route parameters in the query object.
If your application cannot tolerate this behavior, you can opt-out of static optimization by capturing the query parameter in getInitialProps.


Note: If deploying to ZEIT Now dynamic routes will work out-of-the-box.
You do not need to configure custom routes in a now.json file.
If you are new to ZEIT Now, you can learn how to deploy a Next.js app to it in the Deploying a Next.js App Learn section.

Populating <head>

Examples

Head elements
Layout component


We expose a built-in component for appending elements to the <head> of the page.
import Head from 'next/head'

function IndexPage() {
  return (
    <div>
      <Head>
        <title>My page title</title>
        <meta name=""viewport"" content=""initial-scale=1.0, width=device-width"" />
      </Head>
      <p>Hello world!</p>
    </div>
  )
}

export default IndexPage
To avoid duplicate tags in your <head> you can use the key property, which will make sure the tag is only rendered once:
import Head from 'next/head'

function IndexPage() {
  return (
    <div>
      <Head>
        <title>My page title</title>
        <meta
          name=""viewport""
          content=""initial-scale=1.0, width=device-width""
          key=""viewport""
        />
      </Head>
      <Head>
        <meta
          name=""viewport""
          content=""initial-scale=1.2, width=device-width""
          key=""viewport""
        />
      </Head>
      <p>Hello world!</p>
    </div>
  )
}

export default IndexPage
In this case only the second <meta name=""viewport"" /> is rendered.
Note: The contents of <head> get cleared upon unmounting the component, so make sure each page completely defines what it needs in <head>, without making assumptions about what other pages added.
Note: <title> and <meta> elements need to be contained as direct children of the <Head> element, or wrapped into maximum one level of <React.Fragment>, otherwise the metatags won't be correctly picked up on clientside navigation.
Fetching data and component lifecycle

Examples

Data fetch


When you need state, lifecycle hooks or initial data population you can export a function component that uses Hooks or a class component.
Using a function component:
import fetch from 'isomorphic-unfetch'

function Page({ stars }) {
  return <div>Next stars: {stars}</div>
}

Page.getInitialProps = async ({ req }) => {
  const res = await fetch('https://api.github.com/repos/zeit/next.js')
  const json = await res.json()
  return { stars: json.stargazers_count }
}

export default Page
Using a class component:
import React from 'react'

class HelloUA extends React.Component {
  static async getInitialProps({ req }) {
    const userAgent = req ? req.headers['user-agent'] : navigator.userAgent
    return { userAgent }
  }

  render() {
    return <div>Hello World {this.props.userAgent}</div>
  }
}

export default HelloUA
Notice that to load data when the page loads, we use getInitialProps which is an async static method. It can asynchronously fetch anything that resolves to a JavaScript plain Object, which populates props.
Data returned from getInitialProps is serialized when server rendering, similar to a JSON.stringify. Make sure the returned object from getInitialProps is a plain Object and not using Date, Map or Set.
For the initial page load, getInitialProps will execute on the server only. getInitialProps will only be executed on the client when navigating to a different route via the Link component or using the routing APIs.



getInitialProps can not be used in children components. Only in pages.
If you are using some server only modules inside getInitialProps, make sure to import them properly, otherwise, it'll slow down your app.



getInitialProps receives a context object with the following properties:

pathname - path section of URL
query - query string section of URL parsed as an object
asPath - String of the actual path (including the query) shows in the browser
req - HTTP request object (server only)
res - HTTP response object (server only)
err - Error object if any error is encountered during the rendering

Routing
Next.js does not ship a routes manifest with every possible route in the application, so the current page is not aware of any other pages on the client side. All subsequent routes get lazy-loaded, for scalability sake.
With <Link>

Examples

Hello World


Client-side transitions between routes can be enabled via a <Link> component.

This component is not required for navigations to static pages that require a hard refresh, like when using AMP.

Basic Example
Consider these two pages:
// pages/index.js
import Link from 'next/link'

function Home() {
  return (
    <>
      <ul>
        <li>Home</li>
        <li>
          <Link href=""/about"">
            <a>About Us</a>
          </Link>
        </li>
      </ul>

      <h1>This is our homepage.</h1>
    </>
  )
}

export default Home
// pages/about.js
import Link from 'next/link'

function About() {
  return (
    <>
      <ul>
        <li>
          <Link href=""/"">
            <a>Home</a>
          </Link>
        </li>
        <li>About Us</li>
      </ul>

      <h1>About</h1>
      <p>We are a cool company.</p>
    </>
  )
}

export default About
Note: if passing a functional component as a child of <Link> you will need to wrap it in React.forwardRef
Example with React.forwardRef
import React from 'react'
import Link from 'next/link'

// `onClick`, `href`, and `ref` need to be passed to the DOM element
// for proper handling
const MyButton = React.forwardRef(({ onClick, href }, ref) => (
  <a href={href} onClick={onClick} ref={ref}>
    Click Me
  </a>
))

export default () => (
  <>
    <Link href=""/another"">
      <MyButton />
    </Link>
  </>
)
Custom routes (using props from URL)
If you find that your use case is not covered by Dynamic Routing then you can create a custom server and manually add dynamic routes.
Example:


Consider you have the URL /post/:slug.


You created pages/post.js:
import { useRouter } from 'next/router'

const Post = () => {
  const router = useRouter()
  const { slug } = router.query

  return <p>My Blog Post: {slug}</p>
}

export default Post


You add the route to express (or any other server) on server.js file (this is only for SSR). This will route the url /post/:slug to pages/post.js and provide slug as part of the query object to the page.
server.get('/post/:slug', (req, res) => {
  return app.render(req, res, '/post', { slug: req.params.slug })
})


For client side routing, use next/link:
<Link href=""/post?slug=something"" as=""/post/something"">

href: the path inside pages directory
as: the path used by your server routes



Client-side routing behaves exactly like the browser:

The component is fetched.
If it defines getInitialProps, data is fetched. If an error occurs, _error.js is rendered.
After 1 and 2 complete, pushState is performed and the new component is rendered.

To inject the pathname, query or asPath in your component, you can use the useRouter hook, or withRouter for class components.
With URL object

Examples

With URL Object Routing


The component <Link> can also receive a URL object and it will automatically format it to create the URL string.
// pages/index.js
import Link from 'next/link'

function Home() {
  return (
    <div>
      Click{' '}
      <Link href={{ pathname: '/about', query: { name: 'Zeit' } }}>
        <a>here</a>
      </Link>{' '}
      to read more
    </div>
  )
}

export default Home
That will generate the URL string /about?name=Zeit, you can use every property as defined in the Node.js URL module documentation.
Replace instead of push url
The default behaviour for the <Link> component is to push a new url into the stack. You can use the replace prop to prevent adding a new entry.
// pages/index.js
import Link from 'next/link'

function Home() {
  return (
    <div>
      Click{' '}
      <Link href=""/about"" replace>
        <a>here</a>
      </Link>{' '}
      to read more
    </div>
  )
}

export default Home
Using a component that supports onClick
<Link> supports any component that supports the onClick event. In case you don't provide an <a> tag, it will only add the onClick event handler and won't pass the href property.
// pages/index.js
import Link from 'next/link'

function Home() {
  return (
    <div>
      Click{' '}
      <Link href=""/about"">
        <img src=""/static/image.png"" alt=""image"" />
      </Link>
    </div>
  )
}

export default Home
Forcing the Link to expose href to its child
If child is an <a> tag and doesn't have a href attribute we specify it so that the repetition is not needed by the user. However, sometimes, you’ll want to pass an <a> tag inside of a wrapper and the Link won’t recognize it as a hyperlink, and, consequently, won’t transfer its href to the child. In cases like that, you should define a boolean passHref property to the Link, forcing it to expose its href property to the child.
Please note: using a tag other than a and failing to pass passHref may result in links that appear to navigate correctly, but, when being crawled by search engines, will not be recognized as links (owing to the lack of href attribute). This may result in negative effects on your site’s SEO.
import Link from 'next/link'
import Unexpected_A from 'third-library'

function NavLink({ href, name }) {
  return (
    <Link href={href} passHref>
      <Unexpected_A>{name}</Unexpected_A>
    </Link>
  )
}

export default NavLink
Disabling the scroll changes to top on page
The default behaviour of <Link> is to scroll to the top of the page. When there is a hash defined it will scroll to the specific id, just like a normal <a> tag. To prevent scrolling to the top / hash scroll={false} can be added to <Link>:
<Link scroll={false} href=""/?counter=10""><a>Disables scrolling</a></Link>
<Link href=""/?counter=10""><a>Changes with scrolling to top</a></Link>
Imperatively

Examples

Basic routing
With a page loading indicator


You can also do client-side page transitions using next/router:
import Router from 'next/router'

function ReadMore() {
  return (
    <div>
      Click <span onClick={() => Router.push('/about')}>here</span> to read more
    </div>
  )
}

export default ReadMore
Intercepting popstate
In some cases (for example, if using a custom router), you may wish
to listen to popstate and react before the router acts on it.
For example, you could use this to manipulate the request, or force an SSR refresh.
import Router from 'next/router'

Router.beforePopState(({ url, as, options }) => {
  // I only want to allow these two routes!
  if (as !== '/' && as !== '/other') {
    // Have SSR render bad routes as a 404.
    window.location.href = as
    return false
  }

  return true
})
If the function you pass into beforePopState returns false, Router will not handle popstate;
you'll be responsible for handling it, in that case.
See Disabling File-System Routing.
Above Router object comes with the following API:

route - String of the current route
pathname - String of the current path excluding the query string
query - Object with the parsed query string. Defaults to {}.
asPath - String of the actual path (including the query) shows in the browser
push(url, as=url) - performs a pushState call with the given url
replace(url, as=url) - performs a replaceState call with the given url
beforePopState(cb=function) - intercept popstate before router processes the event

The second as parameter for push and replace is an optional decoration of the URL. Useful if you configured custom routes on the server.
With URL object
You can use a URL object the same way you use it in a <Link> component to push and replace a URL.
import Router from 'next/router'

const handler = () => {
  Router.push({
    pathname: '/about',
    query: { name: 'Zeit' },
  })
}

function ReadMore() {
  return (
    <div>
      Click <span onClick={handler}>here</span> to read more
    </div>
  )
}

export default ReadMore
This uses the same exact parameters as in the <Link> component. The first parameter maps to href while the second parameter maps to as in the <Link> component as documented here.
Router Events
You can also listen to different events happening inside the Router.
Here's a list of supported events:

routeChangeStart(url) - Fires when a route starts to change
routeChangeComplete(url) - Fires when a route changed completely
routeChangeError(err, url) - Fires when there's an error when changing routes, or a route load is cancelled
beforeHistoryChange(url) - Fires just before changing the browser's history
hashChangeStart(url) - Fires when the hash will change but not the page
hashChangeComplete(url) - Fires when the hash has changed but not the page


Here url is the URL shown in the browser. If you call Router.push(url, as) (or similar), then the value of url will be as.

Here's how to properly listen to the router event routeChangeStart:
const handleRouteChange = url => {
  console.log('App is changing to: ', url)
}

Router.events.on('routeChangeStart', handleRouteChange)
If you no longer want to listen to that event, you can unsubscribe with the off method:
Router.events.off('routeChangeStart', handleRouteChange)
If a route load is cancelled (for example by clicking two links rapidly in succession), routeChangeError will fire. The passed err will contain a cancelled property set to true.
Router.events.on('routeChangeError', (err, url) => {
  if (err.cancelled) {
    console.log(`Route to ${url} was cancelled!`)
  }
})

Note: Using router events in getInitialProps is discouraged as it may result in unexpected behavior.
Router events should be registered when a component mounts (useEffect or componentDidMount/componentWillUnmount) or imperatively when an event happens.
useEffect(() => {
  const handleRouteChange = url => {
    console.log('App is changing to: ', url)
  }

  Router.events.on('routeChangeStart', handleRouteChange)
  return () => {
    Router.events.off('routeChangeStart', handleRouteChange)
  }
}, [])

Shallow Routing

Examples

Shallow Routing


Shallow routing allows you to change the URL without running getInitialProps. You'll receive the updated pathname and the query via the router prop (injected by using useRouter or withRouter), without losing state.
You can do this by invoking either Router.push or Router.replace with the shallow: true option. Here's an example:
// Current URL is ""/""
const href = '/?counter=10'
const as = href
Router.push(href, as, { shallow: true })
Now, the URL is updated to /?counter=10. You can see the updated URL with this.props.router.query inside the Component (make sure you are using withRouter around your Component to inject the router prop).
You can watch for URL changes via componentDidUpdate hook as shown below:
componentDidUpdate(prevProps) {
  const { pathname, query } = this.props.router
  // verify props have changed to avoid an infinite loop
  if (query.id !== prevProps.router.query.id) {
    // fetch data based on the new query
  }
}

NOTES:
Shallow routing works only for same page URL changes. For an example, let's assume we have another page called about, and you run this:
Router.push('/?counter=10', '/about?counter=10', { shallow: true })
Since that's a new page, it'll unload the current page, load the new one and call getInitialProps even though we asked to do shallow routing.

useRouter

Examples

Dynamic routing


If you want to access the router object inside any functional component in your app, you can use the useRouter hook, here's how to use it:
import { useRouter } from 'next/router'

export default function ActiveLink({ children, href }) {
  const router = useRouter()
  const style = {
    marginRight: 10,
    color: router.pathname === href ? 'red' : 'black',
  }

  const handleClick = e => {
    e.preventDefault()
    router.push(href)
  }

  return (
    <a href={href} onClick={handleClick} style={style}>
      {children}
    </a>
  )
}

Note: useRouter is a React hook, meaning it cannot be used with classes.
You can either use withRouter (a higher order component) or wrap your class in a functional component.

The above router object comes with an API similar to next/router.
Using a Higher Order Component

Examples

Using the `withRouter` utility


If useRouter is not the best fit for you, withRouter can also add the same router object to any component, here's how to use it:
import { withRouter } from 'next/router'

function Page({ router }) {
  return <p>{router.pathname}</p>
}

export default withRouter(Page)
Prefetching Pages
⚠️ This is a production only feature ⚠️

Examples

Prefetching


Next.js has an API which allows you to prefetch pages.
Since Next.js server-renders your pages, this allows all the future interaction paths of your app to be instant. Effectively Next.js gives you the great initial download performance of a website, with the ahead-of-time download capabilities of an app. Read more.

With prefetching Next.js only downloads JS code. When the page is getting rendered, you may need to wait for the data.


Automatic prefetching is disabled if your device is connected with 2G network or Save-Data header is on.


<link rel=""preload""> is used for prefetching. Sometimes browsers will show a warning if the resource is not used within 3 seconds, these warnings can be ignored as per https://github.com/zeit/next.js/issues/6517#issuecomment-469063892.

With <Link>
<Link> will automatically prefetch pages in the background as they appear in the view. If certain pages are rarely visited you can manually set prefetch to false, here's how:
<Link href=""/about"" prefetch={false}>
  <a>About</a>
</Link>
Imperatively
Most prefetching needs are addressed by <Link />, but we also expose an imperative API for advanced usage:
import { useRouter } from 'next/router'

export default function MyLink() {
  const router = useRouter()

  return (
    <>
      <a onClick={() => setTimeout(() => router.push('/dynamic'), 100)}>
        A route transition will happen after 100ms
      </a>
      {// and we can prefetch it!
      router.prefetch('/dynamic')}
    </>
  )
}
router methods should be only used inside the client side of your app though. In order to prevent any error regarding this subject use the imperatively prefetch method in the useEffect() hook:
import { useRouter } from 'next/router'

export default function MyLink() {
  const router = useRouter()

  useEffect(() => {
    router.prefetch('/dynamic')
  })

  return (
    <a onClick={() => setTimeout(() => router.push('/dynamic'), 100)}>
      A route transition will happen after 100ms
    </a>
  )
}
You can also add it to the componentDidMount() lifecycle method when using React.Component:
import React from 'react'
import { withRouter } from 'next/router'

class MyLink extends React.Component {
  componentDidMount() {
    const { router } = this.props
    router.prefetch('/dynamic')
  }

  render() {
    const { router } = this.props

    return (
      <a onClick={() => setTimeout(() => router.push('/dynamic'), 100)}>
        A route transition will happen after 100ms
      </a>
    )
  }
}

export default withRouter(MyLink)
API Routes

Examples

Basic API routes
API routes with micro
API routes with middleware
API routes with GraphQL server
API routes with REST


API routes provide a straightforward solution to build your API with Next.js.
Start by creating the api/ folder inside the ./pages/ folder.
Every file inside ./pages/api is mapped to /api/*.
For example, ./pages/api/posts.js is mapped to the route /api/posts.
Here's an example API route file:
export default (req, res) => {
  res.setHeader('Content-Type', 'application/json')
  res.statusCode = 200
  res.end(JSON.stringify({ name: 'Nextjs' }))
}


req refers to NextApiRequest which extends http.IncomingMessage


res refers to NextApiResponse which extends http.ServerResponse


For API routes there are built-in types NextApiRequest and NextApiResponse, which extend the Node.js request and response objects.
import { NextApiRequest, NextApiResponse } from 'next'

export default (req: NextApiRequest, res: NextApiResponse) => {
  res.status(200).json({ title: 'Next.js' })
}
To handle different HTTP methods for API calls you can access req.method in your resolver function:
export default (req, res) => {
  if (req.method === 'POST') {
    // Process your POST request
  } else {
    // Handle the rest of your HTTP methods
  }
}

Note: API Routes do not specify CORS headers, so they'll be same-origin only by default.
You can customize this behavior by wrapping your export with CORS middleware.
We provide an example of this below.

API Routes do not increase your client-side bundle size. They are server-side only bundles.
Dynamic routes support
API pages support dynamic routing, so you can use all benefits mentioned already above.
Consider the following page ./pages/api/post/[pid].js, here is how you get parameters inside the resolver method:
export default (req, res) => {
  const {
    query: { pid },
  } = req

  res.end(`Post: ${pid}`)
}
API Middlewares
API routes provides built in middlewares which parse the incoming req.
Those middlewares are:

req.cookies - an object containing the cookies sent by the request. Defaults to {}
req.query - an object containing the query string. Defaults to {}
req.body - an object containing the body parsed by content-type, or null if no body is sent

Body parsing is enabled by default with a size limit of 1mb for the parsed body.
You can opt-out of automatic body parsing if you need to consume it as a Stream:
// ./pages/api/my-endpoint.js
export default (req, res) => {
  // ...
}

export const config = {
  api: {
    bodyParser: false,
  },
}
You can adjust size of parsed body by adding sizeLimit key to bodyParser, supported values are by bytes library.
// ./pages/api/my-endpoint.js
export default (req, res) => {
  // ...
}

export const config = {
  api: {
    bodyParser: {
      sizeLimit: '1mb',
    },
  },
}
As an added bonus, you can also use any Micro compatible middleware!
For example, configuring CORS for your API endpoint can be done leveraging micro-cors.
First, install micro-cors:
npm i micro-cors
# or
yarn add micro-cors
Then, import micro-cors and configure it. Finally, wrap your exported function in the middleware:
import Cors from 'micro-cors'

const cors = Cors({
  allowMethods: ['GET', 'HEAD'],
})

function Endpoint(req, res) {
  res.json({ message: 'Hello Everyone!' })
}

export default cors(Endpoint)
Helper Functions
We're providing a set of Express.js-like methods to improve the developer experience and increase the speed of creating new API endpoints:
export default (req, res) => {
  res.status(200).json({ name: 'Next.js' })
}

res.status(code) - a function to set the status code. code must be a valid HTTP status code
res.json(json) - Sends a JSON response. json must be a valid JSON object
res.send(body) - Sends the HTTP response. body can be a string, an object or a Buffer

Custom server and routing

Examples

Basic custom server
Express integration
Hapi integration
Koa integration
SSR caching


Typically you start your next server with next start. It's possible, however, to start a server 100% programmatically in order to customize routes, use route patterns, etc.
When using a custom server with a server file, for example called server.js, make sure you update the scripts key in package.json to:
{
  ""scripts"": {
    ""dev"": ""node server.js"",
    ""build"": ""next build"",
    ""start"": ""NODE_ENV=production node server.js""
  }
}
This example makes /a resolve to ./pages/b, and /b resolve to ./pages/a:
// This file doesn't go through babel or webpack transformation.
// Make sure the syntax and sources this file requires are compatible with the current node version you are running
// See https://github.com/zeit/next.js/issues/1245 for discussions on Universal Webpack or universal Babel
const { createServer } = require('http')
const { parse } = require('url')
const next = require('next')

const dev = process.env.NODE_ENV !== 'production'
const app = next({ dev })
const handle = app.getRequestHandler()

app.prepare().then(() => {
  createServer((req, res) => {
    // Be sure to pass `true` as the second argument to `url.parse`.
    // This tells it to parse the query portion of the URL.
    const parsedUrl = parse(req.url, true)
    const { pathname, query } = parsedUrl

    if (pathname === '/a') {
      app.render(req, res, '/b', query)
    } else if (pathname === '/b') {
      app.render(req, res, '/a', query)
    } else {
      handle(req, res, parsedUrl)
    }
  }).listen(3000, err => {
    if (err) throw err
    console.log('> Ready on http://localhost:3000')
  })
})
The next API is as follows:

next(opts: object)

Supported options:

dev (bool) whether to launch Next.js in dev mode - default false
dir (string) where the Next project is located - default '.'
quiet (bool) Hide error messages containing server information - default false
conf (object) the same object you would use in next.config.js - default {}

Then, change your start script to NODE_ENV=production node server.js.
Disabling file-system routing
By default, Next will serve each file in /pages under a pathname matching the filename (eg, /pages/some-file.js is served at site.com/some-file.
If your project uses custom routing, this behavior may result in the same content being served from multiple paths, which can present problems with SEO and UX.
To disable this behavior & prevent routing based on files in /pages, simply set the following option in your next.config.js:
// next.config.js
module.exports = {
  useFileSystemPublicRoutes: false,
}
Note that useFileSystemPublicRoutes simply disables filename routes from SSR; client-side routing may still access those paths. If using this option, you should guard against navigation to routes you do not want programmatically.
You may also wish to configure the client-side Router to disallow client-side redirects to filename routes; please refer to Intercepting popstate.
Dynamic assetPrefix
Sometimes we need to set the assetPrefix dynamically. This is useful when changing the assetPrefix based on incoming requests.
For that, we can use app.setAssetPrefix.
Here's an example usage of it:
const next = require('next')
const http = require('http')

const dev = process.env.NODE_ENV !== 'production'
const app = next({ dev })
const handleNextRequests = app.getRequestHandler()

app.prepare().then(() => {
  const server = new http.Server((req, res) => {
    // Add assetPrefix support based on the hostname
    if (req.headers.host === 'my-app.com') {
      app.setAssetPrefix('http://cdn.com/myapp')
    } else {
      app.setAssetPrefix('')
    }

    handleNextRequests(req, res)
  })

  server.listen(port, err => {
    if (err) {
      throw err
    }

    console.log(`> Ready on http://localhost:${port}`)
  })
})
Changing x-powered-by
By default Next.js will add x-powered-by to the request headers. There's an optional way to opt-out of this:
// next.config.js
module.exports = {
  poweredByHeader: false,
}
Dynamic Import

Examples

With Dynamic Import


Next.js supports ES2020 dynamic import() for JavaScript.
With that, you could import JavaScript modules (inc. React Components) dynamically and work with them.
You can think dynamic imports as another way to split your code into manageable chunks.
Since Next.js supports dynamic imports with SSR, you could do amazing things with it.
Here are a few ways to use dynamic imports.
Basic Usage (Also does SSR)
import dynamic from 'next/dynamic'

const DynamicComponent = dynamic(() => import('../components/hello'))

function Home() {
  return (
    <div>
      <Header />
      <DynamicComponent />
      <p>HOME PAGE is here!</p>
    </div>
  )
}

export default Home
With named exports
// components/hello.js
export function Hello() {
  return <p>Hello!</p>
}
import dynamic from 'next/dynamic'

const DynamicComponent = dynamic(() =>
  import('../components/hello').then(mod => mod.Hello)
)

function Home() {
  return (
    <div>
      <Header />
      <DynamicComponent />
      <p>HOME PAGE is here!</p>
    </div>
  )
}

export default Home
With Custom Loading Component
import dynamic from 'next/dynamic'

const DynamicComponentWithCustomLoading = dynamic(
  () => import('../components/hello2'),
  { loading: () => <p>...</p> }
)

function Home() {
  return (
    <div>
      <Header />
      <DynamicComponentWithCustomLoading />
      <p>HOME PAGE is here!</p>
    </div>
  )
}

export default Home
With No SSR
import dynamic from 'next/dynamic'

const DynamicComponentWithNoSSR = dynamic(
  () => import('../components/hello3'),
  { ssr: false }
)

function Home() {
  return (
    <div>
      <Header />
      <DynamicComponentWithNoSSR />
      <p>HOME PAGE is here!</p>
    </div>
  )
}

export default Home
Custom <App>

Examples

Using `_app.js` for layout
Using `_app.js` to override `componentDidCatch`


Next.js uses the App component to initialize pages. You can override it and control the page initialization. Which allows you to do amazing things like:

Persisting layout between page changes
Keeping state when navigating pages
Inject additional data into pages (for example by processing GraphQL queries)

To override, create the ./pages/_app.js file and override the App class as shown below:
function MyApp({ Component, pageProps }) {
  return <Component {...pageProps} />
}

// Only uncomment this method if you have blocking data requirements for
// every single page in your application. This disables the ability to
// perform automatic static optimization, causing every page in your app to
// be server-side rendered.
//
// MyApp.getInitialProps = async (appContext) => {
//   // calls page's `getInitialProps` and fills `appProps.pageProps`
//   const appProps = await App.getInitialProps(appContext);
//
//   return { ...appProps }
// }

export default MyApp

Note: Adding a custom getInitialProps in App will affect Automatic Static Optimization

Custom <Document>

Examples

Styled components custom document


A custom <Document> is commonly used to augment your application's <html> and <body> tags.
This is necessary because Next.js pages skip the definition of the surrounding document's markup.
This allows you to support Server-Side Rendering for CSS-in-JS libraries like
styled-components or emotion.
Note, styled-jsx is included in Next.js by default.
A custom <Document> can also include getInitialProps for expressing asynchronous server-rendering data requirements.

Note: <Document>'s getInitialProps function is not called during client-side transitions,
nor when a page is automatically statically optimized.


Note: Make sure to check if ctx.req / ctx.res are defined in getInitialProps.
These variables will be undefined when a page is being statically exported for next export or automatic static optimization.

To use a custom <Document>, you must create a file at ./pages/_document.js and extend the Document class:
// _document is only rendered on the server side and not on the client side
// Event handlers like onClick can't be added to this file

// ./pages/_document.js
import Document, { Html, Head, Main, NextScript } from 'next/document'

class MyDocument extends Document {
  static async getInitialProps(ctx) {
    const initialProps = await Document.getInitialProps(ctx)
    return { ...initialProps }
  }

  render() {
    return (
      <Html>
        <Head />
        <body>
          <Main />
          <NextScript />
        </body>
      </Html>
    )
  }
}

export default MyDocument
All of <Html>, <Head />, <Main /> and <NextScript /> are required for page to be properly rendered.
Note: React-components outside of <Main /> will not be initialised by the browser. Do not add application logic here. If you need shared components in all your pages (like a menu or a toolbar), take a look at the <App> component instead.
The ctx object is equivalent to the one received in all getInitialProps hooks, with one addition:

renderPage (Function) a callback that executes the actual React rendering logic (synchronously). It's useful to decorate this function in order to support server-rendering wrappers like Aphrodite's renderStatic.

Customizing renderPage
🚧 It should be noted that the only reason you should be customizing renderPage is for usage with css-in-js libraries
that need to wrap the application to properly work with server-rendering. 🚧

It takes as argument an options object for further customization:

import Document from 'next/document'

class MyDocument extends Document {
  static async getInitialProps(ctx) {
    const originalRenderPage = ctx.renderPage

    ctx.renderPage = () =>
      originalRenderPage({
        // useful for wrapping the whole react tree
        enhanceApp: App => App,
        // useful for wrapping in a per-page basis
        enhanceComponent: Component => Component,
      })

    // Run the parent `getInitialProps` using `ctx` that now includes our custom `renderPage`
    const initialProps = await Document.getInitialProps(ctx)

    return initialProps
  }
}

export default MyDocument
Custom error handling
404 or 500 errors are handled both client and server side by a default component error.js. If you wish to override it, define a _error.js in the pages folder:
⚠️ The pages/_error.js component is only used in production. In development you get an error with call stack to know where the error originated from. ⚠️
import React from 'react'

function Error({ statusCode }) {
  return (
    <p>
      {statusCode
        ? `An error ${statusCode} occurred on server`
        : 'An error occurred on client'}
    </p>
  )
}

Error.getInitialProps = ({ res, err }) => {
  const statusCode = res ? res.statusCode : err ? err.statusCode : 404
  return { statusCode }
}

export default Error
Reusing the built-in error page
If you want to render the built-in error page you can by using next/error:
import React from 'react'
import Error from 'next/error'
import fetch from 'isomorphic-unfetch'

const Page = ({ errorCode, stars }) => {
  if (errorCode) {
    return <Error statusCode={errorCode} />
  }

  return <div>Next stars: {stars}</div>
}

Page.getInitialProps = async () => {
  const res = await fetch('https://api.github.com/repos/zeit/next.js')
  const errorCode = res.statusCode > 200 ? res.statusCode : false
  const json = await res.json()

  return { errorCode, stars: json.stargazers_count }
}

export default Page

If you have created a custom error page you have to import your own _error component from ./_error instead of next/error.

The Error component also takes title as a property if you want to pass in a text message along with a statusCode.
Custom configuration
For custom advanced behavior of Next.js, you can create a next.config.js in the root of your project directory (next to pages/ and package.json).
Note: next.config.js is a regular Node.js module, not a JSON file. It gets used by the Next server and build phases, and not included in the browser build.
// next.config.js
module.exports = {
  /* config options here */
}
Or use a function:
module.exports = (phase, { defaultConfig }) => {
  return {
    /* config options here */
  }
}
phase is the current context in which the configuration is loaded. You can see all phases here: constants
Phases can be imported from next/constants:
const { PHASE_DEVELOPMENT_SERVER } = require('next/constants')
module.exports = (phase, { defaultConfig }) => {
  if (phase === PHASE_DEVELOPMENT_SERVER) {
    return {
      /* development only config options here */
    }
  }

  return {
    /* config options for all phases except development here */
  }
}
Setting a custom build directory
You can specify a name to use for a custom build directory. For example, the following config will create a build folder instead of a .next folder. If no configuration is specified then next will create a .next folder.
// next.config.js
module.exports = {
  distDir: 'build',
}
Disabling etag generation
You can disable etag generation for HTML pages depending on your cache strategy. If no configuration is specified then Next will generate etags for every page.
// next.config.js
module.exports = {
  generateEtags: false,
}
Configuring the onDemandEntries
Next exposes some options that give you some control over how the server will dispose or keep in memories pages built:
module.exports = {
  onDemandEntries: {
    // period (in ms) where the server will keep pages in the buffer
    maxInactiveAge: 25 * 1000,
    // number of pages that should be kept simultaneously without being disposed
    pagesBufferLength: 2,
  },
}
This is development-only feature. If you want to cache SSR pages in production, please see SSR-caching example.
Configuring extensions looked for when resolving pages in pages
Aimed at modules like @next/mdx, that add support for pages ending with .mdx. pageExtensions allows you to configure the extensions looked for in the pages directory when resolving pages.
// next.config.js
module.exports = {
  pageExtensions: ['mdx', 'jsx', 'js'],
}
Configuring the build ID
Next.js uses a constant generated at build time to identify which version of your application is being served. This can cause problems in multi-server deployments when next build is ran on every server. In order to keep a static build id between builds you can provide the generateBuildId function:
// next.config.js
module.exports = {
  generateBuildId: async () => {
    // For example get the latest git commit hash here
    return 'my-build-id'
  },
}
To fall back to the default of generating a unique id return null from the function:
module.exports = {
  generateBuildId: async () => {
    // When process.env.YOUR_BUILD_ID is undefined we fall back to the default
    if (process.env.YOUR_BUILD_ID) {
      return process.env.YOUR_BUILD_ID
    }

    return null
  },
}
Configuring next process script
You can pass any node arguments to next CLI command.
NODE_OPTIONS=""--throw-deprecation"" next
NODE_OPTIONS=""-r esm"" next
NODE_OPTIONS=""--inspect"" next
Customizing webpack config

Examples

Custom webpack bundle analyzer


Some commonly asked for features are available as modules:

@zeit/next-css
@zeit/next-sass
@zeit/next-less
@zeit/next-preact
@next/mdx


Warning: The webpack function is executed twice, once for the server and once for the client. This allows you to distinguish between client and server configuration using the isServer property.

Multiple configurations can be combined together with function composition. For example:
const withMDX = require('@next/mdx')
const withSass = require('@zeit/next-sass')

module.exports = withMDX(
  withSass({
    webpack(config, options) {
      // Further custom configuration here
      return config
    },
  })
)
In order to extend our usage of webpack, you can define a function that extends its config via next.config.js.
// next.config.js is not transformed by Babel. So you can only use javascript features supported by your version of Node.js.

module.exports = {
  webpack: (config, { buildId, dev, isServer, defaultLoaders, webpack }) => {
    // Note: we provide webpack above so you should not `require` it
    // Perform customizations to webpack config
    // Important: return the modified config

    // Example using webpack option
    config.plugins.push(new webpack.IgnorePlugin(/\/__tests__\//))
    return config
  },
  webpackDevMiddleware: config => {
    // Perform customizations to webpack dev middleware config
    // Important: return the modified config
    return config
  },
}
The second argument to webpack is an object containing properties useful when customizing its configuration:

buildId - String the build id used as a unique identifier between builds
dev - Boolean shows if the compilation is done in development mode
isServer - Boolean shows if the resulting configuration will be used for server side (true), or client side compilation (false)
defaultLoaders - Object Holds loader objects Next.js uses internally, so that you can use them in custom configuration

babel - Object the babel-loader configuration for Next.js



Example usage of defaultLoaders.babel:
// Example next.config.js for adding a loader that depends on babel-loader
// This source was taken from the @next/mdx plugin source:
// https://github.com/zeit/next.js/tree/canary/packages/next-mdx
module.exports = {
  webpack: (config, options) => {
    config.module.rules.push({
      test: /\.mdx/,
      use: [
        options.defaultLoaders.babel,
        {
          loader: '@mdx-js/loader',
          options: pluginOptions.options,
        },
      ],
    })

    return config
  },
}
Customizing babel config

Examples

Custom babel configuration


In order to extend our usage of babel, you can simply define a .babelrc file at the root of your app. This file is optional.
If found, we're going to consider it the source of truth, therefore it needs to define what next needs as well, which is the next/babel preset.
This is designed so that you are not surprised by modifications we could make to the babel configurations.
Here's an example .babelrc file:
{
  ""presets"": [""next/babel""],
  ""plugins"": []
}
The next/babel preset includes everything needed to transpile React applications. This includes:

preset-env
preset-react
preset-typescript
plugin-proposal-class-properties
plugin-proposal-object-rest-spread
plugin-transform-runtime
styled-jsx

These presets / plugins should not be added to your custom .babelrc. Instead, you can configure them on the next/babel preset:
{
  ""presets"": [
    [
      ""next/babel"",
      {
        ""preset-env"": {},
        ""transform-runtime"": {},
        ""styled-jsx"": {},
        ""class-properties"": {}
      }
    ]
  ],
  ""plugins"": []
}
The modules option on ""preset-env"" should be kept to false otherwise webpack code splitting is disabled.
Exposing configuration to the server / client side
There is a common need in applications to provide configuration values.
Next.js supports 2 ways of providing configuration:

Build-time configuration
Runtime configuration

Build-time configuration
The way build-time configuration works is by inlining the provided values into the Javascript bundle.
You can add the env key in next.config.js:
// next.config.js
module.exports = {
  env: {
    customKey: 'value',
  },
}
This will allow you to use process.env.customKey in your code. For example:
// pages/index.js
function Index() {
  return <h1>The value of customKey is: {process.env.customKey}</h1>
}

export default Index

Warning: Note that it is not possible to destructure process.env variables due to the webpack DefinePlugin replacing process.env.XXXX inline at build time.

// Will not work
const { CUSTOM_KEY, CUSTOM_SECRET } = process.env
AuthMethod({ key: CUSTOM_KEY, secret: CUSTOM_SECRET })

// Will work as replaced inline
AuthMethod({ key: process.env.CUSTOM_KEY, secret: process.env.CUSTOM_SECRET })
Runtime configuration

Warning: Note that these options are not available when using target: 'serverless'


Warning: Generally you want to use build-time configuration to provide your configuration.
The reason for this is that runtime configuration adds rendering / initialization overhead and is incompatible with automatic static optimization.

The next/config module gives your app access to the publicRuntimeConfig and serverRuntimeConfig stored in your next.config.js.
Place any server-only runtime config under a serverRuntimeConfig property.
Anything accessible to both client and server-side code should be under publicRuntimeConfig.

Note: A page that relies on publicRuntimeConfig must use getInitialProps to opt-out of automatic static optimization.
You can also de-optimize your entire application by creating a Custom <App> with getInitialProps.

// next.config.js
module.exports = {
  serverRuntimeConfig: {
    // Will only be available on the server side
    mySecret: 'secret',
    secondSecret: process.env.SECOND_SECRET, // Pass through env variables
  },
  publicRuntimeConfig: {
    // Will be available on both server and client
    staticFolder: '/static',
  },
}
// pages/index.js
import getConfig from 'next/config'
// Only holds serverRuntimeConfig and publicRuntimeConfig from next.config.js nothing else.
const { serverRuntimeConfig, publicRuntimeConfig } = getConfig()

console.log(serverRuntimeConfig.mySecret) // Will only be available on the server side
console.log(publicRuntimeConfig.staticFolder) // Will be available on both server and client

function MyImage() {
  return (
    <div>
      <img src={`${publicRuntimeConfig.staticFolder}/logo.png`} alt=""logo"" />
    </div>
  )
}

export default MyImage
Starting the server on alternative hostname
To start the development server using a different default hostname you can use --hostname hostname_here or -H hostname_here option with next dev. This will start a TCP server listening for connections on the provided host.
CDN support with Asset Prefix
To set up a CDN, you can set up the assetPrefix setting and configure your CDN's origin to resolve to the domain that Next.js is hosted on.
const isProd = process.env.NODE_ENV === 'production'
module.exports = {
  // You may only need to add assetPrefix in the production.
  assetPrefix: isProd ? 'https://cdn.mydomain.com' : '',
}
Note: Next.js will automatically use that prefix in the scripts it loads, but this has no effect whatsoever on /static. If you want to serve those assets over the CDN, you'll have to introduce the prefix yourself. One way of introducing a prefix that works inside your components and varies by environment is documented in this example.
If your CDN is on a separate domain and you would like assets to be requested using a CORS aware request you can set a config option for that.
// next.config.js
module.exports = {
  crossOrigin: 'anonymous',
}
Automatic Static Optimization
Next.js automatically determines that a page is static (can be prerendered) if it has no blocking data requirements.
This determination is made by the absence of getInitialProps in the page.
If getInitialProps is present, Next.js will not statically optimize the page.
Instead, Next.js will use its default behavior and render the page on-demand, per-request (meaning Server-Side Rendering).
If getInitialProps is absent, Next.js will statically optimize your page automatically by prerendering it to static HTML. During prerendering, the router's query object will be empty since we do not have query information to provide during this phase. Any query values will be populated client side after hydration.
This feature allows Next.js to emit hybrid applications that contain both server-rendered and statically generated pages.
This ensures Next.js always emits applications that are fast by default.

Note: Statically generated pages are still reactive: Next.js will hydrate your application client-side to give it full interactivity.

This feature provides many benefits.
For example, optimized pages require no server-side computation and can be instantly streamed to the end-user from CDN locations.
The result is an ultra fast loading experience for your users.
next build will emit .html files for statically optimized pages.
The result will be a file named .next/server/static/${BUILD_ID}/about.html instead of .next/server/static/${BUILD_ID}/about.js.
This behavior is similar for target: 'serverless'.
The built-in Next.js server (next start) and programmatic API (app.getRequestHandler()) both support this build output transparently.
There is no configuration or special handling required.

Note: If you have a custom <App> with getInitialProps then this optimization will be disabled.


Note: If you have a custom <Document> with getInitialProps be sure you check if ctx.req is defined before assuming the page is server-side rendered.
ctx.req will be undefined for pages that are prerendered.

Automatic Static Optimization Indicator
When a page qualifies for automatic static optimization we show an indicator to let you know.
This is helpful since the automatic static optimization can be very beneficial and knowing immediately in development if it qualifies can be useful.
See above for information on the benefits of this optimization.
In some cases this indicator might not be as useful like when working on electron applications. For these cases you can disable the indicator in your next.config.js by setting
module.exports = {
  devIndicators: {
    autoPrerender: false,
  },
}
Production deployment
To deploy, instead of running next, you want to build for production usage ahead of time. Therefore, building and starting are separate commands:
next build
next start
To deploy Next.js with ZEIT Now see the ZEIT Guide for Deploying Next.js or the Next.js Learn section about deploying on ZEIT Now.
Next.js can be deployed to other hosting solutions too. Please have a look at the 'Deployment' section of the wiki.
Note: NODE_ENV is properly configured by the next subcommands, if absent, to maximize performance. if you’re using Next.js programmatically, it’s your responsibility to set NODE_ENV=production manually!
Note: we recommend putting .next, or your custom dist folder, in .gitignore or .npmignore. Otherwise, use files or now.files to opt-into a whitelist of files you want to deploy, excluding .next or your custom dist folder.
Compression
Next.js provides gzip compression to compress rendered content and static files. Compression only works with the server target. In general you will want to enable compression on a HTTP proxy like nginx, to offload load from the Node.js process.
To disable compression in Next.js, set compress to false in next.config.js:
// next.config.js
module.exports = {
  compress: false,
}
Serverless deployment

Examples

now.sh
anna-artemov.now.sh
We encourage contributing more examples to this section


Serverless deployment dramatically improves reliability and scalability by splitting your application into smaller parts (also called lambdas).
In the case of Next.js, each page in the pages directory becomes a serverless lambda.
There are a number of benefits to serverless.
The referenced link talks about some of them in the context of Express, but the principles apply universally:
serverless allows for distributed points of failure, infinite scalability, and is incredibly affordable with a ""pay for what you use"" model.
To enable serverless mode in Next.js, add the serverless build target in next.config.js:
// next.config.js
module.exports = {
  target: 'serverless',
}
The serverless target will output a single lambda or HTML file per page.
This file is completely standalone and doesn't require any dependencies to run:

pages/index.js => .next/serverless/pages/index.js
pages/about.js => .next/serverless/pages/about.js
pages/blog.js => .next/serverless/pages/blog.html

The signature of the Next.js Serverless function is similar to the Node.js HTTP server callback:
export function render(req: http.IncomingMessage, res: http.ServerResponse) => void

http.IncomingMessage
http.ServerResponse
void refers to the function not having a return value and is equivalent to JavaScript's undefined. Calling the function will finish the request.

The static HTML files are ready to be served as-is.
You can read more about this feature, including how to opt-out, in the Automatic Static Optimization section.
Using the serverless target, you can deploy Next.js to ZEIT Now with all of the benefits and added ease of control like for example; custom routes and caching headers. See the ZEIT Guide for Deploying Next.js with Now for more information.
One Level Lower
Next.js provides low-level APIs for serverless deployments as hosting platforms have different function signatures. In general you will want to wrap the output of a Next.js serverless build with a compatibility layer.
For example if the platform supports the Node.js http.Server class:
const http = require('http')
const page = require('./.next/serverless/pages/about.js')
const server = new http.Server((req, res) => page.render(req, res))
server.listen(3000, () => console.log('Listening on http://localhost:3000'))
For specific platform examples see the examples section above.
Summary

Low-level API for implementing serverless deployment
Every page in the pages directory becomes a serverless function (lambda)
Creates the smallest possible serverless function (50Kb base zip size)
Optimized for fast cold start of the function
The serverless function has 0 dependencies (they are included in the function bundle)
Uses the http.IncomingMessage and http.ServerResponse from Node.js
opt-in using target: 'serverless' in next.config.js
Does not load next.config.js when executing the function, note that this means publicRuntimeConfig / serverRuntimeConfig are not supported

Browser support
Next.js supports IE11 and all modern browsers out of the box using @babel/preset-env. In order to support IE11 Next.js adds a global Promise polyfill. In cases where your own code or any external NPM dependencies you are using requires features not supported by your target browsers you will need to implement polyfills.
The polyfills example demonstrates the recommended approach to implement polyfills.
TypeScript
Next.js provides an integrated TypeScript experience out of the box, similar to an IDE.
To get started, create a empty tsconfig.json file in the root of your project:
touch tsconfig.json
Next.js will automatically configure this file with default values (providing your own tsconfig.json is also supported).
Then, run next dev (normally npm run dev) and Next.js will guide you through installing the necessary packages to complete setup.
npm run dev

# You'll see instructions like these:
#
# Please install typescript, @types/react, and @types/node by running:
#
#         yarn add --dev typescript @types/react @types/node
#
# ...
You're now ready to start converting files from .js to .tsx and leveraging the benefits TypeScript provides!
To learn more about TypeScript checkout its documentation.

Note: Next.js will create a file named next-env.d.ts in the root of your project.
This file ensures Next.js' types are picked up by the TypeScript compiler.
You cannot remove this file, however, you can edit it (but don't need to).


Note: Next.js does not enable TypeScript's strict mode by default.
When you feel comfortable with TypeScript, you may turn this option on in your tsconfig.json.


Note: By default, Next.js reports TypeScript errors during development for pages you are actively working on.
TypeScript errors for inactive pages do not block the development process.
If you don't want to leverage this behavior and instead, e.g. prefer your editor's integration, you can set the following option in next.config.js:
// next.config.js
module.exports = {
  typescript: {
    ignoreDevErrors: true,
  },
}
Next.js will still fail your production build (next build) when TypeScript errors are present in your project.
If you'd like Next.js to dangerously produce production code even when your application is broken, you can set the following option in your next.config.js.
Be sure you are running type checks as part of your build or deploy process!
// next.config.js
module.exports = {
  typescript: {
    // !! WARN !!
    // Dangerously allow production builds to successfully complete even if
    // your project has type errors.
    //
    // This option is rarely needed, and should be reserved for advanced
    // setups. You may be looking for `ignoreDevErrors` instead.
    // !! WARN !!
    ignoreBuildErrors: true,
  },
}

Exported types
Next.js provides NextPage type that can be used for pages in the pages directory. NextPage adds definitions for getInitialProps so that it can be used without any extra typing needed.
import { NextPage } from 'next'

interface Props {
  userAgent?: string
}

const Page: NextPage<Props> = ({ userAgent }) => (
  <main>Your user agent: {userAgent}</main>
)

Page.getInitialProps = async ({ req }) => {
  const userAgent = req ? req.headers['user-agent'] : navigator.userAgent
  return { userAgent }
}

export default Page
For React.Component you can use NextPageContext:
import React from 'react'
import { NextPageContext } from 'next'

interface Props {
  userAgent?: string
}

export default class Page extends React.Component<Props> {
  static async getInitialProps({ req }: NextPageContext) {
    const userAgent = req ? req.headers['user-agent'] : navigator.userAgent
    return { userAgent }
  }

  render() {
    const { userAgent } = this.props
    return <main>Your user agent: {userAgent}</main>
  }
}
AMP Support

Examples

amp


Enabling AMP Support
To enable AMP support for a page, add export const config = { amp: true } to your page.
AMP First Page
// pages/about.js
export const config = { amp: true }

export default function AboutPage(props) {
  return <h3>My AMP About Page!</h3>
}
Hybrid AMP Page
// pages/hybrid-about.js
import { useAmp } from 'next/amp'

export const config = { amp: 'hybrid' }

export default function AboutPage(props) {
  return (
    <div>
      <h3>My AMP Page</h3>
      {useAmp() ? (
        <amp-img
          width=""300""
          height=""300""
          src=""/my-img.jpg""
          alt=""a cool image""
          layout=""responsive""
        />
      ) : (
        <img width=""300"" height=""300"" src=""/my-img.jpg"" alt=""a cool image"" />
      )}
    </div>
  )
}
AMP Page Modes
AMP pages can specify two modes:

AMP-only (default)

Pages have no Next.js or React client-side runtime
Pages are automatically optimized with AMP Optimizer, an optimizer that applies the same transformations as AMP caches (improves performance by up to 42%)
Pages have a user-accessible (optimized) version of the page and a search-engine indexable (unoptimized) version of the page
Opt-in via export const config = { amp: true }


Hybrid

Pages are able to be rendered as traditional HTML (default) and AMP HTML (by adding ?amp=1 to the URL)
The AMP version of the page only has valid optimizations applied with AMP Optimizer so that it is indexable by search-engines
Opt-in via export const config = { amp: 'hybrid' }
Able to differentiate between modes using useAmp from next/amp



Both of these page modes provide a consistently fast experience for users accessing pages through search engines.
AMP Behavior with next export
When using next export to statically prerender pages Next.js will detect if the page supports AMP and change the exporting behavior based on that.
Hybrid AMP (pages/about.js) would output:

out/about.html - with client-side React runtime
out/about.amp.html - AMP page

AMP-only (pages/about.js) would output:

out/about.html - Optimized AMP page

During export Next.js automatically detects if a page is hybrid AMP and outputs the AMP version to page.amp.html. We also automatically insert the <link rel=""amphtml"" href=""/page.amp"" /> and <link rel=""canonical"" href=""/"" /> tags for you.

Note: When using exportTrailingSlash: true in next.config.js, output will be different. For Hybrid AMP pages, output will be out/page/index.html and out/page.amp/index.html, and for AMP-only pages, output will be out/page/index.html

Adding AMP Components
The AMP community provides many components to make AMP pages more interactive. You can add these components to your page by using next/head:
// pages/hello.js
import Head from 'next/head'

export const config = { amp: true }

export default function MyAmpPage() {
  return (
    <div>
      <Head>
        <script
          async
          key=""amp-timeago""
          custom-element=""amp-timeago""
          src=""https://cdn.ampproject.org/v0/amp-timeago-0.1.js""
        />
      </Head>

      <p>Some time: {date.toJSON()}</p>
      <amp-timeago
        width=""0""
        height=""15""
        datetime={date.toJSON()}
        layout=""responsive""
      >
        .
      </amp-timeago>
    </div>
  )
}
AMP Validation
AMP pages are automatically validated with amphtml-validator during development. Errors and warnings will appear in the terminal where you started Next.js.
Pages are also validated during next export and any warnings / errors will be printed to the terminal.
Any AMP errors will cause next export to exit with status code 1 because the export is not valid AMP.
TypeScript Support
AMP currently doesn't have built-in types for TypeScript, but it's in their roadmap (#13791). As a workaround you can manually add the types to amp.d.ts like here.
Static HTML export

Examples

Static export


next export is a way to run your Next.js app as a standalone static app without the need for a Node.js server.
The exported app supports almost every feature of Next.js, including dynamic urls, prefetching, preloading and dynamic imports.
The way next export works is by prerendering all pages possible to HTML. It does so based on a mapping of pathname key to page object. This mapping is called the exportPathMap.
The page object has 2 values:

page - String the page inside the pages directory to render
query - Object the query object passed to getInitialProps when prerendering. Defaults to {}

Usage
Simply develop your app as you normally do with Next.js. Then run:
next build
next export

By default next export doesn't require any configuration. It will generate a default exportPathMap containing the routes to pages inside the pages directory. This default mapping is available as defaultPathMap in the example below.
If your application has dynamic routes you can add a dynamic exportPathMap in next.config.js.
This function is asynchronous and gets the default exportPathMap as a parameter.
// next.config.js
module.exports = {
  exportPathMap: async function(
    defaultPathMap,
    { dev, dir, outDir, distDir, buildId }
  ) {
    return {
      '/': { page: '/' },
      '/about': { page: '/about' },
      '/readme.md': { page: '/readme' },
      '/p/hello-nextjs': { page: '/post', query: { title: 'hello-nextjs' } },
      '/p/learn-nextjs': { page: '/post', query: { title: 'learn-nextjs' } },
      '/p/deploy-nextjs': { page: '/post', query: { title: 'deploy-nextjs' } },
    }
  },
}
The pages will be exported as html files, i.e. /about will become /about.html.
It is possible to configure Next.js to export pages as index.html files and require trailing slashes, i.e. /about becomes /about/index.html and is routable via /about/.
This was the default behavior prior to Next.js 9.
You can use the following next.config.js to switch back to this behavior:
// next.config.js
module.exports = {
  exportTrailingSlash: true,
}

Note: If the export path is a filename (e.g. /readme.md) and is different than .html, you may need to set the Content-Type header to text/html when serving this content.

The second argument is an object with:

dev - true when exportPathMap is being called in development. false when running next export. In development exportPathMap is used to define routes.
dir - Absolute path to the project directory
outDir - Absolute path to the out/ directory (configurable with -o or --outdir). When dev is true the value of outDir will be null.
distDir - Absolute path to the .next/ directory (configurable using the distDir config key)
buildId - The buildId the export is running for

Then simply run these commands:
next build
next export
For that you may need to add a NPM script to package.json like this:
{
  ""scripts"": {
    ""build"": ""next build"",
    ""export"": ""npm run build && next export""
  }
}
And run it at once with:
npm run export
Then you have a static version of your app in the out directory.

You can also customize the output directory. For that run next export -h for the help.

Now you can deploy the out directory to any static hosting service. Note that there is an additional step for deploying to GitHub Pages, documented here.
For an example, simply visit the out directory and run following command to deploy your app to ZEIT Now.
now
Limitation
With next export, we build a HTML version of your app. At export time we will run getInitialProps of your pages.
The req and res fields of the context object passed to getInitialProps are empty objects during export as there is no server running.

Note: If your pages don't have getInitialProps you may not need next export at all, next build is already enough thanks to automatic static optimization.


You won't be able to render HTML dynamically when static exporting, as we pre-build the HTML files. If you want to do dynamic rendering use next start or the custom server API

Multi Zones

Examples

With Zones


A zone is a single deployment of a Next.js app. Just like that, you can have multiple zones and then you can merge them as a single app.
For an example, you can have two zones like this:

An app for serving /blog/**
Another app for serving all other pages

With multi zones support, you can merge both these apps into a single one allowing your customers to browse it using a single URL, but you can develop and deploy both apps independently.

This is exactly the same concept of microservices, but for frontend apps.

How to define a zone
There are no special zones related APIs. You only need to do following:

Make sure to keep only the pages you need in your app, meaning that an app can't have pages from another app, if app A has /blog then app B shouldn't have it too.
Make sure to add an assetPrefix to avoid conflicts with static files.

How to merge them
You can merge zones using any HTTP proxy.
You can use now dev as your local development server. It allows you to easily define routing routes for multiple apps like below:
{
  ""version"": 2,
  ""builds"": [
    { ""src"": ""docs/next.config.js"", ""use"": ""@now/next"" },
    { ""src"": ""home/next.config.js"", ""use"": ""@now/next"" }
  ],
  ""routes"": [
    { ""src"": ""/docs(.*)"", ""dest"": ""docs$1"", ""continue"": true },
    { ""src"": ""(?!/?docs)(.*)"", ""dest"": ""home$1"", ""continue"": true }
  ]
}
For the production deployment, you can use the same configuration and run now to do the deployment with ZEIT Now. Otherwise you can also configure a proxy server to route using a set of routes like the ones above, e.g deploy the docs app to https://docs.example.com and the home app to https://home.example.com and then add a proxy server for both apps in https://example.com.
FAQ

Is this production ready?
  Next.js has been powering https://zeit.co since its inception.
We’re ecstatic about both the developer experience and end-user performance, so we decided to share it with the community.


How big is it?
The client side bundle size should be measured in a per-app basis.
A small Next main bundle is around 65kb gzipped.


Is this like `create-react-app`?
Yes and No.
Yes in that both make your life easier.
No in that it enforces a structure so that we can do more advanced things like:

Server side rendering
Automatic code splitting

In addition, Next.js provides two built-in features that are critical for every single website:

Routing with lazy component loading: <Link> (by importing next/link)
A way for components to alter <head>: <Head> (by importing next/head)

If you want to create re-usable React components that you can embed in your Next.js app or other React applications, using create-react-app is a great idea. You can later import it and keep your codebase clean!


How do I use CSS-in-JS solutions?
Next.js bundles styled-jsx supporting scoped css. However you can use any CSS-in-JS solution in your Next app by just including your favorite library as mentioned before in the document.


What syntactic features are transpiled? How do I change them?
We track V8. Since V8 has wide support for ES6 and async and await, we transpile those. Since V8 doesn’t support class decorators, we don’t transpile those.
See the documentation about customizing the babel config and next/preset for more information.


Why a new Router?
Next.js is special in that:

Routes don’t need to be known ahead of time
Routes are always lazy-loadable
Top-level components can define getInitialProps that should block the loading of the route (either when server-rendering or lazy-loading)

As a result, we were able to introduce a very simple approach to routing that consists of two pieces:

Every top level component receives a url object to inspect the url or perform modifications to the history
A <Link /> component is used to wrap elements like anchors (<a/>) to perform client-side transitions



How do I define a custom fancy route?
Next.js provide dynamic routing solution out of the box. This allows to use pretty links in url.
You can check an example to see how it works.


How do I fetch data?
It’s up to you. getInitialProps is an async function (or a regular function that returns a Promise). It can retrieve data from anywhere.


Can I use it with GraphQL?
Yes! Here's an example with Apollo.


Can I use it with Redux and thunk?
Yes! Here's an example.


Can I use it with Redux?
Yes! Here's an example.


Can I use Next with my favorite Javascript library or toolkit?
Since our first release we've had many example contributions, you can check them out in the examples directory.


What is this inspired by?
Many of the goals we set out to accomplish were the ones listed in The 7 principles of Rich Web Applications by Guillermo Rauch.
The ease-of-use of PHP is a great inspiration. We feel Next.js is a suitable replacement for many scenarios where you otherwise would use PHP to output HTML.
Unlike PHP, we benefit from the ES6 module system and every file exports a component or function that can be easily imported for lazy evaluation or testing.
As we were researching options for server-rendering React that didn’t involve a large number of steps, we came across react-page (now deprecated), a similar approach to Next.js by the creator of React Jordan Walke.

Contributing
Please see our contributing.md.
Authors

Arunoda Susiripala (@arunoda) – ZEIT
Tim Neutkens (@timneutkens) – ZEIT
Naoyuki Kanezawa (@nkzawa) – ZEIT
Tony Kovanen (@tonykovanen) – ZEIT
Guillermo Rauch (@rauchg) – ZEIT
Dan Zajdband (@impronunciable) – Knight-Mozilla / Coral Project


"
2,"
Front-end Job Interview Questions
This repository contains a number of front-end interview questions that can be used when vetting potential candidates. It is by no means recommended to use every single question here on the same candidate (that would take hours). Choosing a few items from this list should help you vet the intended skills you require.
Note: Keep in mind that many of these questions are open-ended and could lead to interesting discussions that tell you more about the person's capabilities than a straight answer would.
You can read more about this project & its history here.
Table of Contents

General Questions
HTML Questions
CSS Questions
JS Questions
Accessibility Questions (external link)
Testing Questions
Performance Questions
Network Questions
Coding Questions
Fun Questions

Getting Involved

Contributors
How to Contribute
License

The project is currently maintained by:

@roblarsen
@vvscode

Contributors
Feeling inspired? Check our Contributing guide to get started!


Darcy Clarke🤔 📖 🚇 👀 💬 📢 🚧
Bo-Yi Wu📖 👀
Nikolay Kostov🌍
Jan Hancic🌍
Rich Gilbank📖 👀
Mattias Wallander🌍


Songhun📖 🌍
Giulia Alfonsi🌍
Mike Myat Min Han🌍
SunLn🌍
Yi, Hangehee🌍
shawnqiang🌍


Guilherme Pontes🌍
lufeihaidao🌍
Donald Zhan🌍
Mina Markham📖
Paul Irish📖
Mathieu Hays🌍


Yong Yin📖
Dale Sande / @anotheruiguy📖 🚇 👀
Marco Biedermann🚇
Victor Coulon🌍 👀
Paulo Ávila📖
Ekrem Karaca🌍


Achal Varma📖
Aurelio De Rosa📖
Min Zhao🌍
Cătălin Mariș📖 🌍
Simon Owen📖
Maxim Khlobystov📖 👀


Sara📖 🌍
Jason Hummel📖
Kunal Sachdeva📖
AJ Jordan📖
paulalexandru📖 🌍
dot🚇 🌍


Everardo Medina🌍
Gaurav Nanda📖
Andrii Malaman📖
Daniele Zanni🌍
Fernando Freitas Alves🌍
John Wu📖


Pablo Nevares📖 🌍
Felipe Ramos📖
Arthur Verschaeve📖 👀 🌍
Richard Denton📖
Andrey Fadeyev🌍
rimager📖


Rishabh Jain📖
Kaijun Chen🌍
Nithya📖
Erwan Jegouzo📖
Tieme van Veen📖
Hsun🌍


Adrien CHRETIEN🌍
Alan Rodríguez🌍
Adam Haris🌍
Toshimaru🌍
Shankar Cabus🌍
Ash🌍


Hank Wang🌍
KILLHAPPY.📖
Marko Švaljek🌍
Antonio Laguna📖 🌍
Mithun Dhiman🌍
Kévin Rocher / @Darklg🌍


Dmitry Pashkevich🌍
karmeljuk🌍
Dirk Schürjohann🌍
Björn Söderqvist🌍
Nitin Hayaran📖
Peter Galiba📖


Neil Heinrich📖
Ohgyun Ahn🌍
nerdog🌍
Leo Picado🌍
Romain Dardour🌍
Alex Seville📖


胡尐睿丶🌍
Vitalii Petrychuk🌍
Tairraos📖 🌍
Dmitrii Raev🌍
Bartek🌍
Vitor Balocco📖


Jonathan Neal📖
Christoffer Lans🌍
Felipe Fialho🌍
Piotrek Mierzejewski🌍
Patrik Wibron🌍
Dmitry Vislov🌍


Krzysztof Romanowski📖
Albert Yu🌍
Sebastian Lara Menares🌍
sunnylost🌍
Daniel Yang📖
Michael P. Pfeiffer🌍


Tyll Weiß🌍
Andrei Sebastian Cîmpean🌍
Denis Sokolov📖
Harijs Deksnis🌍
Rob Larsen🤔 👀 🚧
Cezar Augusto🤔 🚇 👀 🚧


Vasiliy Vanchuk🤔 👀 🚧


License
Copyright (c) Contributors of the Front-end Developer Interview Questions

"
3,"

   



A lightweight JavaScript date library for parsing, validating, manipulating, and formatting dates.
Documentation
Port to ECMAScript 6 (version 2.10.0)
Moment 2.10.0 does not bring any new features, but the code is now written in
ECMAScript 6 modules and placed inside src/. Previously moment.js, locale/*.js and
test/moment/*.js, test/locale/*.js contained the source of the project. Now
the source is in src/, temporary build (ECMAScript 5) files are placed under
build/umd/ (for running tests during development), and the moment.js and
locale/*.js files are updated only on release.
If you want to use a particular revision of the code, make sure to run
grunt transpile update-index, so moment.js and locales/*.js are synced
with src/*. We might place that in a commit hook in the future.
Upgrading to 2.0.0
There are a number of small backwards incompatible changes with version 2.0.0. See the full descriptions here


Changed language ordinal method to return the number + ordinal instead of just the ordinal.


Changed two digit year parsing cutoff to match strptime.


Removed moment#sod and moment#eod in favor of moment#startOf and moment#endOf.


Removed moment.humanizeDuration() in favor of moment.duration().humanize().


Removed the lang data objects from the top level namespace.


Duplicate Date passed to moment() instead of referencing it.


Changelog
Contributing 
We're looking for co-maintainers! If you want to become a master of time please
write to ichernev.
In addition to contributing code, you can help to triage issues. This can include reproducing bug reports, or asking for vital information such as version numbers or reproduction instructions. If you would like to start triaging issues, one easy way to get started is to subscribe to moment/moment on CodeTriage.
License
Moment.js is freely distributable under the terms of the MIT license.


"
4,"
Awesome Machine Learning 
A curated list of awesome machine learning frameworks, libraries and software (by language). Inspired by awesome-php.
If you want to contribute to this list (please do), send me a pull request or contact me @josephmisiti.
Also, a listed repository should be deprecated if:

Repository's owner explicitly say that ""this library is not maintained"".
Not committed for long time (2~3 years).

Further resources:


For a list of free machine learning books available for download, go here.


For a list of (mostly) free machine learning courses available online, go here.


For a list of blogs and newsletters on data science and machine learning, go here.


For a list of free-to-attend meetups and local events, go here.


Table of Contents
Frameworks and Libraries

Awesome Machine Learning 

Table of Contents

Frameworks and Libraries
Tools


APL

General-Purpose Machine Learning


C

General-Purpose Machine Learning
Computer Vision


C++

Computer Vision
General-Purpose Machine Learning
Natural Language Processing
Speech Recognition
Sequence Analysis
Gesture Detection


Common Lisp

General-Purpose Machine Learning


Clojure

Natural Language Processing
General-Purpose Machine Learning
Data Analysis / Data Visualization


Crystal

General-Purpose Machine Learning


Elixir

General-Purpose Machine Learning
Natural Language Processing


Erlang

General-Purpose Machine Learning


Go

Natural Language Processing
General-Purpose Machine Learning
Spatial analysis and geometry
Data Analysis / Data Visualization
Computer vision


Haskell

General-Purpose Machine Learning


Java

Natural Language Processing
General-Purpose Machine Learning
Speech Recognition
Data Analysis / Data Visualization
Deep Learning


Javascript

Natural Language Processing
Data Analysis / Data Visualization
General-Purpose Machine Learning
Misc
Demos and Scripts


Julia

General-Purpose Machine Learning
Natural Language Processing
Data Analysis / Data Visualization
Misc Stuff / Presentations


Lua

General-Purpose Machine Learning
Demos and Scripts


Matlab

Computer Vision
Natural Language Processing
General-Purpose Machine Learning
Data Analysis / Data Visualization


.NET

Computer Vision
Natural Language Processing
General-Purpose Machine Learning
Data Analysis / Data Visualization


Objective C

General-Purpose Machine Learning


OCaml

General-Purpose Machine Learning


Perl

Data Analysis / Data Visualization
General-Purpose Machine Learning


Perl 6

Data Analysis / Data Visualization
General-Purpose Machine Learning


PHP

Natural Language Processing
General-Purpose Machine Learning


Python

Computer Vision
Natural Language Processing
General-Purpose Machine Learning
Data Analysis / Data Visualization
Misc Scripts / iPython Notebooks / Codebases
Neural Networks
Kaggle Competition Source Code
Reinforcement Learning


Ruby

Natural Language Processing
General-Purpose Machine Learning
Data Analysis / Data Visualization
Misc


Rust

General-Purpose Machine Learning


R

General-Purpose Machine Learning
Data Analysis / Data Visualization


SAS

General-Purpose Machine Learning
Data Analysis / Data Visualization
Natural Language Processing
Demos and Scripts


Scala

Natural Language Processing
Data Analysis / Data Visualization
General-Purpose Machine Learning


Scheme

Neural Networks


Swift

General-Purpose Machine Learning


TensorFlow

General-Purpose Machine Learning


Tools

Neural Networks
Misc


Credits
写个脚本把它们爬下来 - Demos and Scripts


Scala

Natural Language Processing
Data Analysis / Data Visualization
General-Purpose Machine Learning


Scheme

Neural Networks


Swift

General-Purpose Machine Learning


TensorFlow

General-Purpose Machine Learning



Tools

Neural Networks
Misc

Credits

APL

General-Purpose Machine Learning

naive-apl - Naive Bayesian Classifier implementation in APL. [Deprecated]


C

General-Purpose Machine Learning

Darknet - Darknet is an open source neural network framework written in C and CUDA. It is fast, easy to install, and supports CPU and GPU computation.
Recommender - A C library for product recommendations/suggestions using collaborative filtering (CF).
Hybrid Recommender System - A hybrid recommender system based upon scikit-learn algorithms. [Deprecated]
neonrvm - neonrvm is an open source machine learning library based on RVM technique. It's written in C programming language and comes with Python programming language bindings.


Computer Vision

CCV - C-based/Cached/Core Computer Vision Library, A Modern Computer Vision Library.
VLFeat - VLFeat is an open and portable library of computer vision algorithms, which has Matlab toolbox.


C++

Computer Vision

DLib - DLib has C++ and Python interfaces for face detection and training general object detectors.
EBLearn - Eblearn is an object-oriented C++ library that implements various machine learning models [Deprecated]
OpenCV - OpenCV has C++, C, Python, Java and MATLAB interfaces and supports Windows, Linux, Android and Mac OS.
VIGRA - VIGRA is a generic cross-platform C++ computer vision and machine learning library for volumes of arbitrary dimensionality with Python bindings.


General-Purpose Machine Learning

BanditLib - A simple Multi-armed Bandit library. [Deprecated]
Caffe - A deep learning framework developed with cleanliness, readability, and speed in mind. [DEEP LEARNING]
CatBoost - General purpose gradient boosting on decision trees library with categorical features support out of the box. It is easy to install, contains fast inference implementation and supports CPU and GPU (even multi-GPU) computation.
CNTK - The Computational Network Toolkit (CNTK) by Microsoft Research, is a unified deep-learning toolkit that describes neural networks as a series of computational steps via a directed graph.
CUDA - This is a fast C++/CUDA implementation of convolutional [DEEP LEARNING]
DeepDetect - A machine learning API and server written in C++11. It makes state of the art machine learning easy to work with and integrate into existing applications.
Distributed Machine learning Tool Kit (DMTK) - A distributed machine learning (parameter server) framework by Microsoft. Enables training models on large data sets across multiple machines. Current tools bundled with it include: LightLDA and Distributed (Multisense) Word Embedding.
DLib - A suite of ML tools designed to be easy to imbed in other applications.
DSSTNE - A software library created by Amazon for training and deploying deep neural networks using GPUs which emphasizes speed and scale over experimental flexibility.
DyNet - A dynamic neural network library working well with networks that have dynamic structures that change for every training instance. Written in C++ with bindings in Python.
Fido - A highly-modular C++ machine learning library for embedded electronics and robotics.
igraph - General purpose graph library.
Intel(R) DAAL - A high performance software library developed by Intel and optimized for Intel's architectures. Library provides algorithmic building blocks for all stages of data analytics and allows to process data in batch, online and distributed modes.
LightGBM - Microsoft's fast, distributed, high performance gradient boosting (GBDT, GBRT, GBM or MART) framework based on decision tree algorithms, used for ranking, classification and many other machine learning tasks.
libfm - A generic approach that allows to mimic most factorization models by feature engineering.
MLDB - The Machine Learning Database is a database designed for machine learning. Send it commands over a RESTful API to store data, explore it using SQL, then train machine learning models and expose them as APIs.
mlpack - A scalable C++ machine learning library.
MXNet - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.
proNet-core - A general-purpose network embedding framework: pair-wise representations optimization Network Edit.
PyCUDA - Python interface to CUDA
ROOT - A modular scientific software framework. It provides all the functionalities needed to deal with big data processing, statistical analysis, visualization and storage.
shark - A fast, modular, feature-rich open-source C++ machine learning library.
Shogun - The Shogun Machine Learning Toolbox.
sofia-ml - Suite of fast incremental algorithms.
Stan - A probabilistic programming language implementing full Bayesian statistical inference with Hamiltonian Monte Carlo sampling.
Timbl - A software package/C++ library implementing several memory-based learning algorithms, among which IB1-IG, an implementation of k-nearest neighbor classification, and IGTree, a decision-tree approximation of IB1-IG. Commonly used for NLP.
Vowpal Wabbit (VW) - A fast out-of-core learning system.
Warp-CTC - A fast parallel implementation of Connectionist Temporal Classification (CTC), on both CPU and GPU.
XGBoost - A parallelized optimized general purpose gradient boosting library.
ThunderGBM - A fast library for GBDTs and Random Forests on GPUs.
ThunderSVM - A fast SVM library on GPUs and CPUs.
LKYDeepNN - A header-only C++11 Neural Network library. Low dependency, native traditional chinese document.
xLearn - A high performance, easy-to-use, and scalable machine learning package, which can be used to solve large-scale machine learning problems. xLearn is especially useful for solving machine learning problems on large-scale sparse data, which is very common in Internet services such as online advertisement and recommender systems.
Featuretools - A library for automated feature engineering. It excels at transforming transactional and relational datasets into feature matrices for machine learning using reusable feature engineering ""primitives"".
skynet - A library for learning neural network, has C-interface, net set in JSON. Written in C++ with bindings in Python, C++ and C#.
Feast - A feature store for the management, discovery, and access of machine learning features. Feast provides a consistent view of feature data for both model training and model serving.
Polyaxon - A platform for reproducible and scalable machine learning and deep learning.


Natural Language Processing

BLLIP Parser - BLLIP Natural Language Parser (also known as the Charniak-Johnson parser).
colibri-core - C++ library, command line tools, and Python binding for extracting and working with basic linguistic constructions such as n-grams and skipgrams in a quick and memory-efficient way.
CRF++ - Open source implementation of Conditional Random Fields (CRFs) for segmenting/labeling sequential data & other Natural Language Processing tasks. [Deprecated]
CRFsuite - CRFsuite is an implementation of Conditional Random Fields (CRFs) for labeling sequential data. [Deprecated]
frog - Memory-based NLP suite developed for Dutch: PoS tagger, lemmatiser, dependency parser, NER, shallow parser, morphological analyzer.
libfolia - C++ library for the FoLiA format
MeTA - MeTA : ModErn Text Analysis is a C++ Data Sciences Toolkit that facilitates mining big text data.
MIT Information Extraction Toolkit - C, C++, and Python tools for named entity recognition and relation extraction
ucto - Unicode-aware regular-expression based tokenizer for various languages. Tool and C++ library. Supports FoLiA format.


Speech Recognition

Kaldi - Kaldi is a toolkit for speech recognition written in C++ and licensed under the Apache License v2.0. Kaldi is intended for use by speech recognition researchers.


Sequence Analysis

ToPS - This is an objected-oriented framework that facilitates the integration of probabilistic models for sequences over a user defined alphabet. [Deprecated]


Gesture Detection

grt - The Gesture Recognition Toolkit (GRT) is a cross-platform, open-source, C++ machine learning library designed for real-time gesture recognition.


Common Lisp

General-Purpose Machine Learning

mgl - Neural networks (boltzmann machines, feed-forward and recurrent nets), Gaussian Processes.
mgl-gpr - Evolutionary algorithms. [Deprecated]
cl-libsvm - Wrapper for the libsvm support vector machine library. [Deprecated]
cl-online-learning - Online learning algorithms (Perceptron, AROW, SCW, Logistic Regression).
cl-random-forest - Implementation of Random Forest in Common Lisp.


Clojure

Natural Language Processing

Clojure-openNLP - Natural Language Processing in Clojure (opennlp).
Infections-clj - Rails-like inflection library for Clojure and ClojureScript.


General-Purpose Machine Learning

Touchstone - Clojure A/B testing library. [Deprecated]
Clojush - The Push programming language and the PushGP genetic programming system implemented in Clojure.
Infer - Inference and machine learning in Clojure. [Deprecated]
Clj-ML - A machine learning library for Clojure built on top of Weka and friends. [Deprecated]
DL4CLJ - Clojure wrapper for Deeplearning4j.
Encog - Clojure wrapper for Encog (v3) (Machine-Learning framework that specializes in neural-nets). [Deprecated]
Fungp - A genetic programming library for Clojure. [Deprecated]
Statistiker - Basic Machine Learning algorithms in Clojure. [Deprecated]
clortex - General Machine Learning library using Numenta’s Cortical Learning Algorithm. [Deprecated]
comportex - Functionally composable Machine Learning library using Numenta’s Cortical Learning Algorithm. [Deprecated]
cortex - Neural networks, regression and feature learning in Clojure.
lambda-ml - Simple, concise implementations of machine learning techniques and utilities in Clojure.


Data Analysis / Data Visualization

Incanter - Incanter is a Clojure-based, R-like platform for statistical computing and graphics.
PigPen - Map-Reduce for Clojure.
Envision - Clojure Data Visualisation library, based on Statistiker and D3.


Crystal

General-Purpose Machine Learning

machine - Simple machine learning algorithm.
crystal-fann - FANN (Fast Artificial Neural Network) binding.


Elixir

General-Purpose Machine Learning

Simple Bayes - A Simple Bayes / Naive Bayes implementation in Elixir.
emel - A simple and functional machine learning library written in Elixir.
Tensorflex - Tensorflow bindings for the Elixir programming language.


Natural Language Processing

Stemmer - An English (Porter2) stemming implementation in Elixir.


Erlang

General-Purpose Machine Learning

Disco - Map Reduce in Erlang. [Deprecated]
Yanni - ANN neural networks using Erlangs leightweight processes.


Go

Natural Language Processing

snowball - Snowball Stemmer for Go.
word-embedding - Word Embeddings: the full implementation of word2vec, GloVe in Go.
sentences - Golang implementation of Punkt sentence tokenizer.
go-ngram - In-memory n-gram index with compression. [Deprecated]
paicehusk - Golang implementation of the Paice/Husk Stemming Algorithm. [Deprecated]
go-porterstemmer - A native Go clean room implementation of the Porter Stemming algorithm. [Deprecated]


General-Purpose Machine Learning

birdland - A recommendation library in Go.
eaopt - An evolutionary optimization library.
leaves - A pure Go implementation of the prediction part of GBRTs, including XGBoost and LightGBM.
gobrain - Neural Networks written in Go.
go-mxnet-predictor - Go binding for MXNet c_predict_api to do inference with pre-trained model.
go-ml-transpiler - An open source Go transpiler for machine learning models.
golearn - Machine learning for Go.
goml - Machine learning library written in pure Go.
gorgonia - Deep learning in Go.
gorse - An offline recommender system backend based on collaborative filtering written in Go.
therfoo - An embedded deep learning library for Go.
neat - Plug-and-play, parallel Go framework for NeuroEvolution of Augmenting Topologies (NEAT). [Deprecated]
go-pr - Pattern recognition package in Go lang. [Deprecated]
go-ml - Linear / Logistic regression, Neural Networks, Collaborative Filtering and Gaussian Multivariate Distribution. [Deprecated]
GoNN - GoNN is an implementation of Neural Network in Go Language, which includes BPNN, RBF, PCN. [Deprecated]
bayesian - Naive Bayesian Classification for Golang. [Deprecated]
go-galib - Genetic Algorithms library written in Go / Golang. [Deprecated]
Cloudforest - Ensembles of decision trees in Go/Golang. [Deprecated]
go-dnn - Deep Neural Networks for Golang (powered by MXNet)


Spatial analysis and geometry

go-geom - Go library to handle geometries.
gogeo - Spherical geometry in Go.


Data Analysis / Data Visualization

gota - Dataframes.
gonum/mat - A linear algebra package for Go.
gonum/optimize - Implementations of optimization algorithms.
gonum/plot - A plotting library.
gonum/stat - A statistics library.
SVGo - The Go Language library for SVG generation.
glot - Glot is a plotting library for Golang built on top of gnuplot.
globe - Globe wireframe visualization.
gonum/graph - General-purpose graph library.
go-graph - Graph library for Go/Golang language. [Deprecated]
RF - Random forests implementation in Go. [Deprecated]


Computer vision

GoCV - Package for computer vision using OpenCV 4 and beyond.


Haskell

General-Purpose Machine Learning

haskell-ml - Haskell implementations of various ML algorithms. [Deprecated]
HLearn - a suite of libraries for interpreting machine learning models according to their algebraic structure. [Deprecated]
hnn - Haskell Neural Network library.
hopfield-networks - Hopfield Networks for unsupervised learning in Haskell. [Deprecated]
DNNGraph - A DSL for deep neural networks. [Deprecated]
LambdaNet - Configurable Neural Networks in Haskell. [Deprecated]


Java

Natural Language Processing

Cortical.io - Retina: an API performing complex NLP operations (disambiguation, classification, streaming text filtering, etc...) as quickly and intuitively as the brain.
IRIS - Cortical.io's FREE NLP, Retina API Analysis Tool (written in JavaFX!) - See the Tutorial Video.
CoreNLP - Stanford CoreNLP provides a set of natural language analysis tools which can take raw English language text input and give the base forms of words.
Stanford Parser - A natural language parser is a program that works out the grammatical structure of sentences.
Stanford POS Tagger - A Part-Of-Speech Tagger (POS Tagger).
Stanford Name Entity Recognizer - Stanford NER is a Java implementation of a Named Entity Recognizer.
Stanford Word Segmenter - Tokenization of raw text is a standard pre-processing step for many NLP tasks.
Tregex, Tsurgeon and Semgrex - Tregex is a utility for matching patterns in trees, based on tree relationships and regular expression matches on nodes (the name is short for ""tree regular expressions"").
Stanford Phrasal: A Phrase-Based Translation System
Stanford English Tokenizer - Stanford Phrasal is a state-of-the-art statistical phrase-based machine translation system, written in Java.
Stanford Tokens Regex - A tokenizer divides text into a sequence of tokens, which roughly correspond to ""words"".
Stanford Temporal Tagger - SUTime is a library for recognizing and normalizing time expressions.
Stanford SPIED - Learning entities from unlabeled text starting with seed sets using patterns in an iterative fashion.
Stanford Topic Modeling Toolbox - Topic modeling tools to social scientists and others who wish to perform analysis on datasets.
Twitter Text Java - A Java implementation of Twitter's text processing library.
MALLET - A Java-based package for statistical natural language processing, document classification, clustering, topic modeling, information extraction, and other machine learning applications to text.
OpenNLP - a machine learning based toolkit for the processing of natural language text.
LingPipe - A tool kit for processing text using computational linguistics.
ClearTK - ClearTK provides a framework for developing statistical natural language processing (NLP) components in Java and is built on top of Apache UIMA. [Deprecated]
Apache cTAKES - Apache clinical Text Analysis and Knowledge Extraction System (cTAKES) is an open-source natural language processing system for information extraction from electronic medical record clinical free-text.
NLP4J - The NLP4J project provides software and resources for natural language processing. The project started at the Center for Computational Language and EducAtion Research, and is currently developed by the Center for Language and Information Research at Emory University. [Deprecated]
CogcompNLP - This project collects a number of core libraries for Natural Language Processing (NLP) developed in the University of Illinois' Cognitive Computation Group, for example illinois-core-utilities which provides a set of NLP-friendly data structures and a number of NLP-related utilities that support writing NLP applications, running experiments, etc, illinois-edison a library for feature extraction from illinois-core-utilities data structures and many other packages.


General-Purpose Machine Learning

aerosolve - A machine learning library by Airbnb designed from the ground up to be human friendly.
AMIDST Toolbox - A Java Toolbox for Scalable Probabilistic Machine Learning.
Datumbox - Machine Learning framework for rapid development of Machine Learning and Statistical applications.
ELKI - Java toolkit for data mining. (unsupervised: clustering, outlier detection etc.)
Encog - An advanced neural network and machine learning framework. Encog contains classes to create a wide variety of networks, as well as support classes to normalize and process data for these neural networks. Encog trains using multithreaded resilient propagation. Encog can also make use of a GPU to further speed processing time. A GUI based workbench is also provided to help model and train neural networks.
FlinkML in Apache Flink - Distributed machine learning library in Flink.
H2O - ML engine that supports distributed learning on Hadoop, Spark or your laptop via APIs in R, Python, Scala, REST/JSON.
htm.java - General Machine Learning library using Numenta’s Cortical Learning Algorithm.
liblinear-java - Java version of liblinear.
Mahout - Distributed machine learning.
Meka - An open source implementation of methods for multi-label classification and evaluation (extension to Weka).
MLlib in Apache Spark - Distributed machine learning library in Spark
Hydrosphere Mist - a service for deployment Apache Spark MLLib machine learning models as realtime, batch or reactive web services.
Neuroph - Neuroph is lightweight Java neural network framework
ORYX - Lambda Architecture Framework using Apache Spark and Apache Kafka with a specialization for real-time large-scale machine learning.
Samoa SAMOA is a framework that includes distributed machine learning for data streams with an interface to plug-in different stream processing platforms.
RankLib - RankLib is a library of learning to rank algorithms. [Deprecated]
rapaio - statistics, data mining and machine learning toolbox in Java.
RapidMiner - RapidMiner integration into Java code.
Stanford Classifier - A classifier is a machine learning tool that will take data items and place them into one of k classes.
SmileMiner - Statistical Machine Intelligence & Learning Engine.
SystemML - flexible, scalable machine learning (ML) language.
Weka - Weka is a collection of machine learning algorithms for data mining tasks.
LBJava - Learning Based Java is a modeling language for the rapid development of software systems, offers a convenient, declarative syntax for classifier and constraint definition directly in terms of the objects in the programmer's application.


Speech Recognition

CMU Sphinx - Open Source Toolkit For Speech Recognition purely based on Java speech recognition library.


Data Analysis / Data Visualization

Flink - Open source platform for distributed stream and batch data processing.
Hadoop - Hadoop/HDFS.
Onyx - Distributed, masterless, high performance, fault tolerant data processing. Written entirely in Clojure.
Spark - Spark is a fast and general engine for large-scale data processing.
Storm - Storm is a distributed realtime computation system.
Impala - Real-time Query for Hadoop.
DataMelt - Mathematics software for numeric computation, statistics, symbolic calculations, data analysis and data visualization.
Dr. Michael Thomas Flanagan's Java Scientific Library [Deprecated]


Deep Learning

Deeplearning4j - Scalable deep learning for industry with parallel GPUs.
Keras Beginner Tutorial - Friendly guide on using Keras to implement a simple Neural Network in Python


Javascript

Natural Language Processing

Twitter-text - A JavaScript implementation of Twitter's text processing library.
natural - General natural language facilities for node.
Knwl.js - A Natural Language Processor in JS.
Retext - Extensible system for analyzing and manipulating natural language.
NLP Compromise - Natural Language processing in the browser.
nlp.js - An NLP library built in node over Natural, with entity extraction, sentiment analysis, automatic language identify, and so more


Data Analysis / Data Visualization

D3.js
High Charts
NVD3.js
dc.js
chartjs
dimple
amCharts
D3xter - Straight forward plotting built on D3. [Deprecated]
statkit - Statistics kit for JavaScript. [Deprecated]
datakit - A lightweight framework for data analysis in JavaScript
science.js - Scientific and statistical computing in JavaScript. [Deprecated]
Z3d - Easily make interactive 3d plots built on Three.js [Deprecated]
Sigma.js - JavaScript library dedicated to graph drawing.
C3.js - customizable library based on D3.js for easy chart drawing.
Datamaps - Customizable SVG map/geo visualizations using D3.js. [Deprecated]
ZingChart - library written on Vanilla JS for big data visualization.
cheminfo - Platform for data visualization and analysis, using the visualizer project.
Learn JS Data
AnyChart
FusionCharts
Nivo - built on top of the awesome d3 and Reactjs libraries


General-Purpose Machine Learning

Auto ML - Automated machine learning, data formatting, ensembling, and hyperparameter optimization for competitions and exploration- just give it a .csv file!
Convnet.js - ConvNetJS is a Javascript library for training Deep Learning models[DEEP LEARNING] [Deprecated]
Clusterfck - Agglomerative hierarchical clustering implemented in Javascript for Node.js and the browser. [Deprecated]
Clustering.js - Clustering algorithms implemented in Javascript for Node.js and the browser. [Deprecated]
Decision Trees - NodeJS Implementation of Decision Tree using ID3 Algorithm. [Deprecated]
DN2A - Digital Neural Networks Architecture. [Deprecated]
figue - K-means, fuzzy c-means and agglomerative clustering.
Gaussian Mixture Model - Unsupervised machine learning with multivariate Gaussian mixture model.
Node-fann - FANN (Fast Artificial Neural Network Library) bindings for Node.js [Deprecated]
Keras.js - Run Keras models in the browser, with GPU support provided by WebGL 2.
Kmeans.js - Simple Javascript implementation of the k-means algorithm, for node.js and the browser. [Deprecated]
LDA.js - LDA topic modeling for Node.js
Learning.js - Javascript implementation of logistic regression/c4.5 decision tree [Deprecated]
machinelearn.js - Machine Learning library for the web, Node.js and developers
mil-tokyo - List of several machine learning libraries.
Node-SVM - Support Vector Machine for Node.js
Brain - Neural networks in JavaScript [Deprecated]
Brain.js - Neural networks in JavaScript - continued community fork of Brain.
Bayesian-Bandit - Bayesian bandit implementation for Node and the browser. [Deprecated]
Synaptic - Architecture-free neural network library for Node.js and the browser.
kNear - JavaScript implementation of the k nearest neighbors algorithm for supervised learning.
NeuralN - C++ Neural Network library for Node.js. It has advantage on large dataset and multi-threaded training. [Deprecated]
kalman - Kalman filter for Javascript. [Deprecated]
shaman - Node.js library with support for both simple and multiple linear regression. [Deprecated]
ml.js - Machine learning and numerical analysis tools for Node.js and the Browser!
ml5 - Friendly machine learning for the web!
Pavlov.js - Reinforcement learning using Markov Decision Processes.
MXNet - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.
TensorFlow.js - A WebGL accelerated, browser based JavaScript library for training and deploying ML models.
JSMLT - Machine learning toolkit with classification and clustering for Node.js; supports visualization (see visualml.io).
xgboost-node - Run XGBoost model and make predictions in Node.js.
Netron - Visualizer for machine learning models.
WebDNN - Fast Deep Neural Network Javascript Framework. WebDNN uses next generation JavaScript API, WebGPU for GPU execution, and WebAssembly for CPU execution.


Misc

stdlib - A standard library for JavaScript and Node.js, with an emphasis on numeric computing. The library provides a collection of robust, high performance libraries for mathematics, statistics, streams, utilities, and more.
sylvester - Vector and Matrix math for JavaScript. [Deprecated]
simple-statistics - A JavaScript implementation of descriptive, regression, and inference statistics. Implemented in literate JavaScript with no dependencies, designed to work in all modern browsers (including IE) as well as in Node.js.
regression-js - A javascript library containing a collection of least squares fitting methods for finding a trend in a set of data.
Lyric - Linear Regression library. [Deprecated]
GreatCircle - Library for calculating great circle distance.
MLPleaseHelp - MLPleaseHelp is a simple ML resource search engine. You can use this search engine right now at https://jgreenemi.github.io/MLPleaseHelp/, provided via Github Pages.


Demos and Scripts

The Bot - Example of how the neural network learns to predict the angle between two points created with Synaptic.
Half Beer - Beer glass classifier created with Synaptic.
NSFWJS - Indecent content checker with TensorFlow.js
Rock Paper Scissors - Rock Paper Scissors trained in the browser with TensorFlow.js


Julia

General-Purpose Machine Learning

MachineLearning - Julia Machine Learning library. [Deprecated]
MLBase - A set of functions to support the development of machine learning algorithms.
PGM - A Julia framework for probabilistic graphical models.
DA - Julia package for Regularized Discriminant Analysis.
Regression - Algorithms for regression analysis (e.g. linear regression and logistic regression). [Deprecated]
Local Regression - Local regression, so smooooth!
Naive Bayes - Simple Naive Bayes implementation in Julia. [Deprecated]
Mixed Models - A Julia package for fitting (statistical) mixed-effects models.
Simple MCMC - basic mcmc sampler implemented in Julia. [Deprecated]
Distances - Julia module for Distance evaluation.
Decision Tree - Decision Tree Classifier and Regressor.
Neural - A neural network in Julia.
MCMC - MCMC tools for Julia. [Deprecated]
Mamba - Markov chain Monte Carlo (MCMC) for Bayesian analysis in Julia.
GLM - Generalized linear models in Julia.
Gaussian Processes - Julia package for Gaussian processes.
Online Learning [Deprecated]
GLMNet - Julia wrapper for fitting Lasso/ElasticNet GLM models using glmnet.
Clustering - Basic functions for clustering data: k-means, dp-means, etc.
SVM - SVM's for Julia. [Deprecated]
Kernel Density - Kernel density estimators for julia.
MultivariateStats - Methods for dimensionality reduction.
NMF - A Julia package for non-negative matrix factorization.
ANN - Julia artificial neural networks. [Deprecated]
Mocha - Deep Learning framework for Julia inspired by Caffe. [Deprecated]
XGBoost - eXtreme Gradient Boosting Package in Julia.
ManifoldLearning - A Julia package for manifold learning and nonlinear dimensionality reduction.
MXNet - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.
Merlin - Flexible Deep Learning Framework in Julia.
ROCAnalysis - Receiver Operating Characteristics and functions for evaluation probabilistic binary classifiers.
GaussianMixtures - Large scale Gaussian Mixture Models.
ScikitLearn - Julia implementation of the scikit-learn API.
Knet - Koç University Deep Learning Framework.
Flux - Relax! Flux is the ML library that doesn't make you tensor


Natural Language Processing

Topic Models - TopicModels for Julia. [Deprecated]
Text Analysis - Julia package for text analysis.
Word Tokenizers - Tokenizers for Natural Language Processing in Julia
Corpus Loaders - A julia package providing a variety of loaders for various NLP corpora.
Embeddings - Functions and data dependencies for loading various word embeddings
Languages - Julia package for working with various human languages
WordNet - A Julia package for Princeton's WordNet


Data Analysis / Data Visualization

Graph Layout - Graph layout algorithms in pure Julia.
LightGraphs - Graph modeling and analysis.
Data Frames Meta - Metaprogramming tools for DataFrames.
Julia Data - library for working with tabular data in Julia. [Deprecated]
Data Read - Read files from Stata, SAS, and SPSS.
Hypothesis Tests - Hypothesis tests for Julia.
Gadfly - Crafty statistical graphics for Julia.
Stats - Statistical tests for Julia.
RDataSets - Julia package for loading many of the data sets available in R.
DataFrames - library for working with tabular data in Julia.
Distributions - A Julia package for probability distributions and associated functions.
Data Arrays - Data structures that allow missing values. [Deprecated]
Time Series - Time series toolkit for Julia.
Sampling - Basic sampling algorithms for Julia.


Misc Stuff / Presentations

DSP - Digital Signal Processing (filtering, periodograms, spectrograms, window functions).
JuliaCon Presentations - Presentations for JuliaCon.
SignalProcessing - Signal Processing tools for Julia.
Images - An image library for Julia.
DataDeps - Reproducible data setup for reproducible science.


Lua

General-Purpose Machine Learning

Torch7

cephes - Cephes mathematical functions library, wrapped for Torch. Provides and wraps the 180+ special mathematical functions from the Cephes mathematical library, developed by Stephen L. Moshier. It is used, among many other places, at the heart of SciPy. [Deprecated]
autograd - Autograd automatically differentiates native Torch code. Inspired by the original Python version.
graph - Graph package for Torch. [Deprecated]
randomkit - Numpy's randomkit, wrapped for Torch. [Deprecated]
signal - A signal processing toolbox for Torch-7. FFT, DCT, Hilbert, cepstrums, stft.
nn - Neural Network package for Torch.
torchnet - framework for torch which provides a set of abstractions aiming at encouraging code re-use as well as encouraging modular programming.
nngraph - This package provides graphical computation for nn library in Torch7.
nnx - A completely unstable and experimental package that extends Torch's builtin nn library.
rnn - A Recurrent Neural Network library that extends Torch's nn. RNNs, LSTMs, GRUs, BRNNs, BLSTMs, etc.
dpnn - Many useful features that aren't part of the main nn package.
dp - A deep learning library designed for streamlining research and development using the Torch7 distribution. It emphasizes flexibility through the elegant use of object-oriented design patterns. [Deprecated]
optim - An optimization library for Torch. SGD, Adagrad, Conjugate-Gradient, LBFGS, RProp and more.
unsup - A package for unsupervised learning in Torch. Provides modules that are compatible with nn (LinearPsd, ConvPsd, AutoEncoder, ...), and self-contained algorithms (k-means, PCA). [Deprecated]
manifold - A package to manipulate manifolds.
svm - Torch-SVM library. [Deprecated]
lbfgs - FFI Wrapper for liblbfgs. [Deprecated]
vowpalwabbit - An old vowpalwabbit interface to torch. [Deprecated]
OpenGM - OpenGM is a C++ library for graphical modeling, and inference. The Lua bindings provide a simple way of describing graphs, from Lua, and then optimizing them with OpenGM. [Deprecated]
spaghetti - Spaghetti (sparse linear) module for torch7 by @MichaelMathieu [Deprecated]
LuaSHKit - A lua wrapper around the Locality sensitive hashing library SHKit [Deprecated]
kernel smoothing - KNN, kernel-weighted average, local linear regression smoothers. [Deprecated]
cutorch - Torch CUDA Implementation.
cunn - Torch CUDA Neural Network Implementation.
imgraph - An image/graph library for Torch. This package provides routines to construct graphs on images, segment them, build trees out of them, and convert them back to images. [Deprecated]
videograph - A video/graph library for Torch. This package provides routines to construct graphs on videos, segment them, build trees out of them, and convert them back to videos. [Deprecated]
saliency - code and tools around integral images. A library for finding interest points based on fast integral histograms. [Deprecated]
stitch - allows us to use hugin to stitch images and apply same stitching to a video sequence. [Deprecated]
sfm - A bundle adjustment/structure from motion package. [Deprecated]
fex - A package for feature extraction in Torch. Provides SIFT and dSIFT modules. [Deprecated]
OverFeat - A state-of-the-art generic dense feature extractor. [Deprecated]
wav2letter - a simple and efficient end-to-end Automatic Speech Recognition (ASR) system from Facebook AI Research.


Numeric Lua
Lunatic Python
SciLua
Lua - Numerical Algorithms [Deprecated]
Lunum [Deprecated]


Demos and Scripts

Core torch7 demos repository.

linear-regression, logistic-regression
face detector (training and detection as separate demos)
mst-based-segmenter
train-a-digit-classifier
train-autoencoder
optical flow demo
train-on-housenumbers
train-on-cifar
tracking with deep nets
kinect demo
filter-bank visualization
saliency-networks


Training a Convnet for the Galaxy-Zoo Kaggle challenge(CUDA demo)
Music Tagging - Music Tagging scripts for torch7.
torch-datasets - Scripts to load several popular datasets including:

BSR 500
CIFAR-10
COIL
Street View House Numbers
MNIST
NORB


Atari2600 - Scripts to generate a dataset with static frames from the Arcade Learning Environment.


Matlab

Computer Vision

Contourlets - MATLAB source code that implements the contourlet transform and its utility functions.
Shearlets - MATLAB code for shearlet transform.
Curvelets - The Curvelet transform is a higher dimensional generalization of the Wavelet transform designed to represent images at different scales and different angles.
Bandlets - MATLAB code for bandlet transform.
mexopencv - Collection and a development kit of MATLAB mex functions for OpenCV library.


Natural Language Processing

NLP - An NLP library for Matlab.


General-Purpose Machine Learning

Training a deep autoencoder or a classifier
on MNIST digits - Training a deep autoencoder or a classifier
on MNIST digits[DEEP LEARNING].
Convolutional-Recursive Deep Learning for 3D Object Classification - Convolutional-Recursive Deep Learning for 3D Object Classification[DEEP LEARNING].
Spider - The spider is intended to be a complete object orientated environment for machine learning in Matlab.
LibSVM - A Library for Support Vector Machines.
ThunderSVM - An Open-Source SVM Library on GPUs and CPUs
LibLinear - A Library for Large Linear Classification.
Machine Learning Module - Class on machine w/ PDF, lectures, code
Caffe - A deep learning framework developed with cleanliness, readability, and speed in mind.
Pattern Recognition Toolbox - A complete object-oriented environment for machine learning in Matlab.
Pattern Recognition and Machine Learning - This package contains the matlab implementation of the algorithms described in the book Pattern Recognition and Machine Learning by C. Bishop.
Optunity - A library dedicated to automated hyperparameter optimization with a simple, lightweight API to facilitate drop-in replacement of grid search. Optunity is written in Python but interfaces seamlessly with MATLAB.
MXNet - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.
Machine Learning in MatLab/Octave - examples of popular machine learning algorithms (neural networks, linear/logistic regressions, K-Means, etc.) with code examples and mathematics behind them being explained.


Data Analysis / Data Visualization

matlab_bgl - MatlabBGL is a Matlab package for working with graphs.
gaimc - Efficient pure-Matlab implementations of graph algorithms to complement MatlabBGL's mex functions.


.NET

Computer Vision

OpenCVDotNet - A wrapper for the OpenCV project to be used with .NET applications.
Emgu CV - Cross platform wrapper of OpenCV which can be compiled in Mono to be run on Windows, Linus, Mac OS X, iOS, and Android.
AForge.NET - Open source C# framework for developers and researchers in the fields of Computer Vision and Artificial Intelligence. Development has now shifted to GitHub.
Accord.NET - Together with AForge.NET, this library can provide image processing and computer vision algorithms to Windows, Windows RT and Windows Phone. Some components are also available for Java and Android.


Natural Language Processing

Stanford.NLP for .NET - A full port of Stanford NLP packages to .NET and also available precompiled as a NuGet package.


General-Purpose Machine Learning

Accord-Framework -The Accord.NET Framework is a complete framework for building machine learning, computer vision, computer audition, signal processing and statistical applications.
Accord.MachineLearning - Support Vector Machines, Decision Trees, Naive Bayesian models, K-means, Gaussian Mixture models and general algorithms such as Ransac, Cross-validation and Grid-Search for machine-learning applications. This package is part of the Accord.NET Framework.
DiffSharp - An automatic differentiation (AD) library providing exact and efficient derivatives (gradients, Hessians, Jacobians, directional derivatives, and matrix-free Hessian- and Jacobian-vector products) for machine learning and optimization applications. Operations can be nested to any level, meaning that you can compute exact higher-order derivatives and differentiate functions that are internally making use of differentiation, for applications such as hyperparameter optimization.
Encog - An advanced neural network and machine learning framework. Encog contains classes to create a wide variety of networks, as well as support classes to normalize and process data for these neural networks. Encog trains using multithreaded resilient propagation. Encog can also make use of a GPU to further speed processing time. A GUI based workbench is also provided to help model and train neural networks.
GeneticSharp - Multi-platform genetic algorithm library for .NET Core and .NET Framework. The library has several implementations of GA operators, like: selection, crossover, mutation, reinsertion and termination.
Infer.NET - Infer.NET is a framework for running Bayesian inference in graphical models. One can use Infer.NET to solve many different kinds of machine learning problems, from standard problems like classification, recommendation or clustering through to customised solutions to domain-specific problems. Infer.NET has been used in a wide variety of domains including information retrieval, bioinformatics, epidemiology, vision, and many others.
ML.NET - ML.NET is a cross-platform open-source machine learning framework which makes machine learning accessible to .NET developers. ML.NET was originally developed in Microsoft Research and evolved into a significant framework over the last decade and is used across many product groups in Microsoft like Windows, Bing, PowerPoint, Excel and more.
Neural Network Designer - DBMS management system and designer for neural networks. The designer application is developed using WPF, and is a user interface which allows you to design your neural network, query the network, create and configure chat bots that are capable of asking questions and learning from your feed back. The chat bots can even scrape the internet for information to return in their output as well as to use for learning.
Synapses - Neural network library in F#.
Vulpes - Deep belief and deep learning implementation written in F# and leverages CUDA GPU execution with Alea.cuBase.


Data Analysis / Data Visualization

numl - numl is a machine learning library intended to ease the use of using standard modeling techniques for both prediction and clustering.
Math.NET Numerics - Numerical foundation of the Math.NET project, aiming to provide methods and algorithms for numerical computations in science, engineering and every day use. Supports .Net 4.0, .Net 3.5 and Mono on Windows, Linux and Mac; Silverlight 5, WindowsPhone/SL 8, WindowsPhone 8.1 and Windows 8 with PCL Portable Profiles 47 and 344; Android/iOS with Xamarin.
Sho - Sho is an interactive environment for data analysis and scientific computing that lets you seamlessly connect scripts (in IronPython) with compiled code (in .NET) to enable fast and flexible prototyping. The environment includes powerful and efficient libraries for linear algebra as well as data visualization that can be used from any .NET language, as well as a feature-rich interactive shell for rapid development.


Objective C

General-Purpose Machine Learning

YCML - A Machine Learning framework for Objective-C and Swift (OS X / iOS).
MLPNeuralNet - Fast multilayer perceptron neural network library for iOS and Mac OS X. MLPNeuralNet predicts new examples by trained neural network. It is built on top of the Apple's Accelerate Framework, using vectorized operations and hardware acceleration if available. [Deprecated]
MAChineLearning - An Objective-C multilayer perceptron library, with full support for training through backpropagation. Implemented using vDSP and vecLib, it's 20 times faster than its Java equivalent. Includes sample code for use from Swift.
BPN-NeuralNetwork - It implemented 3 layers neural network ( Input Layer, Hidden Layer and Output Layer ) and it named Back Propagation Neural Network (BPN). This network can be used in products recommendation, user behavior analysis, data mining and data analysis. [Deprecated]
Multi-Perceptron-NeuralNetwork - it implemented multi-perceptrons neural network (ニューラルネットワーク) based on Back Propagation Neural Network (BPN) and designed unlimited-hidden-layers.
KRHebbian-Algorithm - It is a non-supervisor and self-learning algorithm (adjust the weights) in neural network of Machine Learning. [Deprecated]
KRKmeans-Algorithm - It implemented K-Means the clustering and classification algorithm. It could be used in data mining and image compression. [Deprecated]
KRFuzzyCMeans-Algorithm - It implemented Fuzzy C-Means (FCM) the fuzzy clustering / classification algorithm on Machine Learning. It could be used in data mining and image compression. [Deprecated]


OCaml

General-Purpose Machine Learning

Oml - A general statistics and machine learning library.
GPR - Efficient Gaussian Process Regression in OCaml.
Libra-Tk - Algorithms for learning and inference with discrete probabilistic models.
TensorFlow - OCaml bindings for TensorFlow.


Perl

Data Analysis / Data Visualization

Perl Data Language, a pluggable architecture for data and image processing, which can
be used for machine learning.


General-Purpose Machine Learning

MXnet for Deep Learning, in Perl,
also released in CPAN.
Perl Data Language,
using AWS machine learning platform from Perl.
Algorithm::SVMLight,
implementation of Support Vector Machines with SVMLight under it. [Deprecated]
Several machine learning and artificial intelligence models are
included in the AI
namespace. For instance, you can
find Naïve Bayes.


Perl 6

Support Vector Machines
Naïve Bayes

Data Analysis / Data Visualization

Perl Data Language,
a pluggable architecture for data and image processing, which can
be
used for machine learning.

General-Purpose Machine Learning

PHP

Natural Language Processing

jieba-php - Chinese Words Segmentation Utilities.


General-Purpose Machine Learning

PHP-ML - Machine Learning library for PHP. Algorithms, Cross Validation, Neural Network, Preprocessing, Feature Extraction and much more in one library.
PredictionBuilder - A library for machine learning that builds predictions using a linear regression.
Rubix ML - A high-level machine learning (ML) library that lets you build programs that learn from data using the PHP language.
19 Questions - A machine learning / bayesian inference assigning attributes to objects.


Python

Computer Vision

Scikit-Image - A collection of algorithms for image processing in Python.
SimpleCV - An open source computer vision framework that gives access to several high-powered computer vision libraries, such as OpenCV. Written on Python and runs on Mac, Windows, and Ubuntu Linux.
Vigranumpy - Python bindings for the VIGRA C++ computer vision library.
OpenFace - Free and open source face recognition with deep neural networks.
PCV - Open source Python module for computer vision. [Deprecated]
face_recognition - Face recognition library that recognize and manipulate faces from Python or from the command line.
dockerface - Easy to install and use deep learning Faster R-CNN face detection for images and video in a docker container.
Detectron - FAIR's software system that implements state-of-the-art object detection algorithms, including Mask R-CNN. It is written in Python and powered by the Caffe2 deep learning framework.
albumentations - А fast and framework agnostic image augmentation library that implements a diverse set of augmentation techniques. Supports classification, segmentation, detection out of the box. Was used to win a number of Deep Learning competitions at Kaggle, Topcoder and those that were a part of the CVPR workshops.
pytessarct - Python-tesseract is an optical character recognition (OCR) tool for python. That is, it will recognize and ""read"" the text embedded in images.Python-tesseract is a wrapper for Google's Tesseract-OCR Engine>.
imutils - A library containg Convenience functions to make basic image processing operations such as translation, rotation, resizing, skeletonization, and displaying Matplotlib images easier with OpenCV and Python.
PyTorchCV - A PyTorch-Based Framework for Deep Learning in Computer Vision.
neural-style-pt - A PyTorch implementation of Justin Johnson's neural-style (neural style transfer).


Natural Language Processing

pkuseg-python - A better version of Jieba, developed by Peking University.
NLTK - A leading platform for building Python programs to work with human language data.
Pattern - A web mining module for the Python programming language. It has tools for natural language processing, machine learning, among others.
Quepy - A python framework to transform natural language questions to queries in a database query language.
TextBlob - Providing a consistent API for diving into common natural language processing (NLP) tasks. Stands on the giant shoulders of NLTK and Pattern, and plays nicely with both.
YAlign - A sentence aligner, a friendly tool for extracting parallel sentences from comparable corpora. [Deprecated]
jieba - Chinese Words Segmentation Utilities.
SnowNLP - A library for processing Chinese text.
spammy - A library for email Spam filtering built on top of nltk
loso - Another Chinese segmentation library. [Deprecated]
genius - A Chinese segment base on Conditional Random Field.
KoNLPy - A Python package for Korean natural language processing.
nut - Natural language Understanding Toolkit. [Deprecated]
Rosetta - Text processing tools and wrappers (e.g. Vowpal Wabbit)
BLLIP Parser - Python bindings for the BLLIP Natural Language Parser (also known as the Charniak-Johnson parser). [Deprecated]
PyNLPl - Python Natural Language Processing Library. General purpose NLP library for Python. Also contains some specific modules for parsing common NLP formats, most notably for FoLiA, but also ARPA language models, Moses phrasetables, GIZA++ alignments.
python-ucto - Python binding to ucto (a unicode-aware rule-based tokenizer for various languages).
python-frog - Python binding to Frog, an NLP suite for Dutch. (pos tagging, lemmatisation, dependency parsing, NER)
python-zpar - Python bindings for ZPar, a statistical part-of-speech-tagger, constiuency parser, and dependency parser for English.
colibri-core - Python binding to C++ library for extracting and working with with basic linguistic constructions such as n-grams and skipgrams in a quick and memory-efficient way.
spaCy - Industrial strength NLP with Python and Cython.
PyStanfordDependencies - Python interface for converting Penn Treebank trees to Stanford Dependencies.
Distance - Levenshtein and Hamming distance computation. [Deprecated]
Fuzzy Wuzzy - Fuzzy String Matching in Python.
jellyfish - a python library for doing approximate and phonetic matching of strings.
editdistance - fast implementation of edit distance.
textacy - higher-level NLP built on Spacy.
stanford-corenlp-python - Python wrapper for Stanford CoreNLP [Deprecated]
CLTK - The Classical Language Toolkit.
rasa_nlu - turn natural language into structured data.
yase - Transcode sentence (or other sequence) to list of word vector .
Polyglot - Multilingual text (NLP) processing toolkit.
DrQA - Reading Wikipedia to answer open-domain questions.
Dedupe - A python library for accurate and scalable fuzzy matching, record deduplication and entity-resolution.
Snips NLU - Natural Language Understanding library for intent classification and entity extraction
NeuroNER - Named-entity recognition using neural networks providing state-of-the-art-results
DeepPavlov - conversational AI library with many pretrained Russian NLP models.
BigARTM - topic modelling platform.


General-Purpose Machine Learning

PyOD -> Python Outlier Detection, comprehensive and scalable Python toolkit for detecting outlying objects in multivariate data. Featured for Advanced models, including Neural Networks/Deep Learning and Outlier Ensembles.
steppy -> Lightweight, Python library for fast and reproducible machine learning experimentation. Introduces very simple interface that enables clean machine learning pipeline design.
steppy-toolkit -> Curated collection of the neural networks, transformers and models that make your machine learning work faster and more effective.
CNTK - Microsoft Cognitive Toolkit (CNTK), an open source deep-learning toolkit. Documentation can be found here.
auto_ml - Automated machine learning for production and analytics. Lets you focus on the fun parts of ML, while outputting production-ready code, and detailed analytics of your dataset and results. Includes support for NLP, XGBoost, CatBoost, LightGBM, and soon, deep learning.
machine learning - automated build consisting of a web-interface, and set of programmatic-interface API, for support vector machines. Corresponding dataset(s) are stored into a SQL database, then generated model(s) used for prediction(s), are stored into a NoSQL datastore.
XGBoost - Python bindings for eXtreme Gradient Boosting (Tree) Library.
Apache SINGA - An Apache Incubating project for developing an open source machine learning library.
Bayesian Methods for Hackers - Book/iPython notebooks on Probabilistic Programming in Python.
Featureforge A set of tools for creating and testing machine learning features, with a scikit-learn compatible API.
MLlib in Apache Spark - Distributed machine learning library in Spark
Hydrosphere Mist - a service for deployment Apache Spark MLLib machine learning models as realtime, batch or reactive web services.
scikit-learn - A Python module for machine learning built on top of SciPy.
metric-learn - A Python module for metric learning.
SimpleAI Python implementation of many of the artificial intelligence algorithms described on the book ""Artificial Intelligence, a Modern Approach"". It focuses on providing an easy to use, well documented and tested library.
astroML - Machine Learning and Data Mining for Astronomy.
graphlab-create - A library with various machine learning models (regression, clustering, recommender systems, graph analytics, etc.) implemented on top of a disk-backed DataFrame.
BigML - A library that contacts external servers.
pattern - Web mining module for Python.
NuPIC - Numenta Platform for Intelligent Computing.
Pylearn2 - A Machine Learning library based on Theano. [Deprecated]
keras - High-level neural networks frontend for TensorFlow, CNTK and Theano.
Lasagne - Lightweight library to build and train neural networks in Theano.
hebel - GPU-Accelerated Deep Learning Library in Python. [Deprecated]
Chainer - Flexible neural network framework.
prophet - Fast and automated time series forecasting framework by Facebook.
gensim - Topic Modelling for Humans.
topik - Topic modelling toolkit. [Deprecated]
PyBrain - Another Python Machine Learning Library.
Brainstorm - Fast, flexible and fun neural networks. This is the successor of PyBrain.
Surprise - A scikit for building and analyzing recommender systems.
Crab - A flexible, fast recommender engine. [Deprecated]
python-recsys - A Python library for implementing a Recommender System.
thinking bayes - Book on Bayesian Analysis.
Image-to-Image Translation with Conditional Adversarial Networks - Implementation of image to image (pix2pix) translation from the paper by isola et al.[DEEP LEARNING]
Restricted Boltzmann Machines -Restricted Boltzmann Machines in Python. [DEEP LEARNING]
Bolt - Bolt Online Learning Toolbox. [Deprecated]
CoverTree - Python implementation of cover trees, near-drop-in replacement for scipy.spatial.kdtree [Deprecated]
nilearn - Machine learning for NeuroImaging in Python.
neuropredict - Aimed at novice machine learners and non-expert programmers, this package offers easy (no coding needed) and comprehensive machine learning (evaluation and full report of predictive performance WITHOUT requiring you to code) in Python for NeuroImaging and any other type of features. This is aimed at absorbing the much of the ML workflow, unlike other packages like nilearn and pymvpa, which require you to learn their API and code to produce anything useful.
imbalanced-learn - Python module to perform under sampling and over sampling with various techniques.
Shogun - The Shogun Machine Learning Toolbox.
Pyevolve - Genetic algorithm framework. [Deprecated]
Caffe - A deep learning framework developed with cleanliness, readability, and speed in mind.
breze - Theano based library for deep and recurrent neural networks.
Cortex - Open source platform for deploying machine learning models in production.
pyhsmm - library for approximate unsupervised inference in Bayesian Hidden Markov Models (HMMs) and explicit-duration Hidden semi-Markov Models (HSMMs), focusing on the Bayesian Nonparametric extensions, the HDP-HMM and HDP-HSMM, mostly with weak-limit approximations.
mrjob - A library to let Python program run on Hadoop.
SKLL - A wrapper around scikit-learn that makes it simpler to conduct experiments.
neurolab
Spearmint - Spearmint is a package to perform Bayesian optimization according to the algorithms outlined in the paper: Practical Bayesian Optimization of Machine Learning Algorithms. Jasper Snoek, Hugo Larochelle and Ryan P. Adams. Advances in Neural Information Processing Systems, 2012. [Deprecated]
Pebl - Python Environment for Bayesian Learning. [Deprecated]
Theano - Optimizing GPU-meta-programming code generating array oriented optimizing math compiler in Python.
TensorFlow - Open source software library for numerical computation using data flow graphs.
pomegranate - Hidden Markov Models for Python, implemented in Cython for speed and efficiency.
python-timbl - A Python extension module wrapping the full TiMBL C++ programming interface. Timbl is an elaborate k-Nearest Neighbours machine learning toolkit.
deap - Evolutionary algorithm framework.
pydeep - Deep Learning In Python. [Deprecated]
mlxtend - A library consisting of useful tools for data science and machine learning tasks.
neon - Nervana's high-performance Python-based Deep Learning framework [DEEP LEARNING].
Optunity - A library dedicated to automated hyperparameter optimization with a simple, lightweight API to facilitate drop-in replacement of grid search.
Neural Networks and Deep Learning - Code samples for my book ""Neural Networks and Deep Learning"" [DEEP LEARNING].
Annoy - Approximate nearest neighbours implementation.
TPOT - Tool that automatically creates and optimizes machine learning pipelines using genetic programming. Consider it your personal data science assistant, automating a tedious part of machine learning.
pgmpy A python library for working with Probabilistic Graphical Models.
DIGITS - The Deep Learning GPU Training System (DIGITS) is a web application for training deep learning models.
Orange - Open source data visualization and data analysis for novices and experts.
MXNet - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.
milk - Machine learning toolkit focused on supervised classification. [Deprecated]
TFLearn - Deep learning library featuring a higher-level API for TensorFlow.
REP - an IPython-based environment for conducting data-driven research in a consistent and reproducible way. REP is not trying to substitute scikit-learn, but extends it and provides better user experience. [Deprecated]
rgf_python - Python bindings for Regularized Greedy Forest (Tree) Library.
skbayes - Python package for Bayesian Machine Learning with scikit-learn API.
fuku-ml - Simple machine learning library, including Perceptron, Regression, Support Vector Machine, Decision Tree and more, it's easy to use and easy to learn for beginners.
Xcessiv - A web-based application for quick, scalable, and automated hyperparameter tuning and stacked ensembling.
PyTorch - Tensors and Dynamic neural networks in Python with strong GPU acceleration
ML-From-Scratch - Implementations of Machine Learning models from scratch in Python with a focus on transparency. Aims to showcase the nuts and bolts of ML in an accessible way.
Edward - A library for probabilistic modeling, inference, and criticism. Built on top of TensorFlow.
xRBM - A library for Restricted Boltzmann Machine (RBM) and its conditional variants in Tensorflow.
CatBoost - General purpose gradient boosting on decision trees library with categorical features support out of the box. It is easy to install, well documented and supports CPU and GPU (even multi-GPU) computation.
stacked_generalization - Implementation of machine learning stacking technic as handy library in Python.
modAL - A modular active learning framework for Python, built on top of scikit-learn.
Cogitare: A Modern, Fast, and Modular Deep Learning and Machine Learning framework for Python.
Parris - Parris, the automated infrastructure setup tool for machine learning algorithms.
neonrvm - neonrvm is an open source machine learning library based on RVM technique. It's written in C programming language and comes with Python programming language bindings.
Turi Create - Machine learning from Apple. Turi Create simplifies the development of custom machine learning models. You don't have to be a machine learning expert to add recommendations, object detection, image classification, image similarity or activity classification to your app.
xLearn - A high performance, easy-to-use, and scalable machine learning package, which can be used to solve large-scale machine learning problems. xLearn is especially useful for solving machine learning problems on large-scale sparse data, which is very common in Internet services such as online advertisement and recommender systems.
mlens - A high performance, memory efficient, maximally parallelized ensemble learning, integrated with scikit-learn.
Netron - Visualizer for machine learning models.
Thampi - Machine Learning Prediction System on AWS Lambda
MindsDB - Open Source framework to streamline use of neural networks.
Microsoft Recommenders: Examples and best practices for building recommendation systems, provided as Jupyter notebooks. The repo contains some of the latest state of the art algorithms from Microsoft Research as well as from other companies and institutions.
StellarGraph: Machine Learning on Graphs, a Python library for machine learning on graph-structured (network-structured) data.
BentoML: Toolkit for package and deploy machine learning models for serving in production
MiraiML: An asynchronous engine for continuous & autonomous machine learning, built for real-time usage.
numpy-ML: Reference implementations of ML models written in numpy
creme: A framework for online machine learning.
Neuraxle: A framework providing the right abstractions to ease research, development, and deployment of your ML pipelines.
Cornac - A comparative framework for multimodal recommender systems with a focus on models leveraging auxiliary data.
JAX - JAX is Autograd and XLA, brought together for high-performance machine learning research.
fast.ai - A library simplifies training fast and accurate neural nets using modern best practices and already supports  vision, text, tabular, and collab (collaborative filtering) models ""out of the box""
Catalyst - High-level utils for PyTorch DL & RL research. It was developed with a focus on reproducibility, fast experimentation and code/ideas reusing. Being able to research/develop something new, rather than write another regular train loop.
Fastai - High-level wrapper built on the top of Pytorch which supports vision, text, tabular data and collaborative filtering.


Data Analysis / Data Visualization

SciPy - A Python-based ecosystem of open-source software for mathematics, science, and engineering.
NumPy - A fundamental package for scientific computing with Python.
Numba - Python JIT (just in time) compiler to LLVM aimed at scientific Python by the developers of Cython and NumPy.
Mars - A tensor-based framework for large-scale data computation which often regarded as a parallel and distributed version of NumPy.
NetworkX - A high-productivity software for complex networks.
igraph - binding to igraph library - General purpose graph library.
Pandas - A library providing high-performance, easy-to-use data structures and data analysis tools.
Open Mining - Business Intelligence (BI) in Python (Pandas web interface) [Deprecated]
PyMC - Markov Chain Monte Carlo sampling toolkit.
zipline - A Pythonic algorithmic trading library.
PyDy - Short for Python Dynamics, used to assist with workflow in the modeling of dynamic motion based around NumPy, SciPy, IPython, and matplotlib.
SymPy - A Python library for symbolic mathematics.
statsmodels - Statistical modeling and econometrics in Python.
astropy - A community Python library for Astronomy.
matplotlib - A Python 2D plotting library.
bokeh - Interactive Web Plotting for Python.
plotly - Collaborative web plotting for Python and matplotlib.
altair - A Python to Vega translator.
d3py - A plotting library for Python, based on D3.js.
PyDexter - Simple plotting for Python. Wrapper for D3xterjs; easily render charts in-browser.
ggplot - Same API as ggplot2 for R. [Deprecated]
ggfortify - Unified interface to ggplot2 popular R packages.
Kartograph.py - Rendering beautiful SVG maps in Python.
pygal - A Python SVG Charts Creator.
PyQtGraph - A pure-python graphics and GUI library built on PyQt4 / PySide and NumPy.
pycascading [Deprecated]
Petrel - Tools for writing, submitting, debugging, and monitoring Storm topologies in pure Python.
Blaze - NumPy and Pandas interface to Big Data.
emcee - The Python ensemble sampling toolkit for affine-invariant MCMC.
windML - A Python Framework for Wind Energy Analysis and Prediction.
vispy - GPU-based high-performance interactive OpenGL 2D/3D data visualization library.
cerebro2 A web-based visualization and debugging platform for NuPIC. [Deprecated]
NuPIC Studio An all-in-one NuPIC Hierarchical Temporal Memory visualization and debugging super-tool! [Deprecated]
SparklingPandas Pandas on PySpark (POPS).
Seaborn - A python visualization library based on matplotlib.
bqplot - An API for plotting in Jupyter (IPython).
pastalog - Simple, realtime visualization of neural network training performance.
Superset - A data exploration platform designed to be visual, intuitive, and interactive.
Dora - Tools for exploratory data analysis in Python.
Ruffus - Computation Pipeline library for python.
SOMPY - Self Organizing Map written in Python (Uses neural networks for data analysis).
somoclu Massively parallel self-organizing maps: accelerate training on multicore CPUs, GPUs, and clusters, has python API.
HDBScan - implementation of the hdbscan algorithm in Python - used for clustering
visualize_ML - A python package for data exploration and data analysis. [Deprecated]
scikit-plot - A visualization library for quick and easy generation of common plots in data analysis and machine learning.
Bowtie - A dashboard library for interactive visualizations using flask socketio and react.
lime - Lime is about explaining what machine learning classifiers (or models) are doing. It is able to explain any black box classifier, with two or more classes.
PyCM - PyCM is a multi-class confusion matrix library written in Python that supports both input data vectors and direct matrix, and a proper tool for post-classification model evaluation that supports most classes and overall statistics parameters
Dash - A framework for creating analytical web applications built on top of Plotly.js, React, and Flask
Lambdo - A workflow engine for solving machine learning problems by combining in one analysis pipeline (i) feature engineering and machine learning (ii) model training and prediction (iii) table population and column evaluation via user-defined (Python) functions.
TensorWatch - Debugging and visualization tool for machine learning and data science. It extensively leverages Jupyter Notebook to show real-time visualizations of data in running processes such as machine learning training.
dowel - A little logger for machine learning research. Output any object to the terminal, CSV, TensorBoard, text logs on disk, and more with just one call to logger.log().


Misc Scripts / iPython Notebooks / Codebases

Map/Reduce implementations of common ML algorithms: Jupyter notebooks that cover how to implement from scratch different ML algorithms (ordinary least squares, gradient descent, k-means, alternating least squares), using Python NumPy, and how to then make these implementations scalable using Map/Reduce and Spark.
BioPy - Biologically-Inspired and Machine Learning Algorithms in Python. [Deprecated]
SVM Explorer - Interactive SVM Explorer, using Dash and scikit-learn
pattern_classification
thinking stats 2
hyperopt
numpic
2012-paper-diginorm
A gallery of interesting IPython notebooks
ipython-notebooks
data-science-ipython-notebooks - Continually updated Data Science Python Notebooks: Spark, Hadoop MapReduce, HDFS, AWS, Kaggle, scikit-learn, matplotlib, pandas, NumPy, SciPy, and various command lines.
decision-weights
Sarah Palin LDA - Topic Modeling the Sarah Palin emails.
Diffusion Segmentation - A collection of image segmentation algorithms based on diffusion methods.
Scipy Tutorials - SciPy tutorials. This is outdated, check out scipy-lecture-notes.
Crab - A recommendation engine library for Python.
BayesPy - Bayesian Inference Tools in Python.
scikit-learn tutorials - Series of notebooks for learning scikit-learn.
sentiment-analyzer - Tweets Sentiment Analyzer
sentiment_classifier - Sentiment classifier using word sense disambiguation.
group-lasso - Some experiments with the coordinate descent algorithm used in the (Sparse) Group Lasso model.
jProcessing - Kanji / Hiragana / Katakana to Romaji Converter. Edict Dictionary & parallel sentences Search. Sentence Similarity between two JP Sentences. Sentiment Analysis of Japanese Text. Run Cabocha(ISO--8859-1 configured) in Python.
mne-python-notebooks - IPython notebooks for EEG/MEG data processing using mne-python.
Neon Course - IPython notebooks for a complete course around understanding Nervana's Neon.
pandas cookbook - Recipes for using Python's pandas library.
climin - Optimization library focused on machine learning, pythonic implementations of gradient descent, LBFGS, rmsprop, adadelta and others.
Allen Downey’s Data Science Course - Code for Data Science at Olin College, Spring 2014.
Allen Downey’s Think Bayes Code - Code repository for Think Bayes.
Allen Downey’s Think Complexity Code - Code for Allen Downey's book Think Complexity.
Allen Downey’s Think OS Code - Text and supporting code for Think OS: A Brief Introduction to Operating Systems.
Python Programming for the Humanities - Course for Python programming for the Humanities, assuming no prior knowledge. Heavy focus on text processing / NLP.
GreatCircle - Library for calculating great circle distance.
Optunity examples - Examples demonstrating how to use Optunity in synergy with machine learning libraries.
Dive into Machine Learning  with Python Jupyter notebook and scikit-learn - ""I learned Python by hacking first, and getting serious later. I wanted to do this with Machine Learning. If this is your style, join me in getting a bit ahead of yourself.""
TDB - TensorDebugger (TDB) is a visual debugger for deep learning. It features interactive, node-by-node debugging and visualization for TensorFlow.
Suiron - Machine Learning for RC Cars.
Introduction to machine learning with scikit-learn - IPython notebooks from Data School's video tutorials on scikit-learn.
Practical XGBoost in Python - comprehensive online course about using XGBoost in Python.
Introduction to Machine Learning with Python - Notebooks and code for the book ""Introduction to Machine Learning with Python""
Pydata book - Materials and IPython notebooks for ""Python for Data Analysis"" by Wes McKinney, published by O'Reilly Media
Homemade Machine Learning - Python examples of popular machine learning algorithms with interactive Jupyter demos and math being explained
Prodmodel - Build tool for data science pipelines.
the-elements-of-statistical-learning - This repository contains Jupyter notebooks implementing the algorithms found in the book and summary of the textbook.


Neural Networks

nn_builder - nn_builder is a python package that lets you build neural networks in 1 line
NeuralTalk - NeuralTalk is a Python+numpy project for learning Multimodal Recurrent Neural Networks that describe images with sentences.
Neuron - Neuron is simple class for time series predictions. It's utilize LNU (Linear Neural Unit), QNU (Quadratic Neural Unit), RBF (Radial Basis Function), MLP (Multi Layer Perceptron), MLP-ELM (Multi Layer Perceptron - Extreme Learning Machine) neural networks learned with Gradient descent or LeLevenberg–Marquardt algorithm.
=======
NeuralTalk - NeuralTalk is a Python+numpy project for learning Multimodal Recurrent Neural Networks that describe images with sentences. [Deprecated]
Neuron - Neuron is simple class for time series predictions. It's utilize LNU (Linear Neural Unit), QNU (Quadratic Neural Unit), RBF (Radial Basis Function), MLP (Multi Layer Perceptron), MLP-ELM (Multi Layer Perceptron - Extreme Learning Machine) neural networks learned with Gradient descent or LeLevenberg–Marquardt algorithm. [Deprecated]
Data Driven Code - Very simple implementation of neural networks for dummies in python without using any libraries, with detailed comments.
Machine Learning, Data Science and Deep Learning with Python - LiveVideo course that covers machine learning, Tensorflow, artificial intelligence, and neural networks.


Kaggle Competition Source Code

open-solution-home-credit -> source code and experiments results for Home Credit Default Risk.
open-solution-googleai-object-detection -> source code and experiments results for Google AI Open Images - Object Detection Track.
open-solution-salt-identification -> source code and experiments results for TGS Salt Identification Challenge.
open-solution-ship-detection -> source code and experiments results for Airbus Ship Detection Challenge.
open-solution-data-science-bowl-2018 -> source code and experiments results for 2018 Data Science Bowl.
open-solution-value-prediction -> source code and experiments results for Santander Value Prediction Challenge.
open-solution-toxic-comments -> source code for Toxic Comment Classification Challenge.
wiki challenge - An implementation of Dell Zhang's solution to Wikipedia's Participation Challenge on Kaggle.
kaggle insults - Kaggle Submission for ""Detecting Insults in Social Commentary"".
kaggle_acquire-valued-shoppers-challenge - Code for the Kaggle acquire valued shoppers challenge.
kaggle-cifar - Code for the CIFAR-10 competition at Kaggle, uses cuda-convnet.
kaggle-blackbox - Deep learning made easy.
kaggle-accelerometer - Code for Accelerometer Biometric Competition at Kaggle.
kaggle-advertised-salaries - Predicting job salaries from ads - a Kaggle competition.
kaggle amazon - Amazon access control challenge.
kaggle-bestbuy_big - Code for the Best Buy competition at Kaggle.
kaggle-bestbuy_small
Kaggle Dogs vs. Cats - Code for Kaggle Dogs vs. Cats competition.
Kaggle Galaxy Challenge - Winning solution for the Galaxy Challenge on Kaggle.
Kaggle Gender - A Kaggle competition: discriminate gender based on handwriting.
Kaggle Merck - Merck challenge at Kaggle.
Kaggle Stackoverflow - Predicting closed questions on Stack Overflow.
kaggle_acquire-valued-shoppers-challenge - Code for the Kaggle acquire valued shoppers challenge.
wine-quality - Predicting wine quality.


Reinforcement Learning

DeepMind Lab - DeepMind Lab is a 3D learning environment based on id Software's Quake III Arena via ioquake3 and other open source software. Its primary purpose is to act as a testbed for research in artificial intelligence, especially deep reinforcement learning.
Gym - OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms.
Serpent.AI - Serpent.AI is a game agent framework that allows you to turn any video game you own into a sandbox to develop AI and machine learning experiments. For both researchers and hobbyists.
ViZDoom - ViZDoom allows developing AI bots that play Doom using only the visual information (the screen buffer). It is primarily intended for research in machine visual learning, and deep reinforcement learning, in particular.
Roboschool - Open-source software for robot simulation, integrated with OpenAI Gym.
Retro - Retro Games in Gym
SLM Lab - Modular Deep Reinforcement Learning framework in PyTorch.
Coach - Reinforcement Learning Coach by Intel® AI Lab enables easy experimentation with state of the art Reinforcement Learning algorithms
garage - A toolkit for reproducible reinforcement learning research
metaworld - An open source robotics benchmark for meta- and multi-task reinforcement learning


Ruby

Natural Language Processing

Awesome NLP with Ruby - Curated link list for practical natural language processing in Ruby.
Treat - Text REtrieval and Annotation Toolkit, definitely the most comprehensive toolkit I’ve encountered so far for Ruby.
Stemmer - Expose libstemmer_c to Ruby. [Deprecated]
Raspel - raspell is an interface binding for ruby. [Deprecated]
UEA Stemmer - Ruby port of UEALite Stemmer - a conservative stemmer for search and indexing.
Twitter-text-rb - A library that does auto linking and extraction of usernames, lists and hashtags in tweets.


General-Purpose Machine Learning

Awesome Machine Learning with Ruby - Curated list of ML related resources for Ruby.
Ruby Machine Learning - Some Machine Learning algorithms, implemented in Ruby. [Deprecated]
Machine Learning Ruby [Deprecated]
jRuby Mahout - JRuby Mahout is a gem that unleashes the power of Apache Mahout in the world of JRuby. [Deprecated]
CardMagic-Classifier - A general classifier module to allow Bayesian and other types of classifications.
rb-libsvm - Ruby language bindings for LIBSVM which is a Library for Support Vector Machines.
Scoruby - Creates Random Forest classifiers from PMML files.


Data Analysis / Data Visualization

rsruby - Ruby - R bridge.
data-visualization-ruby - Source code and supporting content for my Ruby Manor presentation on Data Visualisation with Ruby. [Deprecated]
ruby-plot - gnuplot wrapper for Ruby, especially for plotting ROC curves into SVG files. [Deprecated]
plot-rb - A plotting library in Ruby built on top of Vega and D3. [Deprecated]
scruffy - A beautiful graphing toolkit for Ruby.
SciRuby
Glean - A data management tool for humans. [Deprecated]
Bioruby
Arel [Deprecated]


Misc

Big Data For Chimps
Listof - Community based data collection, packed in gem. Get list of pretty much anything (stop words, countries, non words) in txt, json or hash. Demo/Search for a list


Rust

General-Purpose Machine Learning

deeplearn-rs - deeplearn-rs provides simple networks that use matrix multiplication, addition, and ReLU under the MIT license.
rustlearn - a machine learning framework featuring logistic regression, support vector machines, decision trees and random forests.
rusty-machine - a pure-rust machine learning library.
leaf - open source framework for machine intelligence, sharing concepts from TensorFlow and Caffe. Available under the MIT license. [Deprecated]
RustNN - RustNN is a feedforward neural network library. [Deprecated]
RusticSOM - A Rust library for Self Organising Maps (SOM).


R

General-Purpose Machine Learning

ahaz - ahaz: Regularization for semiparametric additive hazards regression. [Deprecated]
arules - arules: Mining Association Rules and Frequent Itemsets
biglasso - biglasso: Extending Lasso Model Fitting to Big Data in R.
bmrm - bmrm: Bundle Methods for Regularized Risk Minimization Package.
Boruta - Boruta: A wrapper algorithm for all-relevant feature selection.
bst - bst: Gradient Boosting.
C50 - C50: C5.0 Decision Trees and Rule-Based Models.
caret - Classification and Regression Training: Unified interface to ~150 ML algorithms in R.
caretEnsemble - caretEnsemble: Framework for fitting multiple caret models as well as creating ensembles of such models. [Deprecated]
CatBoost - General purpose gradient boosting on decision trees library with categorical features support out of the box for R.
Clever Algorithms For Machine Learning
CORElearn - CORElearn: Classification, regression, feature evaluation and ordinal evaluation.
CoxBoost - CoxBoost: Cox models by likelihood based boosting for a single survival endpoint or competing risks [Deprecated]
Cubist - Cubist: Rule- and Instance-Based Regression Modeling.
e1071 - e1071: Misc Functions of the Department of Statistics (e1071), TU Wien
earth - earth: Multivariate Adaptive Regression Spline Models
elasticnet - elasticnet: Elastic-Net for Sparse Estimation and Sparse PCA.
ElemStatLearn - ElemStatLearn: Data sets, functions and examples from the book: ""The Elements of Statistical Learning, Data Mining, Inference, and Prediction"" by Trevor Hastie, Robert Tibshirani and Jerome Friedman Prediction"" by Trevor Hastie, Robert Tibshirani and Jerome Friedman.
evtree - evtree: Evolutionary Learning of Globally Optimal Trees.
forecast - forecast: Timeseries forecasting using ARIMA, ETS, STLM, TBATS, and neural network models.
forecastHybrid - forecastHybrid: Automatic ensemble and cross validation of ARIMA, ETS, STLM, TBATS, and neural network models from the ""forecast"" package.
fpc - fpc: Flexible procedures for clustering.
frbs - frbs: Fuzzy Rule-based Systems for Classification and Regression Tasks. [Deprecated]
GAMBoost - GAMBoost: Generalized linear and additive models by likelihood based boosting. [Deprecated]
gamboostLSS - gamboostLSS: Boosting Methods for GAMLSS.
gbm - gbm: Generalized Boosted Regression Models.
glmnet - glmnet: Lasso and elastic-net regularized generalized linear models.
glmpath - glmpath: L1 Regularization Path for Generalized Linear Models and Cox Proportional Hazards Model.
GMMBoost - GMMBoost: Likelihood-based Boosting for Generalized mixed models. [Deprecated]
grplasso - grplasso: Fitting user specified models with Group Lasso penalty.
grpreg - grpreg: Regularization paths for regression models with grouped covariates.
h2o - A framework for fast, parallel, and distributed machine learning algorithms at scale -- Deeplearning, Random forests, GBM, KMeans, PCA, GLM.
hda - hda: Heteroscedastic Discriminant Analysis. [Deprecated]
Introduction to Statistical Learning
ipred - ipred: Improved Predictors.
kernlab - kernlab: Kernel-based Machine Learning Lab.
klaR - klaR: Classification and visualization.
L0Learn - L0Learn: Fast algorithms for best subset selection.
lars - lars: Least Angle Regression, Lasso and Forward Stagewise. [Deprecated]
lasso2 - lasso2: L1 constrained estimation aka ‘lasso’.
LiblineaR - LiblineaR: Linear Predictive Models Based On The Liblinear C/C++ Library.
LogicReg - LogicReg: Logic Regression.
Machine Learning For Hackers
maptree - maptree: Mapping, pruning, and graphing tree models. [Deprecated]
mboost - mboost: Model-Based Boosting.
medley - medley: Blending regression models, using a greedy stepwise approach.
mlr - mlr: Machine Learning in R.
ncvreg - ncvreg: Regularization paths for SCAD- and MCP-penalized regression models.
nnet - nnet: Feed-forward Neural Networks and Multinomial Log-Linear Models. [Deprecated]
pamr - pamr: Pam: prediction analysis for microarrays. [Deprecated]
party - party: A Laboratory for Recursive Partytioning.
partykit - partykit: A Toolkit for Recursive Partytioning.
penalized - penalized: L1 (lasso and fused lasso) and L2 (ridge) penalized estimation in GLMs and in the Cox model.
penalizedLDA - penalizedLDA: Penalized classification using Fisher's linear discriminant. [Deprecated]
penalizedSVM - penalizedSVM: Feature Selection SVM using penalty functions.
quantregForest - quantregForest: Quantile Regression Forests.
randomForest - randomForest: Breiman and Cutler's random forests for classification and regression.
randomForestSRC - randomForestSRC: Random Forests for Survival, Regression and Classification (RF-SRC).
rattle - rattle: Graphical user interface for data mining in R.
rda - rda: Shrunken Centroids Regularized Discriminant Analysis.
rdetools - rdetools: Relevant Dimension Estimation (RDE) in Feature Spaces. [Deprecated]
REEMtree - REEMtree: Regression Trees with Random Effects for Longitudinal (Panel) Data. [Deprecated]
relaxo - relaxo: Relaxed Lasso. [Deprecated]
rgenoud - rgenoud: R version of GENetic Optimization Using Derivatives
Rmalschains - Rmalschains: Continuous Optimization using Memetic Algorithms with Local Search Chains (MA-LS-Chains) in R.
rminer - rminer: Simpler use of data mining methods (e.g. NN and SVM) in classification and regression. [Deprecated]
ROCR - ROCR: Visualizing the performance of scoring classifiers. [Deprecated]
RoughSets - RoughSets: Data Analysis Using Rough Set and Fuzzy Rough Set Theories. [Deprecated]
rpart - rpart: Recursive Partitioning and Regression Trees.
RPMM - RPMM: Recursively Partitioned Mixture Model.
RSNNS - RSNNS: Neural Networks in R using the Stuttgart Neural Network Simulator (SNNS).
RWeka - RWeka: R/Weka interface.
RXshrink - RXshrink: Maximum Likelihood Shrinkage via Generalized Ridge or Least Angle Regression.
sda - sda: Shrinkage Discriminant Analysis and CAT Score Variable Selection. [Deprecated]
spectralGraphTopology - spectralGraphTopology: Learning Graphs from Data via Spectral Constraints.
SuperLearner - Multi-algorithm ensemble learning packages.
svmpath - svmpath: svmpath: the SVM Path algorithm. [Deprecated]
tgp - tgp: Bayesian treed Gaussian process models. [Deprecated]
tree - tree: Classification and regression trees.
varSelRF - varSelRF: Variable selection using random forests.
XGBoost.R - R binding for eXtreme Gradient Boosting (Tree) Library.
Optunity - A library dedicated to automated hyperparameter optimization with a simple, lightweight API to facilitate drop-in replacement of grid search. Optunity is written in Python but interfaces seamlessly to R.
igraph - binding to igraph library - General purpose graph library.
MXNet - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.
TDSP-Utilities - Two data science utilities in R from Microsoft: 1) Interactive Data Exploration, Analysis, and Reporting (IDEAR) ; 2) Automated Modeling and Reporting (AMR).


Data Analysis / Data Visualization

ggplot2 - A data visualization package based on the grammar of graphics.
tmap for visualizing geospatial data with static maps and leaflet for interactive maps
tm and quanteda are the main packages for managing,  analyzing, and visualizing textual data.
shiny is the basis for truly interactive displays and dashboards in R. However, some measure of interactivity can be achieved with htmlwidgets bringing javascript libraries to R. These include, plotly, dygraphs, highcharter, and several others.


SAS

General-Purpose Machine Learning

Visual Data Mining and Machine Learning - Interactive, automated, and programmatic modeling with the latest machine learning algorithms in and end-to-end analytics environment, from data prep to deployment. Free trial available.
Enterprise Miner - Data mining and machine learning that creates deployable models using a GUI or code.
Factory Miner - Automatically creates deployable machine learning models across numerous market or customer segments using a GUI.


Data Analysis / Data Visualization

SAS/STAT - For conducting advanced statistical analysis.
University Edition - FREE! Includes all SAS packages necessary for data analysis and visualization, and includes online SAS courses.


Natural Language Processing

Contextual Analysis - Add structure to unstructured text using a GUI.
Sentiment Analysis - Extract sentiment from text using a GUI.
Text Miner - Text mining using a GUI or code.


Demos and Scripts

ML_Tables - Concise cheat sheets containing machine learning best practices.
enlighten-apply - Example code and materials that illustrate applications of SAS machine learning techniques.
enlighten-integration - Example code and materials that illustrate techniques for integrating SAS with other analytics technologies in Java, PMML, Python and R.
enlighten-deep - Example code and materials that illustrate using neural networks with several hidden layers in SAS.
dm-flow - Library of SAS Enterprise Miner process flow diagrams to help you learn by example about specific data mining topics.


Scala

Natural Language Processing

ScalaNLP - ScalaNLP is a suite of machine learning and numerical computing libraries.
Breeze - Breeze is a numerical processing library for Scala.
Chalk - Chalk is a natural language processing library. [Deprecated]
FACTORIE - FACTORIE is a toolkit for deployable probabilistic modeling, implemented as a software library in Scala. It provides its users with a succinct language for creating relational factor graphs, estimating parameters and performing inference.
Montague - Montague is a semantic parsing library for Scala with an easy-to-use DSL.
Spark NLP - Natural language processing library built on top of Apache Spark ML to provide simple, performant, and accurate NLP annotations for machine learning pipelines, that scale easily in a distributed environment.


Data Analysis / Data Visualization

MLlib in Apache Spark - Distributed machine learning library in Spark
Hydrosphere Mist - a service for deployment Apache Spark MLLib machine learning models as realtime, batch or reactive web services.
Scalding - A Scala API for Cascading.
Summing Bird - Streaming MapReduce with Scalding and Storm.
Algebird - Abstract Algebra for Scala.
xerial - Data management utilities for Scala. [Deprecated]
PredictionIO - PredictionIO, a machine learning server for software developers and data engineers.
BIDMat - CPU and GPU-accelerated matrix library intended to support large-scale exploratory data analysis.
Flink - Open source platform for distributed stream and batch data processing.
Spark Notebook - Interactive and Reactive Data Science using Scala and Spark.


General-Purpose Machine Learning

DeepLearning.scala - Creating statically typed dynamic neural networks from object-oriented & functional programming constructs.
Conjecture - Scalable Machine Learning in Scalding.
brushfire - Distributed decision tree ensemble learning in Scala.
ganitha - Scalding powered machine learning. [Deprecated]
adam - A genomics processing engine and specialized file format built using Apache Avro, Apache Spark and Parquet. Apache 2 licensed.
bioscala - Bioinformatics for the Scala programming language
BIDMach - CPU and GPU-accelerated Machine Learning Library.
Figaro - a Scala library for constructing probabilistic models.
H2O Sparkling Water - H2O and Spark interoperability.
FlinkML in Apache Flink - Distributed machine learning library in Flink.
DynaML - Scala Library/REPL for Machine Learning Research.
Saul - Flexible Declarative Learning-Based Programming.
SwiftLearner - Simply written algorithms to help study ML or write your own implementations.
Smile - Statistical Machine Intelligence and Learning Engine.
doddle-model - An in-memory machine learning library built on top of Breeze. It provides immutable objects and exposes its functionality through a scikit-learn-like API.
TensorFlow Scala -   Strongly-typed Scala API for TensorFlow.


Scheme

Neural Networks

layer - Neural network inference from the command line, implemented in CHICKEN Scheme.


Swift

General-Purpose Machine Learning

Bender - Fast Neural Networks framework built on top of Metal. Supports TensorFlow models.
Swift AI - Highly optimized artificial intelligence and machine learning library written in Swift.
Swift for Tensorflow - a next-generation platform for machine learning, incorporating the latest research across machine learning, compilers, differentiable programming, systems design, and beyond.
BrainCore - The iOS and OS X neural network framework.
swix - A bare bones library that includes a general matrix language and wraps some OpenCV for iOS development. [Deprecated]
AIToolbox - A toolbox framework of AI modules written in Swift: Graphs/Trees, Linear Regression, Support Vector Machines, Neural Networks, PCA, KMeans, Genetic Algorithms, MDP, Mixture of Gaussians.
MLKit - A simple Machine Learning Framework written in Swift. Currently features Simple Linear Regression, Polynomial Regression, and Ridge Regression.
Swift Brain - The first neural network / machine learning library written in Swift. This is a project for AI algorithms in Swift for iOS and OS X development. This project includes algorithms focused on Bayes theorem, neural networks, SVMs, Matrices, etc...
Perfect TensorFlow - Swift Language Bindings of TensorFlow. Using native TensorFlow models on both macOS / Linux.
PredictionBuilder - A library for machine learning that builds predictions using a linear regression.
Awesome CoreML - A curated list of pretrained CoreML models.
Awesome Core ML Models - A curated list of machine learning models in CoreML format.


TensorFlow

General-Purpose Machine Learning

Awesome TensorFlow - A list of all things related to TensorFlow.
Golden TensorFlow - A page of content on TensorFlow, including academic papers and links to related topics.


Tools

Neural Networks

layer - Neural network inference from the command line


Misc

ML Workspace - All-in-one web-based IDE for machine learning and data science. The workspace is deployed as a docker container and is preloaded with a variety of popular data science libraries (e.g., Tensorflow, PyTorch) and dev tools (e.g., Jupyter, VS Code).
Notebooks - A starter kit for Jupyter notebooks and machine learning. Companion docker images consist of all combinations of python versions, machine learning frameworks (Keras, PyTorch and Tensorflow) and CPU/CUDA versions.
DVC - Data Science Version Control is an open-source version control system for machine learning projects with pipelines support. It makes ML projects reproducible and shareable.
Kedro - Kedro is a data and development workflow framework that implements best practices for data pipelines with an eye towards productionizing machine learning models.
guild.ai - Tool to log, analyze, compare and ""optimize"" experiments. It's cross-platform and framework independent, and provided integrated visualizers such as tensorboard.
Sacred - Python tool to help  you configure, organize, log and reproduce experiments. Like a notebook lab in the context of Chemestry/Biology. The community has built multiple add-ons leveraging the proposed standard.
MLFlow - platform to manage the ML lifecycle, including experimentation, reproducibility and deployment. Framework anf language agnostic, take a look at all the built-in integrations.
More tools to improve the ML lifecycle: Catalyst, PachydermIO. The following are Github-alike and targetting teams Weights & Biases, Neptune.Ml, Comet.ml, Valohai.ai.


Credits

Some of the python libraries were cut-and-pasted from vinta
References for Go were mostly cut-and-pasted from gopherdata


"
5,"
GitHub Résumé
A service that creates a résumé based on your GitHub repos/activity.
GitHub Résumé is opt-in. To make your resume visible, just star this project. To view your résumé, go to https://resume.github.io/?yourusername or follow the instructions on the home page.
Great for all the tech-savy bosses who want to have a quick view of person's git/github activity, before the interview.
Development
To run the app in development mode:
$ rackup config.ru

(You must have Ruby and the rack gem installed.)

"
6,"





































Special thanks to the generous sponsorship by:























A Vue.js 2.0 UI Toolkit for Web.

Links

Homepage and documentation

International users
Chinese users
Spanish users
French users


awesome-element
FAQ
Customize theme
Preview and generate theme online
Element for React
Element for Angular
Atom helper
Visual Studio Code helper
Starter kit

element-starter
element-in-laravel-starter


Design resources
Gitter

International users
Chinese users



Install
npm install element-ui -S
Quick Start
import Vue from 'vue'
import Element from 'element-ui'

Vue.use(Element)

// or
import {
  Select,
  Button
  // ...
} from 'element-ui'

Vue.component(Select.name, Select)
Vue.component(Button.name, Button)
For more information, please refer to Quick Start in our documentation.
Browser Support
Modern browsers and Internet Explorer 10+.
Development
Skip this part if you just want to use Element.
For those who are interested in contributing to Element, please refer to our contributing guide (中文 | English | Español | Français) to see how to run this project.
Changelog
Detailed changes for each release are documented in the release notes.
FAQ
We have collected some frequently asked questions. Before reporting an issue, please search if the FAQ has the answer to your problem.
Contribution
Please make sure to read the contributing guide (中文 | English | Español | Français) before making a pull request.

Special Thanks
English documentation is brought to you by SwiftGG Translation Team:

raychenfj
kevin
曾小涛
湾仔王二
BlooDLine
陈铭嘉
千叶知风
梁杰
Changing
mmoaay

Spanish documentation is made possible by these community developers:

adavie1
carmencitaqiu
coderdiaz
fedegar33
Gonzalo2310
lesterbx
ProgramerGuy
SantiagoGdaR
sigfriedCub1990
thechosenjuan

French documentation is made possible by these community developers:

smalesys
blombard

Donation
If you find Element useful, you can buy us a cup of coffee

Backers
Support us with a monthly donation and help us continue our activities. [Become a backer]






























Sponsors
Become a sponsor and get your logo on our README on Github with a link to your site. [Become a sponsor]






























Join Discussion Group
Scan the QR code using Dingtalk App to join in discussion group :

LICENSE
MIT

"
7,"
lodash
Site |
Docs |
FP Guide |
Contributing |
Wiki |
Code of Conduct |
Twitter |
Chat
The Lodash library exported as a UMD module.
Generated using lodash-cli:
$ npm run build
$ lodash -o ./dist/lodash.js
$ lodash core -o ./dist/lodash.core.js
Download

Core build (~4 kB gzipped)
Full build (~24 kB gzipped)
CDN copies 

Lodash is released under the MIT license & supports modern environments.
Review the build differences & pick one that’s right for you.
Installation
In a browser:
<script src=""lodash.js""></script>
Using npm:
$ npm i -g npm
$ npm i lodash
Note: add --save if you are using npm < 5.0.0
In Node.js:
// Load the full build.
var _ = require('lodash');
// Load the core build.
var _ = require('lodash/core');
// Load the FP build for immutable auto-curried iteratee-first data-last methods.
var fp = require('lodash/fp');

// Load method categories.
var array = require('lodash/array');
var object = require('lodash/fp/object');

// Cherry-pick methods for smaller browserify/rollup/webpack bundles.
var at = require('lodash/at');
var curryN = require('lodash/fp/curryN');
Looking for Lodash modules written in ES6 or smaller bundle sizes? Check out lodash-es.
Why Lodash?
Lodash makes JavaScript easier by taking the hassle out of working with arrays,
numbers, objects, strings, etc. Lodash’s modular methods are great for:

Iterating arrays, objects, & strings
Manipulating & testing values
Creating composite functions

Module Formats
Lodash is available in a variety of builds & module formats.

lodash & per method packages
lodash-es, babel-plugin-lodash, & lodash-webpack-plugin
lodash/fp
lodash-amd


"
8,"
 Markdown Here
Visit the website.
Get it for Chrome.
Get it for Firefox.
Get it for Safari.
Get it for Thunderbird and Postbox.
Get it for Opera.
Discuss it and ask questions in the Google Group.
Markdown Here is a Google Chrome, Firefox, Safari, Opera, and Thunderbird extension that lets you write email† in Markdown‡ and render them before sending. It also supports syntax highlighting (just specify the language in a fenced code block).
Writing email with code in it is pretty tedious. Writing Markdown with code in it is easy. I found myself writing email in Markdown in the Github in-browser editor, then copying the preview into email. This is a pretty absurd workflow, so I decided create a tool to write and render Markdown right in the email.
To discover what can be done with Markdown in Markdown Here, check out the Markdown Here Cheatsheet and the other wiki pages.
†: And Google Groups posts, and Blogger posts, and Evernote notes, and Wordpress posts! See more.
‡: And TeX mathematical formulae!

Table of Contents
Installation Instructions
Usage Instructions
Troubleshooting
Compatibility
Notes and Miscellaneous
Building the Extension Bundles
Next Steps, Credits, Feedback, License
Installation Instructions
Chrome
Chrome Web Store
Go to the Chrome Web Store page for Markdown Here and install normally.
After installing, make sure to reload your webmail or restart Chrome!
Manual/Development

Clone this repo.
In Chrome, open the Extensions settings. (Wrench button, Tools, Extensions.)
On the Extensions settings page, click the ""Developer Mode"" checkbox.
Click the now-visible ""Load unpacked extension…"" button. Navigate to the directory where you cloned the repo, then the src directory under that.
The Markdown Here extension should now be visible in your extensions list.
Reload your webmail page (and maybe application) before trying to convert an email.

Firefox and Thunderbird
Mozilla Add-ons site
Go to the Firefox Add-ons page for Markdown Here and install normally.
Or go to the ""Tools > Add-ons"" menu and then search for ""Markdown Here"".
After installing, make sure to restart Firefox/Thunderbird!
Note: It takes up to a month for Mozilla to approve changes to the Firefox/Thunderbird extension, so updates (features, fixes) will lag behind what is shown here. You can manually choose to install the newest version before it's reviewed from the list of versions: https://addons.mozilla.org/en-US/firefox/addon/markdown-here/versions/
Manual/Development

Clone this repo.
Follow the instructions in the MDN ""Setting up an extension development environment"" article.

Safari
Download the extension directly. When it has finished downloading, double click it to install.
Preferences
To get to the Markdown Here preferences, open the Safari preferences and then go to the ""Extensions"" tab. Then click the ""Click me to show Markdown Here options"" box.
Opera
Note that Markdown Here only works with Opera versions 16 and higher (i.e., the ones that are based on Chromium).
Go to the Opera Add-ons store page for Markdown Here and install normally.
After installing, make sure to reload your webmail or restart Chrome!
Usage Instructions
Install it, and then…


In Chrome/Safari/Opera, make sure you reload your web mail page before trying to use Markdown Here.


In Chrome/Firefox/Safari/Opera, log into your Gmail, Hotmail, or Yahoo account and start a new email. In Thunderbird, start a new message.


Make sure you're using the rich editor.

In Gmail, click the ""Rich formatting"" link, if it's visible.
In Thunderbird, make sure ""Compose messages in HTML format"" is enabled in your ""Account Settings"", ""Composition & Addressing"" pane.



Compose an email in Markdown. For example:
**Hello** `world`.

```javascript
alert('Hello syntax highlighting.');
```



Right-click in the compose box and choose the ""Markdown Toggle"" item from the context menu. Or click the button that appears in your address bar. Or use the hotkey (CTRL+ALT+M by default).


You should see your email rendered correctly from Markdown into rich HTML.


Send your awesome email to everyone you know. It will appear to them the same way it looks to you.


Revert to Markdown
After rendering your Markdown to pretty HTML, you can still get back to your original Markdown. Just right-click anywhere in the newly rendered Markdown and click ""Markdown Toggle"" -- your email compose body will change back to the Markdown you had written.
Note that any changes you make to the pretty HTML will be lost when you revert to Markdown.
In Gmail, you can also use the browser's Undo command (CTRL+Z / CMD+Z, or from the Edit menu). Be warned that you might also lose the last few characters you entered.
Replies
In Gmail, Thunderbird, and Google Groups, you can use ""Markdown Toggle"" normally: just write your reply (top, bottom, inline, wherever) and then convert. The original email that you're replying to will be left alone. (Technically: Existing blockquote blocks will be left intact.)
In Hotmail and Yahoo (which don't put the original in a blockquote), and optionally in Gmail, Thunderbird, and Google Groups, you can ensure that only the part of the reply that you wrote gets converted by selecting what you want to convert and then clicking ""Markdown Toggle"" -- see the next section.
Selection/Piecemeal Conversion
Sometimes you don't want to convert the entire email; sometimes your email isn't entirely Markdown. To convert only part of the email, select the text (with your mouse or keyboard), right-click on it, and click the ""Markdown Toggle"" menu item. Your selection is magically rendered into pretty HTML.
To revert back to Markdown, just put your cursor anywhere in the block of converted text, right click, and click the ""Markdown Toggle"" menu item again. Now it's magically back to the original Markdown.

Things to know about converting/reverting a selection


If you select only part of a block of text, only that text will be converted. The converted block will be wrapped in a paragraph element, so the original line will be broken up. You probably don't want to ever do this.


You can select and revert multiple converted blocks at the same time. One upshot of this is that you can select your entire email, click ""Markdown Toggle"", and all portions of it that you had converted will be reverted.


If you don't have anything selected when you click ""Markdown Toggle"", Markdown Here will check if there are converted blocks anywhere in the message and revert them. If there no converted blocks are found, it will convert the entire email.


Options
The Markdown Here Options page can be accessed via the Chrome, Firefox, Safari, or Thunderbird extensions list. The available options include:

Styling modifications for the rendered Markdown.
Syntax highlighting theme selection and modification.
TeX math formulae processing enabling and customization.
What the hotkey should be.

For Chrome and Firefox, any changes made in the Markdown Here Options are automatically synchronized between your other installations of that browser (if you have the sync feature enabled in the browser).

Troubleshooting
See the Troubleshooting wiki page.
Compatibility
See the Compatibility wiki page.
Notes and Miscellaneous


Markdown Here uses Github Flavored Markdown, with the limitation that GFM special links are not supported (issue #11); nor will they be, as MDH is not Github-specific.


Available languages for syntax highlighting (and the way they should be written in the fenced code block) can be seen on the highlight.js demo page.


Images embedded inline in your Markdown will be retained when you ""Markdown Toggle"". Gmail allows you to put images inline in your email -- this can be much easier than referencing an external image.


Email signatures are automatically excluded from conversion. Specifically, anything after the semi-standard '-- ' (note the trailing space) is left alone.

Note that Hotmail and Yahoo do not automatically add the '-- ' to signatures, so you have to add it yourself.



The ""Markdown Toggle"" menu item shows up for more element types than it can correctly render. This is intended to help people realize that they're not using a rich editor. Otherwise they just don't see the menu item and don't know why.


Styling:

The use of browser-specific styles (-moz-, -webkit-) should be avoided. If used, they may not render correctly for people reading the email in a different browser from the one where the email was sent.
The use of state-dependent styles (like a:hover) don't work because they don't match at the time the styles are made explicit. (In email, styles must be explicitly applied to all elements -- stylesheets get stripped.)



For more tweaky features, visit the Tips and Tricks section.


Building the Extension Bundles
cd utils
node build.js

Chrome and Opera extension
Create a file with a .zip extension containing these files and directories:
manifest.json
common/
chrome/

Firefox/Thunderbird extension
Create a file with a .xpi extension containing these files and directories:
chrome.manifest
install.rdf
common/
firefox/

Safari extension
The browser-specific code is located in the markdown-here-safari project.
Use the Safari Extension Builder.
Next Steps
See the issues list and the Notes Wiki. All ideas, bugs, plans, complaints, and dreams will end up in one of those two places.
Feel free to create a feature request issue if what you want isn't already there. If you'd prefer a less formal approach to floating an idea, post to the ""markdown-here"" Google Group.
It also takes a fair bit of work to stay up-to-date with the latest changes in all the applications and web sites where Markdown Here works.
Credits
Markdown Here was coded on the shoulders of giants.

Markdown-to-HTML: chjj / marked
Syntax highlighting: isagalaev / highlight.js
HTML-to-text: mtrimpe / jsHtmlToText

Feedback
All bugs, feature requests, pull requests, feedback, etc., are welcome. Create an issue. Or post to the ""markdown-here"" Google Group.
License
Code
MIT License: http://adampritchard.mit-license.org/ or see the LICENSE file.
Logo
Copyright 2015, Austin Anderson. Licensed to Markdown Here under the MDH contributor license agreement.
Other images
Creative Commons Attribution 3.0 Unported (CC BY 3.0) License



"
9,"
Fira Code: monospaced font with programming ligatures

Problem
Programmers use a lot of symbols, often encoded with several characters. For the human brain, sequences like ->, <= or := are single logical tokens, even if they take two or three characters on the screen. Your eye spends a non-zero amount of energy to scan, parse and join multiple characters into a single logical one. Ideally, all programming languages should be designed with full-fledged Unicode symbols for operators, but that’s not the case yet.
Solution
Download v.2 · How to install · Troubleshooting · News & updates

Fira Code is an extension of the Fira Mono font containing a set of ligatures for common programming multi-character combinations. This is just a font rendering feature: underlying code remains ASCII-compatible. This helps to read and understand code faster. For some frequent sequences like .. or //, ligatures allow us to correct spacing.

Code examples
Ruby:

JavaScript:

PHP:

Erlang:

Elixir:

Go:

LiveScript:

Clojure:

Swift:

Stylistic sets
See How to enable

Terminal support



Works
Doesn’t work




Butterfly
Alacritty


crosh (ChromeOS, instructions)
Windows Console (conhost.exe)


Hyper.app
Cmder


iTerm 2 (3.1+)
ConEmu


Kitty
GNOME Terminal


Konsole
mate-terminal


mintty (partial support 2.8.3+)
PuTTY


QTerminal
rxvt


Terminal.app
xterm


Termux
ZOC (Windows)


Token2Shell/MD
gtkterm, guake, LXTerminal, sakura, Terminator, xfce4-terminal, and other libvte-based terminals (bug report)


upterm



Windows Terminal



ZOC (macOS)




Editor support



Works
Doesn’t work




Abricotine
Arduino IDE


Android Studio (2.3+, instructions)
Adobe Dreamweaver


Anjuta (unless at the EOF)
Delphi IDE


AppCode (2016.2+, instructions)
Eclipse (Windows, vote here)


Atom 1.1 or newer (instructions)
Standalone Emacs (workaround)


BBEdit/TextWrangler (v. 11 only, instructions)
Geany (Windows)


Brackets (with this plugin)
gVim (Windows workaround)


Chocolat
IDLE


CLion (2016.2+, instructions)
KDevelop 4


Cloud9 (instructions)
Monkey Studio IDE


Coda 2



CodeLite



Eclipse (macOS 4.7+, Linux)



elementary Code



Geany (macOS)



gEdit / Pluma



GNOME Builder



GoormIDE (instructions)



IntelliJ IDEA (2016.2+, instructions)



Kate, KWrite



KDevelop 5+



Komodo



Leafpad



LibreOffice



LightTable (instructions)



LINQPad



MacVim 7.4 or newer (instructions)



Mancy



Meld



Mousepad



NeoVim-gtk



NetBeans



Notepad (Windows)



Notepad++ (with a workaround)



Notepad3 (instructions)



PhpStorm (2016.2+, instructions)



PyCharm (2016.2+, instructions)



QtCreator



Rider



RStudio (instructions)



RubyMine (2016.2+, instructions)



Scratch



Scribus (1.5.3+)



SublimeText (3146+)



Spyder IDE (only with Qt5)



SuperCollider 3



TextAdept (Linux, macOS)



TextEdit



TextMate 2



VimR (instructions)



Visual Studio (2015+, instructions)



Visual Studio Code (instructions)



WebStorm (2016.2+, instructions)



Xamarin Studio/Monodevelop



Xcode (8.0+, otherwise with plugin)



Probably work: Smultron, Vico
Under question: Code::Blocks IDE



Browser support
<!-- HTML -->
<link rel=""stylesheet"" href=""https://cdn.jsdelivr.net/gh/tonsky/FiraCode@1.207/distr/fira_code.css"">
/* CSS */
@import url(https://cdn.jsdelivr.net/gh/tonsky/FiraCode@1.207/distr/fira_code.css);
/* Specify in CSS */
code { font-family: 'Fira Code', monospace; }

@supports (font-variation-settings: normal) {
  code { font-family: 'Fira Code VF', monospace; }
}

IE 10+, Edge: enable with font-feature-settings: ""calt"";
Firefox
Safari
Chromium-based browsers (Chrome, Opera)
ACE
CodeMirror (enable with font-variant-ligatures: contextual;)

Projects using Fira Code

All JetBrains products
CodePen
Blink Shell
Klipse
IlyaBirman.net
EvilMartians.com
Web Maker
FromScratch
PEP20.org

Alternatives
Other monospaced fonts with ligatures:

Hasklig (free)
PragmataPro (€59)
Monoid (free)
Fixedsys Excelsior (free)
Iosevka (free)
DejaVu Sans Code (free)
Victor Mono (free)
Cascadia Code (free)

Building Fira Code locally
In case you want to alter FiraCode.glyphs and build OTF/TTF/WOFF files yourself, this is setup I use on macOS:
# install all required build tools
./script/bootstrap

# build the font files
./script/build

# install OTFs to ~/Library/Fonts
./script/install
Credits

Author: Nikita Prokopov @nikitonsky
Based on: Fira Mono
Inspired by: Hasklig


"
10,"
Bitcoin Core integration/staging tree
https://bitcoincore.org
What is Bitcoin?
Bitcoin is an experimental digital currency that enables instant payments to
anyone, anywhere in the world. Bitcoin uses peer-to-peer technology to operate
with no central authority: managing transactions and issuing money are carried
out collectively by the network. Bitcoin Core is the name of open source
software which enables the use of this currency.
For more information, as well as an immediately usable, binary version of
the Bitcoin Core software, see https://bitcoincore.org/en/download/, or read the
original whitepaper.
License
Bitcoin Core is released under the terms of the MIT license. See COPYING for more
information or see https://opensource.org/licenses/MIT.
Development Process
The master branch is regularly built and tested, but is not guaranteed to be
completely stable. Tags are created
regularly to indicate new official, stable release versions of Bitcoin Core.
The contribution workflow is described in CONTRIBUTING.md
and useful hints for developers can be found in doc/developer-notes.md.
Testing
Testing and code review is the bottleneck for development; we get more pull
requests than we can review and test on short notice. Please be patient and help out by testing
other people's pull requests, and remember this is a security-critical project where any mistake might cost people
lots of money.
Automated Testing
Developers are strongly encouraged to write unit tests for new code, and to
submit new unit tests for old code. Unit tests can be compiled and run
(assuming they weren't disabled in configure) with: make check. Further details on running
and extending unit tests can be found in /src/test/README.md.
There are also regression and integration tests, written
in Python, that are run automatically on the build server.
These tests can be run (if the test dependencies are installed) with: test/functional/test_runner.py
The Travis CI system makes sure that every pull request is built for Windows, Linux, and macOS, and that unit/sanity tests are run automatically.
Manual Quality Assurance (QA) Testing
Changes should be tested by somebody other than the developer who wrote the
code. This is especially important for large or high-risk changes. It is useful
to add a test plan to the pull request description if testing the changes is
not straightforward.
Translations
Changes to translations as well as new translations can be submitted to
Bitcoin Core's Transifex page.
Translations are periodically pulled from Transifex and merged into the git repository. See the
translation process for details on how this works.
Important: We do not accept translation changes as GitHub pull requests because the next
pull from Transifex would automatically overwrite them again.
Translators should also subscribe to the mailing list.

"
11,"




Meteor is an ultra-simple environment for building modern web
applications.
With Meteor you write apps:

in modern JavaScript
that send data over the wire, rather than HTML
using your choice of popular open-source libraries

Try a getting started tutorial:

React
Blaze
Angular

Next, read the guide and the documentation.
Quick Start
On Windows, the installer can be found at https://www.meteor.com/install.
On Linux/macOS, use this line:
curl https://install.meteor.com/ | sh
Create a project:
meteor create try-meteor
Run it:
cd try-meteor
meteor
Developer Resources
Building an application with Meteor?

Announcement list: sign up at https://www.meteor.com/
Having problems? Ask for help at: https://stackoverflow.com/questions/tagged/meteor
Discussion forums: https://forums.meteor.com/

Interested in helping or contributing to Meteor?  These resources will help:

Core development guide
Contribution guidelines
Feature requests
Issue tracker

We are hiring!  Visit meteor.io/jobs to
learn more about working full-time on the Meteor project.
Uninstalling Meteor
Aside from a short launcher shell script, Meteor installs itself inside your
home directory. To uninstall Meteor, run:
rm -rf ~/.meteor/
sudo rm /usr/local/bin/meteor
On Windows, just run the uninstaller from your Control Panel.

"
12,"
RxJava: Reactive Extensions for the JVM



RxJava is a Java VM implementation of Reactive Extensions: a library for composing asynchronous and event-based programs by using observable sequences.
It extends the observer pattern to support sequences of data/events and adds operators that allow you to compose sequences together declaratively while abstracting away concerns about things like low-level threading, synchronization, thread-safety and concurrent data structures.
Version 3.x (Javadoc)

single dependency: Reactive-Streams
continued support for Java 6+ & Android 2.3+
fixed API mistakes and many limits of RxJava 2
intended to be a replacement for RxJava 2 with relatively few binary incompatible changes
Java 8 lambda-friendly API
non-opinionated about the source of concurrency (threads, pools, event loops, fibers, actors, etc.)
async or synchronous execution
virtual time and schedulers for parameterized concurrency
test and diagnostic support via test schedulers, test consumers and plugin hooks

Learn more about RxJava in general on the Wiki Home.
Version 2.x
The 2.x version will be supported with bugfixes and important documentation updates until
December 31, 2020. No new features will be added to 2.x.
Version 1.x
The 1.x version is end-of-life as of March 31, 2018. No further development, support, maintenance, PRs and updates will happen. The Javadoc of the very last version, 1.3.8, will remain accessible.
Getting started
Setting up the dependency
The first step is to include RxJava 3 into your project, for example, as a Gradle compile dependency:
implementation ""io.reactivex.rxjava3:rxjava:3.x.y""
(Please replace x and y with the latest version numbers: 
)
Hello World
The second is to write the Hello World program:
package rxjava.examples;

import io.reactivex.rxjava3.core.*;

public class HelloWorld {
    public static void main(String[] args) {
        Flowable.just(""Hello world"").subscribe(System.out::println);
    }
}
If your platform doesn't support Java 8 lambdas (yet), you have to create an inner class of Consumer manually:
import io.reactivex.rxjava3.functions.Consumer;

Flowable.just(""Hello world"")
  .subscribe(new Consumer<String>() {
      @Override public void accept(String s) {
          System.out.println(s);
      }
  });
Note that RxJava 3 components now live under io.reactivex.rxjava3 and the base classes and interfaces live under io.reactivex.rxjava3.core.
Base classes
RxJava 3 features several base classes you can discover operators on:

io.reactivex.rxjava3.core.Flowable: 0..N flows, supporting Reactive-Streams and backpressure
io.reactivex.rxjava3.core.Observable: 0..N flows, no backpressure,
io.reactivex.rxjava3.core.Single: a flow of exactly 1 item or an error,
io.reactivex.rxjava3.core.Completable: a flow without items but only a completion or error signal,
io.reactivex.rxjava3.core.Maybe: a flow with no items, exactly one item or an error.

Some terminology
Upstream, downstream
The dataflows in RxJava consist of a source, zero or more intermediate steps followed by a data consumer or combinator step (where the step is responsible to consume the dataflow by some means):
source.operator1().operator2().operator3().subscribe(consumer);

source.flatMap(value -> source.operator1().operator2().operator3());
Here, if we imagine ourselves on operator2, looking to the left towards the source is called the upstream. Looking to the right towards the subscriber/consumer is called the downstream. This is often more apparent when each element is written on a separate line:
source
  .operator1()
  .operator2()
  .operator3()
  .subscribe(consumer)
Objects in motion
In RxJava's documentation, emission, emits, item, event, signal, data and message are considered synonyms and represent the object traveling along the dataflow.
Backpressure
When the dataflow runs through asynchronous steps, each step may perform different things with different speed. To avoid overwhelming such steps, which usually would manifest itself as increased memory usage due to temporary buffering or the need for skipping/dropping data, so-called backpressure is applied, which is a form of flow control where the steps can express how many items are they ready to process. This allows constraining the memory usage of the dataflows in situations where there is generally no way for a step to know how many items the upstream will send to it.
In RxJava, the dedicated Flowable class is designated to support backpressure and Observable is dedicated to the non-backpressured operations (short sequences, GUI interactions, etc.). The other types, Single, Maybe and Completable don't support backpressure nor should they; there is always room to store one item temporarily.
Assembly time
The preparation of dataflows by applying various intermediate operators happens in the so-called assembly time:
Flowable<Integer> flow = Flowable.range(1, 5)
.map(v -> v * v)
.filter(v -> v % 3 == 0)
;
At this point, the data is not flowing yet and no side-effects are happening.
Subscription time
This is a temporary state when subscribe() is called on a flow that establishes the chain of processing steps internally:
flow.subscribe(System.out::println)
This is when the subscription side-effects are triggered (see doOnSubscribe). Some sources block or start emitting items right away in this state.
Runtime
This is the state when the flows are actively emitting items, errors or completion signals:
Observable.create(emitter -> {
     while (!emitter.isDisposed()) {
         long time = System.currentTimeMillis();
         emitter.onNext(time);
         if (time % 2 != 0) {
             emitter.onError(new IllegalStateException(""Odd millisecond!""));
             break;
         }
     }
})
.subscribe(System.out::println, Throwable::printStackTrace);
Practically, this is when the body of the given example above executes.
Simple background computation
One of the common use cases for RxJava is to run some computation, network request on a background thread and show the results (or error) on the UI thread:
import io.reactivex.rxjava3.schedulers.Schedulers;

Flowable.fromCallable(() -> {
    Thread.sleep(1000); //  imitate expensive computation
    return ""Done"";
})
  .subscribeOn(Schedulers.io())
  .observeOn(Schedulers.single())
  .subscribe(System.out::println, Throwable::printStackTrace);

Thread.sleep(2000); // <--- wait for the flow to finish
This style of chaining methods is called a fluent API which resembles the builder pattern. However, RxJava's reactive types are immutable; each of the method calls returns a new Flowable with added behavior. To illustrate, the example can be rewritten as follows:
Flowable<String> source = Flowable.fromCallable(() -> {
    Thread.sleep(1000); //  imitate expensive computation
    return ""Done"";
});

Flowable<String> runBackground = source.subscribeOn(Schedulers.io());

Flowable<String> showForeground = runBackground.observeOn(Schedulers.single());

showForeground.subscribe(System.out::println, Throwable::printStackTrace);

Thread.sleep(2000);
Typically, you can move computations or blocking IO to some other thread via subscribeOn. Once the data is ready, you can make sure they get processed on the foreground or GUI thread via observeOn.
Schedulers
RxJava operators don't work with Threads or ExecutorServices directly but with so-called Schedulers that abstract away sources of concurrency behind a uniform API. RxJava 3 features several standard schedulers accessible via Schedulers utility class.

Schedulers.computation(): Run computation intensive work on a fixed number of dedicated threads in the background. Most asynchronous operators use this as their default Scheduler.
Schedulers.io(): Run I/O-like or blocking operations on a dynamically changing set of threads.
Schedulers.single(): Run work on a single thread in a sequential and FIFO manner.
Schedulers.trampoline(): Run work in a sequential and FIFO manner in one of the participating threads, usually for testing purposes.

These are available on all JVM platforms but some specific platforms, such as Android, have their own typical Schedulers defined: AndroidSchedulers.mainThread(), SwingScheduler.instance() or JavaFXSchedulers.gui().
In addition, there is an option to wrap an existing Executor (and its subtypes such as ExecutorService) into a Scheduler via Schedulers.from(Executor). This can be used, for example, to have a larger but still fixed pool of threads (unlike computation() and io() respectively).
The Thread.sleep(2000); at the end is no accident. In RxJava the default Schedulers run on daemon threads, which means once the Java main thread exits, they all get stopped and background computations may never happen. Sleeping for some time in this example situations lets you see the output of the flow on the console with time to spare.
Concurrency within a flow
Flows in RxJava are sequential in nature split into processing stages that may run concurrently with each other:
Flowable.range(1, 10)
  .observeOn(Schedulers.computation())
  .map(v -> v * v)
  .blockingSubscribe(System.out::println);
This example flow squares the numbers from 1 to 10 on the computation Scheduler and consumes the results on the ""main"" thread (more precisely, the caller thread of blockingSubscribe). However, the lambda v -> v * v doesn't run in parallel for this flow; it receives the values 1 to 10 on the same computation thread one after the other.
Parallel processing
Processing the numbers 1 to 10 in parallel is a bit more involved:
Flowable.range(1, 10)
  .flatMap(v ->
      Flowable.just(v)
        .subscribeOn(Schedulers.computation())
        .map(w -> w * w)
  )
  .blockingSubscribe(System.out::println);
Practically, parallelism in RxJava means running independent flows and merging their results back into a single flow. The operator flatMap does this by first mapping each number from 1 to 10 into its own individual Flowable, runs them and merges the computed squares.
Note, however, that flatMap doesn't guarantee any order and the items from the inner flows may end up interleaved. There are alternative operators:

concatMap that maps and runs one inner flow at a time and
concatMapEager which runs all inner flows ""at once"" but the output flow will be in the order those inner flows were created.

Alternatively, the Flowable.parallel() operator and the ParallelFlowable type help achieve the same parallel processing pattern:
Flowable.range(1, 10)
  .parallel()
  .runOn(Schedulers.computation())
  .map(v -> v * v)
  .sequential()
  .blockingSubscribe(System.out::println);
Dependent sub-flows
flatMap is a powerful operator and helps in a lot of situations. For example, given a service that returns a Flowable, we'd like to call another service with values emitted by the first service:
Flowable<Inventory> inventorySource = warehouse.getInventoryAsync();

inventorySource
    .flatMap(inventoryItem -> erp.getDemandAsync(inventoryItem.getId())
            .map(demand -> ""Item "" + inventoryItem.getName() + "" has demand "" + demand))
    .subscribe(System.out::println);
Continuations
Sometimes, when an item has become available, one would like to perform some dependent computations on it. This is sometimes called continuations and, depending on what should happen and what types are involved, may involve various operators to accomplish.
Dependent
The most typical scenario is to given a value, invoke another service, await and continue with its result:
service.apiCall()
.flatMap(value -> service.anotherApiCall(value))
.flatMap(next -> service.finalCall(next))
It is often the case also that later sequences would require values from earlier mappings. This can be achieved by moving the outer flatMap into the inner parts of the previous flatMap for example:
service.apiCall()
.flatMap(value ->
    service.anotherApiCall(value)
    .flatMap(next -> service.finalCallBoth(value, next))
)
Here, the original value will be available inside the inner flatMap, courtesy of lambda variable capture.
Non-dependent
In other scenarios, the result(s) of the first source/dataflow is irrelevant and one would like to continue with a quasi independent another source. Here, flatMap works as well:
Observable continued = sourceObservable.flatMapSingle(ignored -> someSingleSource)
continued.map(v -> v.toString())
  .subscribe(System.out::println, Throwable::printStackTrace);
however, the continuation in this case stays Observable instead of the likely more appropriate Single. (This is understandable because
from the perspective of flatMapSingle, sourceObservable is a multi-valued source and thus the mapping may result in multiple values as well).
Often though there is a way that is somewhat more expressive (and also lower overhead) by using Completable as the mediator and its operator andThen to resume with something else:
sourceObservable
  .ignoreElements()           // returns Completable
  .andThen(someSingleSource)
  .map(v -> v.toString())
The only dependency between the sourceObservable and the someSingleSource is that the former should complete normally in order for the latter to be consumed.
Deferred-dependent
Sometimes, there is an implicit data dependency between the previous sequence and the new sequence that, for some reason, was not flowing through the ""regular channels"". One would be inclined to write such continuations as follows:
AtomicInteger count = new AtomicInteger();

Observable.range(1, 10)
  .doOnNext(ignored -> count.incrementAndGet())
  .ignoreElements()
  .andThen(Single.just(count.get()))
  .subscribe(System.out::println);
Unfortunately, this prints 0 because Single.just(count.get()) is evaluated at assembly time when the dataflow hasn't even run yet. We need something that defers the evaluation of this Single source until runtime when the main source completes:
AtomicInteger count = new AtomicInteger();

Observable.range(1, 10)
  .doOnNext(ignored -> count.incrementAndGet())
  .ignoreElements()
  .andThen(Single.defer(() -> Single.just(count.get())))
  .subscribe(System.out::println);
or
AtomicInteger count = new AtomicInteger();

Observable.range(1, 10)
  .doOnNext(ignored -> count.incrementAndGet())
  .ignoreElements()
  .andThen(Single.fromCallable(() -> count.get()))
  .subscribe(System.out::println);
Type conversions
Sometimes, a source or service returns a different type than the flow that is supposed to work with it. For example, in the inventory example above, getDemandAsync could return a Single<DemandRecord>. If the code example is left unchanged, this will result in a compile-time error (however, often with a misleading error message about lack of overload).
In such situations, there are usually two options to fix the transformation: 1) convert to the desired type or 2) find and use an overload of the specific operator supporting the different type.
Converting to the desired type
Each reactive base class features operators that can perform such conversions, including the protocol conversions, to match some other type. The following matrix shows the available conversion options:




Flowable
Observable
Single
Maybe
Completable




Flowable

toObservable
first, firstOrError, single, singleOrError, last, lastOrError1
firstElement, singleElement, lastElement
ignoreElements


Observable
toFlowable2

first, firstOrError, single, singleOrError, last, lastOrError1
firstElement, singleElement, lastElement
ignoreElements


Single
toFlowable3
toObservable

toMaybe
ignoreElement


Maybe
toFlowable3
toObservable
toSingle

ignoreElement


Completable
toFlowable
toObservable
toSingle
toMaybe




1: When turning a multi-valued source into a single-valued source, one should decide which of the many source values should be considered as the result.
2: Turning an Observable into Flowable requires an additional decision: what to do with the potential unconstrained flow
of the source Observable? There are several strategies available (such as buffering, dropping, keeping the latest) via the BackpressureStrategy parameter or via standard Flowable operators such as onBackpressureBuffer, onBackpressureDrop, onBackpressureLatest which also
allow further customization of the backpressure behavior.
3: When there is only (at most) one source item, there is no problem with backpressure as it can be always stored until the downstream is ready to consume.
Using an overload with the desired type
Many frequently used operator has overloads that can deal with the other types. These are usually named with the suffix of the target type:



Operator
Overloads




flatMap
flatMapSingle, flatMapMaybe, flatMapCompletable, flatMapIterable


concatMap
concatMapSingle, concatMapMaybe, concatMapCompletable, concatMapIterable


switchMap
switchMapSingle, switchMapMaybe, switchMapCompletable



The reason these operators have a suffix instead of simply having the same name with different signature is type erasure. Java doesn't consider signatures such as operator(Function<T, Single<R>>) and operator(Function<T, Maybe<R>>) different (unlike C#) and due to erasure, the two operators would end up as duplicate methods with the same signature.
Operator naming conventions
Naming in programming is one of the hardest things as names are expected to be not long, expressive, capturing and easily memorable. Unfortunately, the target language (and pre-existing conventions) may not give too much help in this regard (unusable keywords, type erasure, type ambiguities, etc.).
Unusable keywords
In the original Rx.NET, the operator that emits a single item and then completes is called Return(T). Since the Java convention is to have a lowercase letter start a method name, this would have been return(T) which is a keyword in Java and thus not available. Therefore, RxJava chose to name this operator just(T). The same limitation exists for the operator Switch, which had to be named switchOnNext. Yet another example is Catch which was named onErrorResumeNext.
Type erasure
Many operators that expect the user to provide some function returning a reactive type can't be overloaded because the type erasure around a Function<T, X> turns such method signatures into duplicates. RxJava chose to name such operators by appending the type as suffix as well:
Flowable<R> flatMap(Function<? super T, ? extends Publisher<? extends R>> mapper)

Flowable<R> flatMapMaybe(Function<? super T, ? extends MaybeSource<? extends R>> mapper)
Type ambiguities
Even though certain operators have no problems from type erasure, their signature may turn up being ambiguous, especially if one uses Java 8 and lambdas. For example, there are several overloads of concatWith taking the various other reactive base types as arguments (for providing convenience and performance benefits in the underlying implementation):
Flowable<T> concatWith(Publisher<? extends T> other);

Flowable<T> concatWith(SingleSource<? extends T> other);
Both Publisher and SingleSource appear as functional interfaces (types with one abstract method) and may encourage users to try to provide a lambda expression:
someSource.concatWith(s -> Single.just(2))
.subscribe(System.out::println, Throwable::printStackTrace);
Unfortunately, this approach doesn't work and the example does not print 2 at all. In fact, since version 2.1.10, it doesn't
even compile because at least 4 concatWith overloads exist and the compiler finds the code above ambiguous.
The user in such situations probably wanted to defer some computation until the someSource has completed, thus the correct
unambiguous operator should have been defer:
someSource.concatWith(Single.defer(() -> Single.just(2)))
.subscribe(System.out::println, Throwable::printStackTrace);
Sometimes, a suffix is added to avoid logical ambiguities that may compile but produce the wrong type in a flow:
Flowable<T> merge(Publisher<? extends Publisher<? extends T>> sources);

Flowable<T> mergeArray(Publisher<? extends T>... sources);
This can get also ambiguous when functional interface types get involved as the type argument T.
Error handling
Dataflows can fail, at which point the error is emitted to the consumer(s). Sometimes though, multiple sources may fail at which point there is a choice whether or not wait for all of them to complete or fail. To indicate this opportunity, many operator names are suffixed with the DelayError words (while others feature a delayError or delayErrors boolean flag in one of their overloads):
Flowable<T> concat(Publisher<? extends Publisher<? extends T>> sources);

Flowable<T> concatDelayError(Publisher<? extends Publisher<? extends T>> sources);
Of course, suffixes of various kinds may appear together:
Flowable<T> concatArrayEagerDelayError(Publisher<? extends T>... sources);
Base class vs base type
The base classes can be considered heavy due to the sheer number of static and instance methods on them. RxJava 3's design was heavily influenced by the Reactive Streams specification, therefore, the library features a class and an interface per each reactive type:



Type
Class
Interface
Consumer




0..N backpressured
Flowable
Publisher1
Subscriber


0..N unbounded
Observable
ObservableSource2
Observer


1 element or error
Single
SingleSource
SingleObserver


0..1 element or error
Maybe
MaybeSource
MaybeObserver


0 element or error
Completable
CompletableSource
CompletableObserver



1The org.reactivestreams.Publisher is part of the external Reactive Streams library. It is the main type to interact with other reactive libraries through a standardized mechanism governed by the Reactive Streams specification.
2The naming convention of the interface was to append Source to the semi-traditional class name. There is no FlowableSource since Publisher is provided by the Reactive Streams library (and subtyping it wouldn't have helped with interoperation either). These interfaces are, however, not standard in the sense of the Reactive Streams specification and are currently RxJava specific only.
R8 and ProGuard settings
By default, RxJava itself doesn't require any ProGuard/R8 settings and should work without problems. Unfortunately, the Reactive Streams dependency since version 1.0.3 has embedded Java 9 class files in its JAR that can cause warnings with the plain ProGuard:
Warning: org.reactivestreams.FlowAdapters$FlowPublisherFromReactive: can't find superclass or interface java.util.concurrent.Flow$Publisher
Warning: org.reactivestreams.FlowAdapters$FlowToReactiveProcessor: can't find superclass or interface java.util.concurrent.Flow$Processor
Warning: org.reactivestreams.FlowAdapters$FlowToReactiveSubscriber: can't find superclass or interface java.util.concurrent.Flow$Subscriber
Warning: org.reactivestreams.FlowAdapters$FlowToReactiveSubscription: can't find superclass or interface java.util.concurrent.Flow$Subscription
Warning: org.reactivestreams.FlowAdapters: can't find referenced class java.util.concurrent.Flow$Publisher

It is recommended one sets up the following -dontwarn entry in the application's proguard-ruleset file:
-dontwarn java.util.concurrent.Flow*

For R8, the RxJava jar includes the META-INF/proguard/rxjava3.pro with the same no-warning clause and should apply automatically.
Further reading
For further details, consult the wiki.
Communication

Google Group: RxJava
Twitter: @RxJava
GitHub Issues
StackOverflow: rx-java and rx-java2
Gitter.im

Versioning
Version 3.x is in development. Bugfixes will be applied to both 2.x and 3.x branches, but new features will only be added to 3.x.
Minor 3.x increments (such as 3.1, 3.2, etc) will occur when non-trivial new functionality is added or significant enhancements or bug fixes occur that may have behavioral changes that may affect some edge cases (such as dependence on behavior resulting from a bug). An example of an enhancement that would classify as this is adding reactive pull backpressure support to an operator that previously did not support it. This should be backwards compatible but does behave differently.
Patch 3.x.y increments (such as 3.0.0 -> 3.0.1, 3.3.1 -> 3.3.2, etc) will occur for bug fixes and trivial functionality (like adding a method overload). New functionality marked with an @Beta or @Experimental annotation can also be added in the patch releases to allow rapid exploration and iteration of unstable new functionality.
@Beta
APIs marked with the @Beta annotation at the class or method level are subject to change. They can be modified in any way, or even removed, at any time. If your code is a library itself (i.e. it is used on the CLASSPATH of users outside your control), you should not use beta APIs, unless you repackage them (e.g. using ProGuard, shading, etc).
@Experimental
APIs marked with the @Experimental annotation at the class or method level will almost certainly change. They can be modified in any way, or even removed, at any time. You should not use or rely on them in any production code. They are purely to allow broad testing and feedback.
@Deprecated
APIs marked with the @Deprecated annotation at the class or method level will remain supported until the next major release but it is recommended to stop using them.
io.reactivex.rxjava3.internal.*
All code inside the io.reactivex.rxjava3.internal.* packages are considered private API and should not be relied upon at all. It can change at any time.
Full Documentation

Wiki
Javadoc
Latest snaphot Javadoc
Javadoc of a specific release version: http://reactivex.io/RxJava/3.x/javadoc/3.x.y/

Binaries
Binaries and dependency information for Maven, Ivy, Gradle and others can be found at http://search.maven.org.
Example for Gradle:
compile 'io.reactivex.rxjava3:rxjava:x.y.z'
and for Maven:
<dependency>
    <groupId>io.reactivex.rxjava3</groupId>
    <artifactId>rxjava</artifactId>
    <version>x.y.z</version>
</dependency>
and for Ivy:
<dependency org=""io.reactivex.rxjava3"" name=""rxjava"" rev=""x.y.z"" />
Snapshots are available via https://oss.jfrog.org/libs-snapshot/io/reactivex/rxjava3/rxjava/
repositories {
    maven { url 'https://oss.jfrog.org/libs-snapshot' }
}

dependencies {
    compile 'io.reactivex.rxjava3:rxjava:3.0.0-SNAPSHOT'
}
Build
To build:
$ git clone git@github.com:ReactiveX/RxJava.git
$ cd RxJava/
$ ./gradlew build

Further details on building can be found on the Getting Started page of the wiki.
Bugs and Feedback
For bugs, questions and discussions please use the Github Issues.
LICENSE
Copyright (c) 2016-present, RxJava Contributors.

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


"
13,"
The Rust Programming Language
This is the main source code repository for Rust. It contains the compiler,
standard library, and documentation.
Quick Start
Read ""Installation"" from The Book.
Installing from Source
Note: If you wish to contribute to the compiler, you should read this
chapter of the rustc-guide instead of this section.
The Rust build system has a Python script called x.py to bootstrap building
the compiler. More information about it may be found by running ./x.py --help
or reading the rustc guide.
Building on *nix


Make sure you have installed the dependencies:

g++ 5.1 or later or clang++ 3.5 or later
python 2.7 (but not 3.x)
GNU make 3.81 or later
cmake 3.4.3 or later
curl
git
ssl which comes in libssl-dev or openssl-devel
pkg-config if you are compiling on Linux and targeting Linux



Clone the source with git:
$ git clone https://github.com/rust-lang/rust.git
$ cd rust




Configure the build settings:
The Rust build system uses a file named config.toml in the root of the
source tree to determine various configuration settings for the build.
Copy the default config.toml.example to config.toml to get started.
$ cp config.toml.example config.toml
It is recommended that if you plan to use the Rust build system to create
an installation (using ./x.py install) that you set the prefix value
in the [install] section to a directory that you have write permissions.
Create install directory if you are not installing in default directory


Build and install:
$ ./x.py build && ./x.py install
When complete, ./x.py install will place several programs into
$PREFIX/bin: rustc, the Rust compiler, and rustdoc, the
API-documentation tool. This install does not include Cargo,
Rust's package manager. To build and install Cargo, you may
run ./x.py install cargo or set the build.extended key in
config.toml to true to build and install all tools.


Building on Windows
There are two prominent ABIs in use on Windows: the native (MSVC) ABI used by
Visual Studio, and the GNU ABI used by the GCC toolchain. Which version of Rust
you need depends largely on what C/C++ libraries you want to interoperate with:
for interop with software produced by Visual Studio use the MSVC build of Rust;
for interop with GNU software built using the MinGW/MSYS2 toolchain use the GNU
build.
MinGW
MSYS2 can be used to easily build Rust on Windows:


Grab the latest MSYS2 installer and go through the installer.


Run mingw32_shell.bat or mingw64_shell.bat from wherever you installed
MSYS2 (i.e. C:\msys64), depending on whether you want 32-bit or 64-bit
Rust. (As of the latest version of MSYS2 you have to run msys2_shell.cmd -mingw32 or msys2_shell.cmd -mingw64 from the command line instead)


From this terminal, install the required tools:
# Update package mirrors (may be needed if you have a fresh install of MSYS2)
$ pacman -Sy pacman-mirrors

# Install build tools needed for Rust. If you're building a 32-bit compiler,
# then replace ""x86_64"" below with ""i686"". If you've already got git, python,
# or CMake installed and in PATH you can remove them from this list. Note
# that it is important that you do **not** use the 'python2' and 'cmake'
# packages from the 'msys2' subsystem. The build has historically been known
# to fail with these packages.
$ pacman -S git \
            make \
            diffutils \
            tar \
            mingw-w64-x86_64-python2 \
            mingw-w64-x86_64-cmake \
            mingw-w64-x86_64-gcc


Navigate to Rust's source code (or clone it), then build it:
$ ./x.py build && ./x.py install


MSVC
MSVC builds of Rust additionally require an installation of Visual Studio 2017
(or later) so rustc can use its linker.  The simplest way is to get the
Visual Studio, check the “C++ build tools” and “Windows 10 SDK” workload.
(If you're installing cmake yourself, be careful that “C++ CMake tools for
Windows” doesn't get included under “Individual components”.)
With these dependencies installed, you can build the compiler in a cmd.exe
shell with:
> python x.py build
Currently, building Rust only works with some known versions of Visual Studio. If
you have a more recent version installed the build system doesn't understand
then you may need to force rustbuild to use an older version. This can be done
by manually calling the appropriate vcvars file before running the bootstrap.
> CALL ""C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Auxiliary\Build\vcvars64.bat""
> python x.py build
Building rustc with older host toolchains
It is still possible to build Rust with the older toolchain versions listed below, but only if the
LLVM_TEMPORARILY_ALLOW_OLD_TOOLCHAIN option is set to true in the config.toml file.

Clang 3.1
Apple Clang 3.1
GCC 4.8
Visual Studio 2015 (Update 3)

Toolchain versions older than what is listed above cannot be used to build rustc.
Specifying an ABI
Each specific ABI can also be used from either environment (for example, using
the GNU ABI in PowerShell) by using an explicit build triple. The available
Windows build triples are:

GNU ABI (using GCC)

i686-pc-windows-gnu
x86_64-pc-windows-gnu


The MSVC ABI

i686-pc-windows-msvc
x86_64-pc-windows-msvc



The build triple can be specified by either specifying --build=<triple> when
invoking x.py commands, or by copying the config.toml file (as described
in Installing From Source), and modifying the
build option under the [build] section.
Configure and Make
While it's not the recommended build system, this project also provides a
configure script and makefile (the latter of which just invokes x.py).
$ ./configure
$ make && sudo make install
When using the configure script, the generated config.mk file may override the
config.toml file. To go back to the config.toml file, delete the generated
config.mk file.
Building Documentation
If you’d like to build the documentation, it’s almost the same:
$ ./x.py doc
The generated documentation will appear under doc in the build directory for
the ABI used. I.e., if the ABI was x86_64-pc-windows-msvc, the directory will be
build\x86_64-pc-windows-msvc\doc.
Notes
Since the Rust compiler is written in Rust, it must be built by a
precompiled ""snapshot"" version of itself (made in an earlier stage of
development). As such, source builds require a connection to the Internet, to
fetch snapshots, and an OS that can execute the available snapshot binaries.
Snapshot binaries are currently built and tested on several platforms:



Platform / Architecture
x86
x86_64




Windows (7, 8, 10, ...)
✓
✓


Linux (2.6.18 or later)
✓
✓


macOS (10.7 Lion or later)
✓
✓



You may find that other platforms work, but these are our officially
supported build environments that are most likely to work.
There is more advice about hacking on Rust in CONTRIBUTING.md.
Getting Help
The Rust community congregates in a few places:

Stack Overflow - Direct questions about using the language.
users.rust-lang.org - General discussion and broader questions.
/r/rust - News and general discussion.

Contributing
To contribute to Rust, please see CONTRIBUTING.
Most real-time collaboration happens in a variety of channels on the
Rust Discord server, with channels dedicated for getting help,
community, documentation, and all major contribution areas in the Rust ecosystem.
A good place to ask for help would be the #help channel.
The rustc guide might be a good place to start if you want to find out how
various parts of the compiler work.
Also, you may find the rustdocs for the compiler itself useful.
License
Rust is primarily distributed under the terms of both the MIT license
and the Apache License (Version 2.0), with portions covered by various
BSD-like licenses.
See LICENSE-APACHE, LICENSE-MIT, and
COPYRIGHT for details.
Trademark
The Rust programming language is an open source, community project governed
by a core team. It is also sponsored by the Mozilla Foundation (“Mozilla”),
which owns and protects the Rust and Cargo trademarks and logos
(the “Rust Trademarks”).
If you want to use these names or brands, please read the media guide.
Third-party logos may be subject to third-party copyrights and trademarks. See
Licenses for details.

"
14,"

    
    
    
    
     Python 3.7.4 (default, Sep  7 2019, 18:27:02)
     >>> import requests
     >>> r = requests.get('https://api.github.com/repos/psf/requests')
     >>> r.json()[""description""]
     'A simple, yet elegant HTTP library. Handcrafted, with ♥, for the Python community.'
    

    
This software has been designed for you, with much joy,
by Kenneth Reitz & is protected by The Python Software Foundation.
   


  
Requests is an elegant and simple HTTP library for Python, built with ♥.
 
>>> import requests
>>> r = requests.get('https://api.github.com/user', auth=('user', 'pass'))
>>> r.status_code
200
>>> r.headers['content-type']
'application/json; charset=utf8'
>>> r.encoding
'utf-8'
>>> r.text
u'{""type"":""User""...'
>>> r.json()
{u'disk_usage': 368627, u'private_gists': 484, ...}

 
Requests allows you to send HTTP/1.1 requests extremely easily. There’s no need to manually add query strings to your URLs, or to form-encode your PUT & POST data — but nowadays, just use the json method!
Requests is the most downloaded Python package today, pulling in around 14M downloads / week— according to GitHub, Requests is currently depended upon by 367_296 repositories. You may certainly put your trust in this code.
 



 
Supported Features & Best–Practices
Requests is ready for the demands of building robust and reliable HTTP–speak applications, for the needs of today.
         + International Domains and URLs       + Keep-Alive & Connection Pooling
         + Sessions with Cookie Persistence     + Browser-style SSL Verification
         + Basic & Digest Authentication        + Familiar `dict`–like Cookies
         + Automatic Decompression of Content   + Automatic Content Decoding
         + Automatic Connection Pooling         + Unicode Response Bodies*
         + Multi-part File Uploads              + SOCKS Proxy Support
         + Connection Timeouts                  + Streaming Downloads
         + Automatic honoring of `.netrc`       + Chunked HTTP Requests

                            &, of course, rock–solid stability!


✨ 🍰 ✨            

 
Requests Module Installation
The recommended way to intall the requests module is to simply use pipenv (or pip, of
course):
$ pipenv install requests
Adding requests to Pipfile's [packages]…
✔ Installation Succeeded
…
Requests officially supports Python 2.7 & 3.4–3.8.

P.S. — Documentation is Available at //requests.readthedocs.io.




 



 




"
15,"

 
 
  
 
 


Ansible
Ansible is a radically simple IT automation system. It handles
configuration management, application deployment, cloud provisioning,
ad-hoc task execution, network automation, and multi-node orchestration. Ansible makes complex
changes like zero-downtime rolling updates with load balancers easy. More information on the Ansible website.

Design Principles

Have a dead simple setup process and a minimal learning curve.
Manage machines very quickly and in parallel.
Avoid custom-agents and additional open ports, be agentless by
leveraging the existing SSH daemon.
Describe infrastructure in a language that is both machine and human
friendly.
Focus on security and easy auditability/review/rewriting of content.
Manage new remote machines instantly, without bootstrapping any
software.
Allow module development in any dynamic language, not just Python.
Be usable as non-root.
Be the easiest IT automation system to use, ever.


Use Ansible
You can install a released version of Ansible via pip, a package manager, or
our release repository. See our
installation guide for details on installing Ansible
on a variety of platforms.
Red Hat offers supported builds of Ansible Engine.
Power users and developers can run the devel branch, which has the latest
features and fixes, directly. Although it is reasonably stable, you are more likely to encounter
breaking changes when running the devel branch. We recommend getting involved
in the Ansible community if you want to run the devel branch.

Get Involved

Read Community
Information for all
kinds of ways to contribute to and interact with the project,
including mailing list information and how to submit bug reports and
code to Ansible.
Join a Working Group, an organized community devoted to a specific technology domain or platform.
Submit a proposed code update through a pull request to the devel branch.
Talk to us before making larger changes
to avoid duplicate efforts. This not only helps everyone
know what is going on, it also helps save time and effort if we decide
some changes are needed.
For a list of email lists, IRC channels and Working Groups, see the
Communication page


Branch Info

The devel branch corresponds to the release actively under development.
The stable-2.X branches correspond to stable releases.
Create a branch based on devel and set up a dev environment if you want to open a PR.
See the Ansible release and maintenance page for information about active branches.


Roadmap
Based on team and community feedback, an initial roadmap will be published for a major or minor version (ex: 2.7, 2.8).
The Ansible Roadmap page details what is planned and how to influence the roadmap.

Authors
Ansible was created by Michael DeHaan
and has contributions from over 4700 users (and growing). Thanks everyone!
Ansible is sponsored by Red Hat, Inc.

License
GNU General Public License v3.0 or later
See COPYING to see the full text.

"
16,"
Interviews

Your personal guide to Software Engineering technical interviews. Video
solutions to the following interview problems with detailed explanations can be found here.

Maintainer - Kevin Naughton Jr.

Translations

简体中文

Table of Contents

YouTube
Instagram
Articles
Online Judges
Live Coding Practice
Data Structures
Algorithms
Greedy Algorithms
Bitmasks
Runtime Analysis
Video Lectures
Interview Books
Computer Science News
Directory Tree

YouTube

Kevin Naughton Jr.

Instagram

Programeme

Articles

Starting Work

Online Judges

LeetCode
Virtual Judge
CareerCup
HackerRank
CodeFights
Kattis
HackerEarth
Codility
Code Forces
Code Chef
Sphere Online Judge - SPOJ
InterviewBit

Live Coding Practice

Pramp
Gainlo
Refdash
Interviewing.io

Data Structures
Linked List

A Linked List is a linear collection of data elements, called nodes, each
pointing to the next node by means of a pointer. It is a data structure
consisting of a group of nodes which together represent a sequence.
Singly-linked list: linked list in which each node points to the next node and the last node points to null
Doubly-linked list: linked list in which each node has two pointers, p and n, such that p points to the previous node and n points to the next node; the last node's n pointer points to null
Circular-linked list: linked list in which each node points to the next node and the last node points back to the first node
Time Complexity:

Access: O(n)
Search: O(n)
Insert: O(1)
Remove: O(1)



Stack

A Stack is a collection of elements, with two principle operations: push, which adds to the collection, and
pop, which removes the most recently added element
Last in, first out data structure (LIFO): the most recently added object is the first to be removed
Time Complexity:

Access: O(n)
Search: O(n)
Insert: O(1)
Remove: O(1)



Queue

A Queue is a collection of elements, supporting two principle operations: enqueue, which inserts an element
into the queue, and dequeue, which removes an element from the queue
First in, first out data structure (FIFO): the oldest added object is the first to be removed
Time Complexity:

Access: O(n)
Search: O(n)
Insert: O(1)
Remove: O(1)



Tree

A Tree is an undirected, connected, acyclic graph

Binary Tree

A Binary Tree is a tree data structure in which each node has at most two children, which are referred to as
the left child and right child
Full Tree: a tree in which every node has either 0 or 2 children
Perfect Binary Tree: a binary tree in which all interior nodes have two children and all leave have the same depth
Complete Tree: a binary tree in which every level except possibly the last is full and all nodes in the last
level are as far left as possible

Binary Search Tree

A binary search tree, sometimes called BST, is a type of binary tree which maintains the property that the value in each
node must be greater than or equal to any value stored in the left sub-tree, and less than or equal to any value stored
in the right sub-tree
Time Complexity:

Access: O(log(n))
Search: O(log(n))
Insert: O(log(n))
Remove: O(log(n))




Trie

A trie, sometimes called a radix or prefix tree, is a kind of search tree that is used to store a dynamic set or associative
array where the keys are usually Strings. No node in the tree stores the key associated with that node; instead, its position
in the tree defines the key with which it is associated. All the descendants of a node have a common prefix of the String associated
with that node, and the root is associated with the empty String.


Fenwick Tree

A Fenwick tree, sometimes called a binary indexed tree, is a tree in concept, but in practice is implemented as an implicit data
structure using an array. Given an index in the array representing a vertex, the index of a vertex's parent or child is calculated
through bitwise operations on the binary representation of its index. Each element of the array contains the pre-calculated sum of
a range of values, and by combining that sum with additional ranges encountered during an upward traversal to the root, the prefix
sum is calculated
Time Complexity:

Range Sum: O(log(n))
Update: O(log(n))




Segment Tree

A Segment tree, is a tree data structure for storing intervals, or segments. It allows querying which of the stored segments contain
a given point
Time Complexity:

Range Query: O(log(n))
Update: O(log(n))




Heap

A Heap is a specialized tree based structure data structure that satisfies the heap property: if A is a parent node of
B, then the key (the value) of node A is ordered with respect to the key of node B with the same ordering applying across the entire heap.
A heap can be classified further as either a ""max heap"" or a ""min heap"". In a max heap, the keys of parent nodes are always greater
than or equal to those of the children and the highest key is in the root node. In a min heap, the keys of parent nodes are less than
or equal to those of the children and the lowest key is in the root node
Time Complexity:

Access Max / Min: O(1)
Insert: O(log(n))
Remove Max / Min: O(log(n))




Hashing

Hashing is used to map data of an arbitrary size to data of a fixed size. The values returned by a hash
function are called hash values, hash codes, or simply hashes. If two keys map to the same value, a collision occurs
Hash Map: a hash map is a structure that can map keys to values. A hash map uses a hash function to compute
an index into an array of buckets or slots, from which the desired value can be found.
Collision Resolution
Separate Chaining: in separate chaining, each bucket is independent, and contains a list of entries for each index. The
time for hash map operations is the time to find the bucket (constant time), plus the time to iterate through the list
Open Addressing: in open addressing, when a new entry is inserted, the buckets are examined, starting with the
hashed-to-slot and proceeding in some sequence, until an unoccupied slot is found. The name open addressing refers to
the fact that the location of an item is not always determined by its hash value


Graph

A Graph is an ordered pair of G = (V, E) comprising a set V of vertices or nodes together with a set E of edges or arcs,
which are 2-element subsets of V (i.e. an edge is associated with two vertices, and that association takes the form of the
unordered pair comprising those two vertices)
Undirected Graph: a graph in which the adjacency relation is symmetric. So if there exists an edge from node u to node
v (u -> v), then it is also the case that there exists an edge from node v to node u (v -> u)
Directed Graph: a graph in which the adjacency relation is not symmetric. So if there exists an edge from node u to node v
(u -> v), this does not imply that there exists an edge from node v to node u (v -> u)


Algorithms
Sorting
Quicksort

Stable: No
Time Complexity:

Best Case: O(nlog(n))
Worst Case: O(n^2)
Average Case: O(nlog(n))




Mergesort

Mergesort is also a divide and conquer algorithm. It continuously divides an array into two halves, recurses on both the
left subarray and right subarray and then merges the two sorted halves
Stable: Yes
Time Complexity:

Best Case: O(nlog(n))
Worst Case: O(nlog(n))
Average Case: O(nlog(n))




Bucket Sort

Bucket Sort is a sorting algorithm that works by distributing the elements of an array into a number of buckets. Each bucket
is then sorted individually, either using a different sorting algorithm, or by recursively applying the bucket sorting algorithm
Time Complexity:

Best Case: Ω(n + k)
Worst Case: O(n^2)
Average Case:Θ(n + k)




Radix Sort

Radix Sort is a sorting algorithm that like bucket sort, distributes elements of an array into a number of buckets. However, radix
sort differs from bucket sort by 're-bucketing' the array after the initial pass as opposed to sorting each bucket and merging
Time Complexity:

Best Case: Ω(nk)
Worst Case: O(nk)
Average Case: Θ(nk)



Graph Algorithms
Depth First Search

Depth First Search is a graph traversal algorithm which explores as far as possible along each branch before backtracking
Time Complexity: O(|V| + |E|)


Breadth First Search

Breadth First Search is a graph traversal algorithm which explores the neighbor nodes first, before moving to the next
level neighbors
Time Complexity: O(|V| + |E|)


Topological Sort

Topological Sort is the linear ordering of a directed graph's nodes such that for every edge from node u to node v, u
comes before v in the ordering
Time Complexity: O(|V| + |E|)

Dijkstra's Algorithm

Dijkstra's Algorithm is an algorithm for finding the shortest path between nodes in a graph
Time Complexity: O(|V|^2)


Bellman-Ford Algorithm

Bellman-Ford Algorithm is an algorithm that computes the shortest paths from a single source node to all other nodes in a weighted graph
Although it is slower than Dijkstra's, it is more versatile, as it is capable of handling graphs in which some of the edge weights are
negative numbers
Time Complexity:

Best Case: O(|E|)
Worst Case: O(|V||E|)




Floyd-Warshall Algorithm

Floyd-Warshall Algorithm is an algorithm for finding the shortest paths in a weighted graph with positive or negative edge weights, but
no negative cycles
A single execution of the algorithm will find the lengths (summed weights) of the shortest paths between all pairs of nodes
Time Complexity:

Best Case: O(|V|^3)
Worst Case: O(|V|^3)
Average Case: O(|V|^3)



Prim's Algorithm

Prim's Algorithm is a greedy algorithm that finds a minimum spanning tree for a weighted undirected graph. In other words, Prim's find a
subset of edges that forms a tree that includes every node in the graph
Time Complexity: O(|V|^2)


Kruskal's Algorithm

Kruskal's Algorithm is also a greedy algorithm that finds a minimum spanning tree in a graph. However, in Kruskal's, the graph does not
have to be connected
Time Complexity: O(|E|log|V|)


Greedy Algorithms

Greedy Algorithms are algorithms that make locally optimal choices at each step in the hope of eventually reaching the globally optimal solution
Problems must exhibit two properties in order to implement a Greedy solution:
Optimal Substructure

An optimal solution to the problem contains optimal solutions to the given problem's subproblems


The Greedy Property

An optimal solution is reached by ""greedily"" choosing the locally optimal choice without ever reconsidering previous choices


Example - Coin Change

Given a target amount V cents and a list of denominations of n coins, i.e. we have coinValue[i] (in cents) for coin types i from [0...n - 1],
what is the minimum number of coins that we must use to represent amount V? Assume that we have an unlimited supply of coins of any type
Coins - Penny (1 cent), Nickel (5 cents), Dime (10 cents), Quarter (25 cents)
Assume V = 41. We can use the Greedy algorithm of continuously selecting the largest coin denomination less than or equal to V, subtract that
coin's value from V, and repeat.
V = 41 | 0 coins used
V = 16 | 1 coin used (41 - 25 = 16)
V = 6  | 2 coins used (16 - 10 = 6)
V = 1  | 3 coins used (6 - 5 = 1)
V = 0  | 4 coins used (1 - 1 = 0)
Using this algorithm, we arrive at a total of 4 coins which is optimal



Bitmasks

Bitmasking is a technique used to perform operations at the bit level. Leveraging bitmasks often leads to faster runtime complexity and
helps limit memory usage
Test kth bit: s & (1 << k);
Set kth bit: s |= (1 << k);
Turn off kth bit: s &= ~(1 << k);
Toggle kth bit: s ^= (1 << k);
Multiple by 2n: s << n;
Divide by 2n: s >> n;
Intersection: s & t;
Union: s | t;
Set Subtraction: s & ~t;
Extract lowest set bit: s & (-s);
Extract lowest unset bit: ~s & (s + 1);
Swap Values:
x ^= y; y ^= x; x ^= y;

Runtime Analysis
Big O Notation

Big O Notation is used to describe the upper bound of a particular algorithm. Big O is used to describe worst case scenarios


Little O Notation

Little O Notation is also used to describe an upper bound of a particular algorithm; however, Little O provides a bound
that is not asymptotically tight

Big Ω Omega Notation

Big Omega Notation is used to provide an asymptotic lower bound on a particular algorithm


Little ω Omega Notation

Little Omega Notation is used to provide a lower bound on a particular algorithm that is not asymptotically tight

Theta Θ Notation

Theta Notation is used to provide a bound on a particular algorithm such that it can be ""sandwiched"" between
two constants (one for an upper limit and one for a lower limit) for sufficiently large values


Video Lectures

Data Structures

UC Berkeley Data Structures
MIT Advanced Data Structures


Algorithms

MIT Introduction to Algorithms
MIT Advanced Algorithms
UC Berkeley Algorithms



Interview Books

Competitive Programming 3 - Steven Halim & Felix Halim
Cracking The Coding Interview - Gayle Laakmann McDowell
Cracking The PM Interview - Gayle Laakmann McDowell & Jackie Bavaro
Introduction to Algorithms -  Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest & Clifford Stein

Computer Science News

Hacker News
Lobsters

Directory Tree
.
├── Array
│   ├── bestTimeToBuyAndSellStock.java
│   ├── findTheCelebrity.java
│   ├── gameOfLife.java
│   ├── increasingTripletSubsequence.java
│   ├── insertInterval.java
│   ├── longestConsecutiveSequence.java
│   ├── maximumProductSubarray.java
│   ├── maximumSubarray.java
│   ├── mergeIntervals.java
│   ├── missingRanges.java
│   ├── productOfArrayExceptSelf.java
│   ├── rotateImage.java
│   ├── searchInRotatedSortedArray.java
│   ├── spiralMatrixII.java
│   ├── subsetsII.java
│   ├── subsets.java
│   ├── summaryRanges.java
│   ├── wiggleSort.java
│   └── wordSearch.java
├── Backtracking
│   ├── androidUnlockPatterns.java
│   ├── generalizedAbbreviation.java
│   └── letterCombinationsOfAPhoneNumber.java
├── BinarySearch
│   ├── closestBinarySearchTreeValue.java
│   ├── firstBadVersion.java
│   ├── guessNumberHigherOrLower.java
│   ├── pow(x,n).java
│   └── sqrt(x).java
├── BitManipulation
│   ├── binaryWatch.java
│   ├── countingBits.java
│   ├── hammingDistance.java
│   ├── maximumProductOfWordLengths.java
│   ├── numberOf1Bits.java
│   ├── sumOfTwoIntegers.java
│   └── utf-8Validation.java
├── BreadthFirstSearch
│   ├── binaryTreeLevelOrderTraversal.java
│   ├── cloneGraph.java
│   ├── pacificAtlanticWaterFlow.java
│   ├── removeInvalidParentheses.java
│   ├── shortestDistanceFromAllBuildings.java
│   ├── symmetricTree.java
│   └── wallsAndGates.java
├── DepthFirstSearch
│   ├── balancedBinaryTree.java
│   ├── battleshipsInABoard.java
│   ├── convertSortedArrayToBinarySearchTree.java
│   ├── maximumDepthOfABinaryTree.java
│   ├── numberOfIslands.java
│   ├── populatingNextRightPointersInEachNode.java
│   └── sameTree.java
├── Design
│   └── zigzagIterator.java
├── DivideAndConquer
│   ├── expressionAddOperators.java
│   └── kthLargestElementInAnArray.java
├── DynamicProgramming
│   ├── bombEnemy.java
│   ├── climbingStairs.java
│   ├── combinationSumIV.java
│   ├── countingBits.java
│   ├── editDistance.java
│   ├── houseRobber.java
│   ├── paintFence.java
│   ├── paintHouseII.java
│   ├── regularExpressionMatching.java
│   ├── sentenceScreenFitting.java
│   ├── uniqueBinarySearchTrees.java
│   └── wordBreak.java
├── HashTable
│   ├── binaryTreeVerticalOrderTraversal.java
│   ├── findTheDifference.java
│   ├── groupAnagrams.java
│   ├── groupShiftedStrings.java
│   ├── islandPerimeter.java
│   ├── loggerRateLimiter.java
│   ├── maximumSizeSubarraySumEqualsK.java
│   ├── minimumWindowSubstring.java
│   ├── sparseMatrixMultiplication.java
│   ├── strobogrammaticNumber.java
│   ├── twoSum.java
│   └── uniqueWordAbbreviation.java
├── LinkedList
│   ├── addTwoNumbers.java
│   ├── deleteNodeInALinkedList.java
│   ├── mergeKSortedLists.java
│   ├── palindromeLinkedList.java
│   ├── plusOneLinkedList.java
│   ├── README.md
│   └── reverseLinkedList.java
├── Queue
│   └── movingAverageFromDataStream.java
├── README.md
├── Sort
│   ├── meetingRoomsII.java
│   └── meetingRooms.java
├── Stack
│   ├── binarySearchTreeIterator.java
│   ├── decodeString.java
│   ├── flattenNestedListIterator.java
│   └── trappingRainWater.java
├── String
│   ├── addBinary.java
│   ├── countAndSay.java
│   ├── decodeWays.java
│   ├── editDistance.java
│   ├── integerToEnglishWords.java
│   ├── longestPalindrome.java
│   ├── longestSubstringWithAtMostKDistinctCharacters.java
│   ├── minimumWindowSubstring.java
│   ├── multiplyString.java
│   ├── oneEditDistance.java
│   ├── palindromePermutation.java
│   ├── README.md
│   ├── reverseVowelsOfAString.java
│   ├── romanToInteger.java
│   ├── validPalindrome.java
│   └── validParentheses.java
├── Tree
│   ├── binaryTreeMaximumPathSum.java
│   ├── binaryTreePaths.java
│   ├── inorderSuccessorInBST.java
│   ├── invertBinaryTree.java
│   ├── lowestCommonAncestorOfABinaryTree.java
│   ├── sumOfLeftLeaves.java
│   └── validateBinarySearchTree.java
├── Trie
│   ├── addAndSearchWordDataStructureDesign.java
│   ├── implementTrie.java
│   └── wordSquares.java
└── TwoPointers
    ├── 3Sum.java
    ├── 3SumSmaller.java
    ├── mergeSortedArray.java
    ├── minimumSizeSubarraySum.java
    ├── moveZeros.java
    ├── removeDuplicatesFromSortedArray.java
    ├── reverseString.java
    └── sortColors.java

18 directories, 124 files


"
17,"






  Gatsby v2


⚛️ 📄 🚀


  Fast in every way that matters


  Gatsby is a free and open source framework based on React that helps developers build blazing fast websites and apps

























Quickstart
 · 
Tutorial
 · 
Plugins
 · 
Starters
 · 
Showcase
 · 
Contribute
 · 
  Support: Spectrum
 & 
Discord

Gatsby is a modern web framework for blazing fast websites.


Go Beyond Static Websites. Get all the benefits of static websites with none of the
limitations. Gatsby sites are fully functional React apps so you can create high-quality,
dynamic web apps, from blogs to e-commerce sites to user dashboards.


Use a Modern Stack for Every Site. No matter where the data comes from, Gatsby sites are
built using React and GraphQL. Build a uniform workflow for you and your team, regardless of
whether the data is coming from the same backend.


Load Data From Anywhere. Gatsby pulls in data from any data source, whether it’s Markdown
files, a headless CMS like Contentful or WordPress, or a REST or GraphQL API. Use source plugins
to load your data, then develop using Gatsby’s uniform GraphQL interface.


Performance Is Baked In. Ace your performance audits by default. Gatsby automates code
splitting, image optimization, inlining critical styles, lazy-loading, and prefetching resources,
and more to ensure your site is fast — no manual tuning required.


Host at Scale for Pennies. Gatsby sites don’t require servers so you can host your entire
site on a CDN for a fraction of the cost of a server-rendered site. Many Gatsby sites can be
hosted entirely free on services like GitHub Pages and Netlify.


Learn how to use Gatsby for your next project.
What’s In This Document

Get Up and Running in 5 Minutes
Learning Gatsby
Migration Guides
How to Contribute
License
Thanks to Our Contributors and Sponsors

🚀 Get Up and Running in 5 Minutes
You can get a new Gatsby site up and running on your local dev environment in 5 minutes with these four steps:


Install the Gatsby CLI.
npm install -g gatsby-cli



Create a Gatsby site from a Gatsby starter.
Get your Gatsby blog set up in a single command:
# create a new Gatsby site using the default starter
gatsby new my-blazing-fast-site


Start the site in develop mode.
Next, move into your new site’s directory and start it up:
cd my-blazing-fast-site/
gatsby develop


Open the source code and start editing!
Your site is now running at http://localhost:8000. Open the my-blazing-fast-site directory in your code editor of choice and edit src/pages/index.js. Save your changes, and the browser will update in real time!


At this point, you’ve got a fully functional Gatsby website. For additional information on how you can customize your Gatsby site, see our plugins and the official tutorial.
🎓 Learning Gatsby
Full documentation for Gatsby lives on the website.


For most developers, we recommend starting with our in-depth tutorial for creating a site with Gatsby. It starts with zero assumptions about your level of ability and walks through every step of the process.


To dive straight into code samples head to our documentation. In particular, check out the “Guides”, “API Reference”, and “Advanced Tutorials” sections in the sidebar.


We welcome suggestions for improving our docs. See the “how to contribute” documentation for more details.
Start Learning Gatsby: Follow the Tutorial · Read the Docs
💼 Migration Guides
Already have a Gatsby site? These handy guides will help you add the improvements of Gatsby v2 to your site without starting from scratch!

Migrate a Gatsby site from v1 to v2
Still on v0? Start here: Migrate a Gatsby site from v0 to v1

❗ Code of Conduct
Gatsby is dedicated to building a welcoming, diverse, safe community. We expect everyone participating in the Gatsby community to abide by our Code of Conduct. Please read it. Please follow it. In the Gatsby community, we work hard to build each other up and create amazing things together. 💪💜
🤝 How to Contribute
Whether you're helping us fix bugs, improve the docs, or spread the word, we'd love to have you as part of the Gatsby community! 💪💜
Check out our Contributing Guide for ideas on contributing and setup steps for getting our repositories up and running on your local machine.
A note on how this repository is organized
This repository is a monorepo managed using Lerna. This means there are multiple packages managed in this codebase, even though we publish them to NPM as separate packages.
Contributing to Gatsby v1
We are currently only accepting bug fixes for Gatsby v1. No new features will be accepted.
📝 License
Licensed under the MIT License.
💜 Thanks
Thanks to our many contributors and to Netlify for hosting gatsbyjs.org and our example sites.

"
18,"
OpenCV: Open Source Computer Vision Library
Resources

Homepage: https://opencv.org
Docs: https://docs.opencv.org/master/
Q&A forum: http://answers.opencv.org
Issue tracking: https://github.com/opencv/opencv/issues

Contributing
Please read the contribution guidelines before starting work on a pull request.
Summary of the guidelines:

One pull request per issue;
Choose the right base branch;
Include tests and documentation;
Clean up ""oops"" commits before submitting;
Follow the coding style guide.


"
19,"

Join our community for professional Software Developers and get more control over your life and career!


Every Programmer Should Know 🤔
A collection of (mostly) technical things every software developer should know.
☝️ These are resources I can recommend to every programmer regardless of their skill level or tech stack
Highly opinionated 💣. Not backed by science.
Comes in no particular order ♻️
U like it? ⭐️ it and share with a friendly developer!
U don't like it? Watch the doggo 🐶
P.S. You don't need to know all of that by heart to be a programmer.
But knowing the stuff will help you become better! 💪
P.P.S. Contributions are welcome!

Introduction

🎥 Map of Computer Science
🎥 40 Key Computer Science Concepts Explained In Layman’s Terms

Falsehoods

Awesome Falsehoods
💊 Curated list of falsehoods programmers believe in.
Check for things you do not know about Strings, Addresses, Names, Numbers, Emails, Timezones and Dates and more.

Algorithms

Big O Cheatsheet
📖 Grokking Algorithms
Algorithms Visualization

Data Structures

🎥 UC Berkeley, Data Structures Course
Foundations of Data Structures - EDX
Data Structures - Coursera
Mathematics for Computer Science - Eric Lehman

Numbers

📖 How to Count
📄 Floating Point Guide
📄 What Every Computer Scientist Should Know About Floating-Point Arithmetic
📄 Basic Number Theory Every Programmer Should Know...

Strings

📄 Unicode and Character Sets
Homoglyphs
Unicode Common Locale Data Repository
🎥 ASCII
🎥 UTF-8

Latency

Interactive Latency Infographics
📄 Latency Numbers Every Programmer Should Know

Time

📄 Some notes about time
🎥 The Problem with Timezones

Memory

📄 What every Programmer should know about memory

Distributed Systems

📖 Designing Data-Intensive Applications
📜 Designs, Lessons and Advice from Building Large Distributed Systems
📜 Time, Clocks and the Ordering of Events in a Distributed System
📄 There is No Now
📄 Jepsen: how different databases behave under partition
📜 Fallacies of Distributed Computing Explained

RegExp

RegexHQ
Learn regex the easy way

Security

📖 Security Programming
📄 Rolling Your Own Crypto
📄 Cryptographic Right Answers
📄 An Open Letter to Developers Everywhere (About Cryptography)
📖 Foundations of Security: What Every Programmer Needs to Know 
OWASP Top 10
Web Application Exploits and Defenses
📄 Hashing, Encryption and Encoding

UX/Usability

📖 Don't Make Me Think: A Common Sense Approach to Web Usability
🎥 Inventing on Principle

SEO

📄 What Every Programmer Should Know About SEO

Architecture

📜 A Field Guide to Boxology
📜 Out of the Tar Pit
📜 No Silver Bullet — Essence and Accidents of Software Engineering
🎥 Growing a Language
🎥 CQRS and Event Sourcing
📖 Practical Object Oriented Design in Ruby
🎥 Evolutionary Software Architectures
System Design: A Primer
📄 How JavaScript works: part-1, 2, 3, 4
🎥 Entity-Component-System Architecture with Unity by example

Engineering Philosophy

🎥 Category Theory in Life
🎥 Simple Made Easy
📄 Speed In Software Development
🎥 #NoEstimates
🎥 The Myth of the Genius Programmer
🎥 Making Badass Developers
📄 The Ten Rules of a Zen Programmer
📄 The mythical 10x programmer
📄 The Debugging Mindset
🎥 The Future of Programming
📄 The Good Software Development Manifesto

Practices

📖 Working Effectively with Legacy Code
📖 Clean Code: A Handbook of Agile Software Craftsmanship
📖 Test Driven Development: By Example
✅ Going To Production Checklist
📖 Release It!
📖 Professor Frisby's Mostly Adequate Guide to Functional Programming
📖 SICP: Structure and Interpretation of Computer Programs
📄 Thirteen Ways of Looking at a Turtle
📜 Programming Paradigms for Dummies: What Every Programmer Should Know
Learn X in Y Minutes
Learn the basics of a language in a highly condensed way.
Hyperpolyglot
Compare commonly used features of more or less similar languages side-by-side. Helps you to jump Python<->Ruby, Ocaml<->Haskell, etc.
📄 Pomodoro for Programmers
📖 Site Reliability Engineering

Career

💰 Levels FYI
Salary stats for various tech companies. Better than Glassdoor.
📄 10 Things Every Programmer Should Know For Their First Job
📄 How Much Do Software Engineers Really Make in Each City?
📄 Software Engineers Tenure in San Francisco
Software Engineering 101
📖 The Passionate Programmer
📖 Soft Skills: The software developer's life manual
📖 The Complete Software Developer's Career Guide
📖 Programming Beyond Practices: Be More Than Just a Code Monkey
A list of European Investors
📄 Ten Rules for Negotiating a Job Offer
📄 How To Interview As a Developer Candidate
📄 How To Get a Tech Job Abroad Faster
📖 How To Be A STAR Engineer
📄 TL;DR; Stock Options
📄 Equity 101 for Startup Employees
📖 Cracking the Coding Interview: 189 Programming Questions and Solutions
🔥 Everything you need to know to get the job
📖 Tech Interview Handbook
📄 Teach Yourself Programming in Ten Years
📄 What you should know as a founder of a software company

Fine-tune Your Resume

🔨 CV Compiler

Open Source

🌐 An Intro to Git and GitHub for Beginners

Remote Work

🌐 Remotive.io: Startups hiring remotely
🌐 Remote Work List for Developers
⚡️ NomadList
📖 The Ultimate Guide to Remote Work
🏠 Awesome Remote Job

Problem Solving

📖 The Art and Craft of Problem Solving
📖 How to Solve It: A New Aspect of Mathematical Method

Soft Skills

📖 Difficult Conversations
📖 Crucial Conversations
📖 How to Win Friends and Influence People

Mental Health

Awesome Mental Health
A curated list of awesome articles, websites and resources about mental health in the software industry.

Papers on Programming

❤️ Papers We Love
📰 The Morning Paper
📜 What Every Programmer Should Know About Memory
📜 Go To Statement Considered Harmful

Free Books on Programming

📚 Free Programming Books

Services ⚡️

Free For Dev
Public APIs
The Noun Project
Without Coding
Simpleicons
Learn Anything
repl.it

Licenses

Choose An Open Source License
Well-explained Software licenses in TLDR version

Where To Look For Further Info

freeCodeCamp Guide
GeeksForGeeks
Dev.To
Stack Overflow
Dzone

Coding Practice Sites ⚡️

🔗 CodeForces
🔗 CodeChef
🔗 Coderbyte
🔗 CodinGame
🔗 Cs Academy
🔗 HackerRank
🔗 Spoj
🔗 HackerEarth
🔗 TopCoder
🔗 Codewars
🔗 Exercism
🔗 CodeSignal
🔗 Project Euler
🔗 LeetCode
🔗 Firecode.io
🔗 InterviewBit
🔗 uCoder
🔗 LintCode
🔗 CodeCombat
🔗 InterviewCake
🔗 At Coder


"
